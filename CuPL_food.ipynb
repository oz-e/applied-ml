{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqZK1m72H9fRkW0kDFEJBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CuPL_food.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT2a2N2Yof_",
        "outputId": "5a4066dd-23a6-418d-8656-387f98f9d50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/applied-ml\n",
            "Requirement already satisfied: openai-clip in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from openai-clip) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "# Change the path if necessary\n",
        "datasets_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "  if not os.path.exists('/content/applied-ml/'):\n",
        "    !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "    import colab_github\n",
        "    colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "    !git clone git@github.com:oz-e/applied-ml.git\n",
        "    if not os.path.exists('/content/applied-ml/'):\n",
        "      raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "  %cd /content/applied-ml/\n",
        "\n",
        "  # Install any other requirements (to be converted to requirements.txt)\n",
        "  !pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"CuPL\"):\n",
        "  !git clone https://github.com/sarahpratt/CuPL.git"
      ],
      "metadata": {
        "id": "CNDfTcucYyJ1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from pkg_resources import packaging\n",
        "from CuPL.imagenet_prompts.standard_image_prompts import imagenet_templates\n",
        "import pdb\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jtmnqOzCbmfr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/16\")"
      ],
      "metadata": {
        "id": "2OOZZLgVbq7k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aml.datasets\n",
        "\n",
        "all_images = aml.datasets.Food101(datasets_path, split='test', transform=preprocess)\n",
        "loader = torch.utils.data.DataLoader(all_images, batch_size=512, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWp11SF3bQwg",
        "outputId": "01e8befe-110c-402a-f895-5df566f6b505"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PROMPTS = './CuPL/all_prompts/full_prompts/food_prompts_full.json'\n",
        "\n",
        "with open(PATH_TO_PROMPTS) as f:\n",
        "  gpt3_prompts = json.load(f)"
      ],
      "metadata": {
        "id": "-EC3XKq2Zis4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_class_mapping = {\n",
        "}"
      ],
      "metadata": {
        "id": "r6KLeBmtarfp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For bridging classsname in dataset with name in CuLP prompts\n",
        "new_gpt3_prompts = {}\n",
        "\n",
        "for key, value in gpt3_prompts.items():\n",
        "  new_key = special_class_mapping.get(key, key)\n",
        "  new_gpt3_prompts[new_key.replace(' ', '_').lower()] = value"
      ],
      "metadata": {
        "id": "1S89k8iXjsYn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier(classnames, templates):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\ttexts = [template.format(classname) for template in templates] #format with class\n",
        "\t\t\ttexts = clip.tokenize(texts).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "9FsIz2qjbwIa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier_gpt(classnames, templates, use_both):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\tif use_both:\n",
        "\t\t\t\ttexts = [template.format(classname) for template in templates]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttexts = []\n",
        "\n",
        "\t\t\tfor t in new_gpt3_prompts[classname]:\n",
        "\t\t\t\ttexts.append(t)\n",
        "\t\t\ttexts = clip.tokenize(texts, truncate=True).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "hUQlruDEbwy1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating standard text embeddings...\")\n",
        "zeroshot_weights_base = zeroshot_classifier(all_images.classnames, imagenet_templates)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating CuPL text embeddings...\")\n",
        "zeroshot_weights_cupl = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, False)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating combined text embeddings...\")\n",
        "zeroshot_weights_gpt_both = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, True)\n",
        "print(\"Done.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uct62dTFbyz4",
        "outputId": "5cfae6ae-aed3-4bf0-decd-a601352b8318"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating standard text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:06<00:00, 16.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating CuPL text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:03<00:00, 32.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating combined text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:08<00:00, 12.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0.\n",
        "correct_base = 0.\n",
        "correct_cupl = 0.\n",
        "correct_both = 0.\n",
        "\n",
        "print(\"Classifying ImageNet...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "\tfor i, (images, target) in enumerate(tqdm(loader)):\n",
        "\t\timages = images.cuda()\n",
        "\t\ttarget = target.cuda()\n",
        "\n",
        "\t\t# predict\n",
        "\t\timage_features = model.encode_image(images)\n",
        "\t\timage_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\t\tlogits_base = image_features @ zeroshot_weights_base\n",
        "\t\tlogits_cupl = image_features @ zeroshot_weights_cupl\n",
        "\t\tlogits_both = image_features @ zeroshot_weights_gpt_both\n",
        "\n",
        "\t\tpred_base = torch.argmax(logits_base, dim =1)\n",
        "\t\tpred_cupl = torch.argmax(logits_cupl, dim =1)\n",
        "\t\tpred_both = torch.argmax(logits_both, dim =1)\n",
        "\n",
        "\t\tfor j in range(len(target)):\n",
        "\t\t\ttotal += 1.\n",
        "\t\t\tif pred_base[j] == target[j]:\n",
        "\t\t\t\tcorrect_base += 1.\n",
        "\t\t\tif pred_cupl[j] == target[j]:\n",
        "\t\t\t\tcorrect_cupl += 1.\n",
        "\t\t\tif pred_both[j] == target[j]:\n",
        "\t\t\t\tcorrect_both += 1."
      ],
      "metadata": {
        "id": "q6odbVXGdAts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aea886b-f6d0-46bd-be65-31b1f8978d19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying ImageNet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [03:13<00:00,  3.22s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "top1 = (correct_base / total) * 100\n",
        "print(f\"Top-1 accuracy standard: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_cupl / total) * 100\n",
        "print(f\"Top-1 accuracy CuPL: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_both / total) * 100\n",
        "print(f\"Top-1 accuracy both: {top1:.2f}\")"
      ],
      "metadata": {
        "id": "YjXwPA6ZdFZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a0c74c-d160-40b1-c829-1531fc9365af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 accuracy standard: 83.84\n",
            "Top-1 accuracy CuPL: 86.02\n",
            "Top-1 accuracy both: 85.06\n"
          ]
        }
      ]
    }
  ]
}