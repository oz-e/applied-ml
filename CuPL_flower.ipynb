{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOm2Iw/4ONTf3iLmj3SY/vw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CuPL_flower.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT2a2N2Yof_",
        "outputId": "cffd87d0-3d7e-4e53-988d-b0fcba9df030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks that a private key is already created. If you have already push it to github, no action required.\n",
            " Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIH5WA3eEJcxX9yCQyq2OSQnH4nSjgUWX9UZ05hCh7Gf3 root@c5964d507137\n",
            "\n",
            "Please use SSH method to clone repo.\n",
            "Cloning into 'applied-ml'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 94 (delta 50), reused 18 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (94/94), 1.92 MiB | 5.86 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content/applied-ml\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=1f603d3c799f8279d27904db1d22d743948f61a40d3b3b940c7a45630dc73751\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Change the path if necessary\n",
        "datasets_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "  if not os.path.exists('/content/applied-ml/'):\n",
        "    !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "    import colab_github\n",
        "    colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "    !git clone git@github.com:oz-e/applied-ml.git\n",
        "    if not os.path.exists('/content/applied-ml/'):\n",
        "      raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "  %cd /content/applied-ml/\n",
        "\n",
        "  # Install any other requirements (to be converted to requirements.txt)\n",
        "  !pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"CuPL\"):\n",
        "  !git clone https://github.com/sarahpratt/CuPL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNDfTcucYyJ1",
        "outputId": "339745a8-ee7d-4670-c121-0d639a0c4473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CuPL'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 56 (delta 6), reused 52 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (56/56), 4.68 MiB | 14.09 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from pkg_resources import packaging\n",
        "from CuPL.imagenet_prompts.standard_image_prompts import imagenet_templates\n",
        "import pdb\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jtmnqOzCbmfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/16\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOZZLgVbq7k",
        "outputId": "db0d233d-6dd4-45c0-d03d-389883c72b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 87.5MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aml.datasets\n",
        "\n",
        "all_images = aml.datasets.Flowers102(datasets_path, split='test', transform=preprocess)\n",
        "loader = torch.utils.data.DataLoader(all_images, batch_size=512, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWp11SF3bQwg",
        "outputId": "48d022ec-9ee6-4fb8-a1bb-32bcecc07ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:25<00:00, 13.5MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 830kB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 20.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Pp0sRXzZFZq15zVOzKjKBu4A9i01nozT\n",
            "To: /content/applied-ml/datasets/flowers-102/split.json\n",
            "100%|██████████| 771k/771k [00:00<00:00, 102MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PROMPTS = './CuPL/all_prompts/full_prompts/flower_prompts_full.json'\n",
        "\n",
        "with open(PATH_TO_PROMPTS) as f:\n",
        "  gpt3_prompts = json.load(f)"
      ],
      "metadata": {
        "id": "-EC3XKq2Zis4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_class_mapping = {\n",
        "    'globe flower': 'globe-flower',\n",
        "    'pink and yellow dahlia': 'pink-yellow dahlia',\n",
        "    'air plant': 'ball moss',\n",
        "}"
      ],
      "metadata": {
        "id": "r6KLeBmtarfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For bridging classsname in dataset with name in CuLP prompts\n",
        "new_gpt3_prompts = {}\n",
        "\n",
        "for key, value in gpt3_prompts.items():\n",
        "  new_key = special_class_mapping.get(key, key)\n",
        "  new_gpt3_prompts[new_key.lower()] = value"
      ],
      "metadata": {
        "id": "1S89k8iXjsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier(classnames, templates):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\ttexts = [template.format(classname) for template in templates] #format with class\n",
        "\t\t\ttexts = clip.tokenize(texts).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "9FsIz2qjbwIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier_gpt(classnames, templates, use_both):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\tif use_both:\n",
        "\t\t\t\ttexts = [template.format(classname) for template in templates]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttexts = []\n",
        "\n",
        "\t\t\tfor t in new_gpt3_prompts[classname]:\n",
        "\t\t\t\ttexts.append(t)\n",
        "\t\t\ttexts = clip.tokenize(texts, truncate=True).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "hUQlruDEbwy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating standard text embeddings...\")\n",
        "zeroshot_weights_base = zeroshot_classifier(all_images.classnames, imagenet_templates)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating CuPL text embeddings...\")\n",
        "zeroshot_weights_cupl = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, False)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating combined text embeddings...\")\n",
        "zeroshot_weights_gpt_both = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, True)\n",
        "print(\"Done.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uct62dTFbyz4",
        "outputId": "d9b0928b-7f9c-4f72-a36d-1c9f242a7ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating standard text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:05<00:00, 18.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating CuPL text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:01<00:00, 62.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating combined text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:06<00:00, 15.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0.\n",
        "correct_base = 0.\n",
        "correct_cupl = 0.\n",
        "correct_both = 0.\n",
        "\n",
        "print(\"Classifying ImageNet...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "\tfor i, (images, target) in enumerate(tqdm(loader)):\n",
        "\t\timages = images.cuda()\n",
        "\t\ttarget = target.cuda()\n",
        "\n",
        "\t\t# predict\n",
        "\t\timage_features = model.encode_image(images)\n",
        "\t\timage_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\t\tlogits_base = image_features @ zeroshot_weights_base\n",
        "\t\tlogits_cupl = image_features @ zeroshot_weights_cupl\n",
        "\t\tlogits_both = image_features @ zeroshot_weights_gpt_both\n",
        "\n",
        "\t\tpred_base = torch.argmax(logits_base, dim =1)\n",
        "\t\tpred_cupl = torch.argmax(logits_cupl, dim =1)\n",
        "\t\tpred_both = torch.argmax(logits_both, dim =1)\n",
        "\n",
        "\t\tfor j in range(len(target)):\n",
        "\t\t\ttotal += 1.\n",
        "\t\t\tif pred_base[j] == target[j]:\n",
        "\t\t\t\tcorrect_base += 1.\n",
        "\t\t\tif pred_cupl[j] == target[j]:\n",
        "\t\t\t\tcorrect_cupl += 1.\n",
        "\t\t\tif pred_both[j] == target[j]:\n",
        "\t\t\t\tcorrect_both += 1."
      ],
      "metadata": {
        "id": "q6odbVXGdAts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83178a68-fa78-4e3f-b6b4-6850d5bb0d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying ImageNet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 5/5 [00:25<00:00,  5.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "top1 = (correct_base / total) * 100\n",
        "print(f\"Top-1 accuracy standard: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_cupl / total) * 100\n",
        "print(f\"Top-1 accuracy CuPL: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_both / total) * 100\n",
        "print(f\"Top-1 accuracy both: {top1:.2f}\")"
      ],
      "metadata": {
        "id": "YjXwPA6ZdFZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67f5f8d-7ceb-4ef8-fe0e-af93f91e8d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 accuracy standard: 65.98\n",
            "Top-1 accuracy CuPL: 73.93\n",
            "Top-1 accuracy both: 68.98\n"
          ]
        }
      ]
    }
  ]
}