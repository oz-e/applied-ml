{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CoOP\n",
        "This notebook performs CoOP using CLIP, This notebook performs fewshot leanrning (1,2...16) on various context length, on different class token position (front, mid, end) on various models."
      ],
      "metadata": {
        "id": "Oi-qSVH1PEnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurations"
      ],
      "metadata": {
        "id": "dzY99FHBQAAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ElykoMZAPC0J"
      },
      "outputs": [],
      "source": [
        "colab_clone_repo = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_name = 'caltech'   #['airplane', 'caltech', 'dtd', 'flower', 'food', 'pets', 'ucf']\n",
        "model_name = \"ViT-B/16\" #['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']\n",
        "path=f\"output/{model_name}/{test_dataset_name}/coop_prompt.pth\" # Model save path\n",
        "# mention all the parameters\n",
        "NUM_SHOTS = 16\n",
        "SEED = 1\n",
        "n_ctx = 16  # few shot learning (1,2,....16)\n",
        "ctx_init = \"\"  # context vector, rn its not initialized\n",
        "class_token_position = \"end\"  #[\"front\", \"middle\", \"end\"]\n",
        "csc = False  # For using the class specific context\n",
        "input_size = 224  # Input Image\n",
        "# if csc is True then initialize it with \"photo of a\" or else False then use the generic\n",
        "if not ctx_init:\n",
        "    if csc:\n",
        "        ctx_init = \"a photo of a\"\n",
        "    else:\n",
        "        ctx_init = \"\""
      ],
      "metadata": {
        "id": "KfOPtjAOQFjQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the Execution Environment"
      ],
      "metadata": {
        "id": "WMTquhW_QMhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the path if necessary\n",
        "dataset_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    if colab_clone_repo and not os.path.exists('/content/applied-ml/'):\n",
        "        !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "        import colab_github\n",
        "        colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "        %cd /content/\n",
        "        !git clone git@github.com:oz-e/applied-ml.git\n",
        "        if not os.path.exists('/content/applied-ml/'):\n",
        "            raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "\n",
        "    if colab_clone_repo:\n",
        "        %cd /content/applied-ml/\n",
        "    else:\n",
        "        %cd /content/\n",
        "\n",
        "    # Install any other requirements (to be converted to requirements.txt)\n",
        "    !pip install openai-clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6jkbSoe4QRcy",
        "outputId": "d3555f8c-9388-42c7-8a44-c7d2f7033f4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks that a private key is already created. If you have already push it to github, no action required.\n",
            " Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICfZFypVx2gQDCQBjTUcVy0spA0rrh2/7GFBc5Qe1+mG root@8459c455b3d1\n",
            "\n",
            "Please use SSH method to clone repo.\n",
            "/content\n",
            "Cloning into 'applied-ml'...\n",
            "remote: Enumerating objects: 410, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 410 (delta 21), reused 22 (delta 10), pack-reused 362 (from 1)\u001b[K\n",
            "Receiving objects: 100% (410/410), 161.61 MiB | 18.38 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n",
            "/content/applied-ml\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=a8d16d8035bffc02774b83a47dfd8c40e1512b0545d7a78434fea5882f877d39\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-bC2SNQnMx",
        "outputId": "7eda5918-5df9-432d-cde5-e0edcbfe3a1c",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hiztr8u7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-hiztr8u7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=182ab36e2ee7d228d9035891db9c14b0690db02b5487790bede7596f5f9fb8ef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dig0viyt/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('CoOp'):\n",
        "  !git clone https://github.com/KaiyangZhou/CoOp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GzqiD99QVd2",
        "outputId": "4bc067b0-c82e-415c-9fe9-096257d523a5",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 198 (delta 198), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 24.31 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import random_split\n",
        "import clip\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import aml.datasets\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from clip.simple_tokenizer import SimpleTokenizer\n",
        "from clip import clip\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "SXYiEGI5QhPW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the CLIP Model"
      ],
      "metadata": {
        "id": "v2rhovC0Q7fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(model_name, device=device)"
      ],
      "metadata": {
        "id": "oJO1fCftQ9Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb55526-5e63-4c22-9cec-ac66a1c658c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:20<00:00, 17.1MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Data Processing"
      ],
      "metadata": {
        "id": "lrj3ZErySndR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the transform variable according to the models architecture\n",
        "transform = preprocess\n",
        "match test_dataset_name:\n",
        "    case 'airplane':\n",
        "        dataset = aml.datasets.FGVCAircraft(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.FGVCAircraft(root=dataset_path,split='test', transform=preprocess)\n",
        "    case 'caltech':\n",
        "        dataset = aml.datasets.Caltech101(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.Caltech101(root=dataset_path,split='test', transform=preprocess)\n",
        "\n",
        "    case 'dtd':\n",
        "        dataset = aml.datasets.DTD(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.DTD(root=dataset_path,split='test', transform=preprocess)\n",
        "\n",
        "    case 'flower':\n",
        "        dataset = aml.datasets.Flowers102(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.Flowers102(root=dataset_path,split='test', transform=preprocess)\n",
        "\n",
        "    case 'food':\n",
        "        dataset = aml.datasets.Food101(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.Food101(root=dataset_path,split='test', transform=preprocess)\n",
        "    case 'pets':\n",
        "        dataset = aml.datasets.OxfordIIITPet(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.OxfordIIITPet(root=dataset_path,split='test', transform=preprocess)\n",
        "    case 'ucf':\n",
        "        dataset = aml.datasets.UCF101(root=dataset_path,split='train', transform=preprocess)\n",
        "        val_dataset = aml.datasets.UCF101(root=dataset_path,split='test', transform=preprocess)\n",
        "\n",
        "classnames = dataset.classnames"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YR1vcDJjSqFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba40ba0-0a4e-4686-cbd2-9e47dff38cb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD&confirm=t&uuid=809ace3f-3b90-4dac-aacc-410f1d943bdd\n",
            "To: /content/applied-ml/datasets/caltech101/101_ObjectCategories.tar.gz\n",
            "100%|██████████| 132M/132M [00:01<00:00, 86.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d\n",
            "To: /content/applied-ml/datasets/caltech101/Annotations.tar\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 60.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/applied-ml/datasets/caltech101/split.json\n",
            "100%|██████████| 809k/809k [00:00<00:00, 135MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_few_shot(dataset, num_shots=16, seed=1):\n",
        "    random.seed(seed)\n",
        "    class_to_indices = defaultdict(list)\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_to_indices[label].append(idx)\n",
        "\n",
        "    few_shot_indices = []\n",
        "    for label, indices in class_to_indices.items():\n",
        "        selected = random.sample(indices, min(num_shots, len(indices)))\n",
        "        few_shot_indices.extend(selected)\n",
        "\n",
        "    return Subset(dataset, few_shot_indices)\n"
      ],
      "metadata": {
        "id": "NWSWdiZPhFJH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = sample_few_shot(dataset, num_shots=NUM_SHOTS, seed=SEED)\n",
        "#Set up the train and val dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "x3XXERP0hMtE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEXT ENCODER"
      ],
      "metadata": {
        "id": "GslttqYMUMCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retruns the text feature vector of shape (batchsize, embedding)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        # Model to process the token Embeddings\n",
        "        self.transformer = clip_model.transformer\n",
        "        # Positional Encoder\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        # Clip final layer\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        # A learned projection matrix that maps the final hidden state to the CLIP embedding space.\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        # Model data type\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        #adds the positional embedding to the token embedding, which includes the information about the token position\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        # Changes shape from (batch_size, seq_len, dim) → (seq_len, batch_size, dim)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        #Passes the sequence through the Transformer (learns contextual relationships between tokens).\n",
        "        x = self.transformer(x)\n",
        "        # Reverts the shape back to (batch_size, seq_len, dim).\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # Applies layer normalization and converts to the appropriate data type.\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        # Selects the embedding at the position of the end-of-text token, assuming it's the most meaningful. and projects the token into clip embedding space\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ],
      "metadata": {
        "id": "AtdER2IyUSpN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Learner"
      ],
      "metadata": {
        "id": "MG-cOzjiUUpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, n_ctx, ctx_init=None, class_token_position=\"end\", csc=False, input_size=224):\n",
        "        super().__init__()\n",
        "        self.n_cls = len(classnames) # total classnames\n",
        "        self.n_ctx = n_ctx # Number of learnable context tokens\n",
        "        self.ctx_init = ctx_init # String to initialize the context from words\n",
        "        self.class_token_position = class_token_position # class token position (front, middle,  end)\n",
        "        self.csc = csc # True / False value to use one prompt per class\n",
        "        self.input_size = input_size # Image resolution\n",
        "\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        device = clip_model.token_embedding.weight.device\n",
        "\n",
        "        assert self.input_size == clip_imsize, f\"cfg_imsize ({self.input_size}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        # if the string is provided to initialize the context this perform the below block (\"a photo of a\")\n",
        "        if self.ctx_init:\n",
        "            # replaces the underscore of the string to spaces\n",
        "            ctx_init = self.ctx_init.replace(\"_\", \" \")\n",
        "            # updates the n_ctx ( number of learnable tokens) based on how many words are in the prompt. here its 4 as (\"a photo of a\")\n",
        "            self.n_ctx = len(ctx_init.split(\" \"))\n",
        "            # Tokenize the prompt, output shape- [1,77]\n",
        "            prompt = clip.tokenize(ctx_init).to(device)\n",
        "            # converts the tokenized prompts imto embeddings, output shape - [1, 77, ctx_dim]\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            #extract the embedding for the context words not for the special tokens, [sos], [eos]\n",
        "            ctx_vectors = embedding[0, 1 : 1 + self.n_ctx, :].to(device)\n",
        "            # stores the original ctx_init prompt\n",
        "            prompt_prefix = ctx_init\n",
        "        # if the string is not provided the perform the below code\n",
        "        else:\n",
        "            # if csc is true which works for the class specific prompts the perform the below code\n",
        "            if self.csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                # creates separate set of learnable context tokens for each number of class (n_cls)\n",
        "                ctx_vectors = torch.empty(self.n_cls, self.n_ctx, ctx_dim, dtype=dtype, device=device) # size of [100, 16, 512] for 100 classes, 16 context length and 512 embedding\n",
        "            # if False then perform the below code\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                # single set of context tokens for all classes\n",
        "                ctx_vectors = torch.empty(self.n_ctx, ctx_dim, dtype=dtype, device=device) #output shape of [16, 512] for ctx = 16, ctx_dim=512\n",
        "            # randomly initializing the context vector (ctx_vector) from the normal distribution\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * self.n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"') # prints number of X for n_ctx\n",
        "        print(f\"Number of context words (tokens): {self.n_ctx}\")\n",
        "\n",
        "        #wraps into nn.Parameter making them trainable weights\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        # data cleaning, replaces the underscore in the class names with the space\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        # stores the length of the tokens of each class name\n",
        "        self.name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        # creates the string stored in prompts variable with class names\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        # clip.tokenize tokenize all class prompts into sequence of token ID and send to the device\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device) #output size of [n_cls, 77] number of class and 77 clip input size\n",
        "        # convert the token ids in the tokenized_prompt variable into embeddings layer\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype) # output shape [n_cls, 77, ctx_dim]\n",
        "        # start of the token (special token) not trainable, so set it as fixed buffer\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "        # end of the token as well, not trainable\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + self.n_ctx :, :])\n",
        "\n",
        "        # stores the original token IDs\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "\n",
        "    # Assembles the final prompt embeddings for each class, the output of this are input to the text encoder\n",
        "    def forward(self):\n",
        "        ctx = self.ctx\n",
        "        # if the context is shared that is [n_ctx, ctx_dim] no n_cls then expands this same context for all the classes [n_cls, n_ctx, ctx_dim]\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        prefix = self.token_prefix # get the starting token\n",
        "        suffix = self.token_suffix # get the end token\n",
        "\n",
        "        # This works on the position of the class token provided (end, middle, front)\n",
        "        if self.class_token_position == \"end\":\n",
        "            #prompt is in form- [start token, context, classname, end token] simple contactenation\n",
        "            prompts = torch.cat([prefix, ctx, suffix], dim=1)\n",
        "\n",
        "        # For Middle position\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2 # get the middle\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat([prefix_i, ctx_i_half1, class_i, ctx_i_half2, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0) #prompt is in form- [start token, context, classname, context, end token] simple contactenation\n",
        "\n",
        "        # For front position\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat([prefix_i, class_i, ctx_i, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0) #prompt is in form- [start token, classname, context, end token] simple contactenation\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid class_token_position: {self.class_token_position}\")\n",
        "\n",
        "        return prompts, self.tokenized_prompts # returns prompt tensor of shape [n_cls, sequence lenght (context), ctx_dim] and tokenized_prompt with token_ids"
      ],
      "metadata": {
        "id": "9ToXrbU-UWcL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clip Model"
      ],
      "metadata": {
        "id": "yEAXqyuKUYl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        # creates a learnable prompt for each class\n",
        "        self.prompt_learner = PromptLearner(\n",
        "            classnames=classnames,\n",
        "            clip_model=clip_model,\n",
        "            n_ctx=n_ctx,\n",
        "            ctx_init=ctx_init,\n",
        "            csc=csc,\n",
        "            class_token_position=class_token_position,\n",
        "            input_size=input_size\n",
        "        )\n",
        "        #loads clip vision encoder\n",
        "        self.image_encoder = clip_model.visual\n",
        "        #text encoder\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        #use to scale similarity logits ( this helps in improving the convergance)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        # clip model tensor data type\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image):\n",
        "      # encodes the image, outputs the image features vectors\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        # generate prompts and tokenized prompts with token IDs from the prompt learner class we declared above\n",
        "        prompts, tokenized_prompts = self.prompt_learner()\n",
        "        # encodes the prompts and tokenized output from the prompt learner into the text encoder of the model\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "        # Normalizing the image and text features\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        # Computing the similaritys\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits # returns [batch_size, num_classes] this later gets passed into the training"
      ],
      "metadata": {
        "id": "a-VCqt-PUfvh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the CLIP model"
      ],
      "metadata": {
        "id": "43fMssvLUlh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomCLIP(classnames=classnames, clip_model=clip_model).to(device) # Initialize the object from the CustomClip class above which takes the classnames and model as an input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3X_wXD4Un1-",
        "outputId": "b8196e7e-4ab9-4224-9d3c-fbad6758a062"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set the hyperparameters for the model training"
      ],
      "metadata": {
        "id": "909jEGo-U2tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the hyperparameters as per the paper\n",
        "MAX_EPOCH = 200\n",
        "LR = 0.002\n",
        "optimizer = optim.SGD(model.prompt_learner.parameters(), lr=LR, momentum=0.9)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=MAX_EPOCH)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "u1PuAm_fU8Vc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to save the model weights"
      ],
      "metadata": {
        "id": "S5tUymvpVEeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to save the trained model\n",
        "def save_prompt_learner(model, model_name, dataset_name):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save({\"state_dict\": model.prompt_learner.state_dict()}, path)\n",
        "    print(f\"Prompt learner saved to {path}\")"
      ],
      "metadata": {
        "id": "7fdC98i2VHHR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop"
      ],
      "metadata": {
        "id": "PCdgHNs8VJTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "PRINT_FREQ = 5\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    # puts the model in the training mode\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    #iterate through train_loader batches\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # zero the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        logits = model(images)\n",
        "        # compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "        # backpropagrate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss, compute results\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i + 1) % PRINT_FREQ == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{MAX_EPOCH}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc = 100. * correct / total #accuracy of the model\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    scheduler.step()\n",
        "# set the model in the evaluation mode, over the validation dataset and get predictions\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {100.0 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVgWujNwVLEv",
        "outputId": "33abf73a-a7c9-4365-8e7b-db53c088a553"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Batch [5/50], Loss: 0.2067\n",
            "Epoch [1/200], Batch [10/50], Loss: 0.3140\n",
            "Epoch [1/200], Batch [15/50], Loss: 0.2311\n",
            "Epoch [1/200], Batch [20/50], Loss: 0.1378\n",
            "Epoch [1/200], Batch [25/50], Loss: 0.2430\n",
            "Epoch [1/200], Batch [30/50], Loss: 0.1271\n",
            "Epoch [1/200], Batch [35/50], Loss: 0.2576\n",
            "Epoch [1/200], Batch [40/50], Loss: 0.3303\n",
            "Epoch [1/200], Batch [45/50], Loss: 0.1059\n",
            "Epoch [1/200], Batch [50/50], Loss: 0.3362\n",
            "Epoch 1: Train Loss = 15.6702, Accuracy = 91.00%\n",
            "Epoch [2/200], Batch [5/50], Loss: 0.1598\n",
            "Epoch [2/200], Batch [10/50], Loss: 0.1493\n",
            "Epoch [2/200], Batch [15/50], Loss: 0.1410\n",
            "Epoch [2/200], Batch [20/50], Loss: 0.1443\n",
            "Epoch [2/200], Batch [25/50], Loss: 0.1561\n",
            "Epoch [2/200], Batch [30/50], Loss: 0.1304\n",
            "Epoch [2/200], Batch [35/50], Loss: 0.1442\n",
            "Epoch [2/200], Batch [40/50], Loss: 0.0900\n",
            "Epoch [2/200], Batch [45/50], Loss: 0.0264\n",
            "Epoch [2/200], Batch [50/50], Loss: 0.4324\n",
            "Epoch 2: Train Loss = 9.3094, Accuracy = 93.31%\n",
            "Epoch [3/200], Batch [5/50], Loss: 0.0900\n",
            "Epoch [3/200], Batch [10/50], Loss: 0.1836\n",
            "Epoch [3/200], Batch [15/50], Loss: 0.1411\n",
            "Epoch [3/200], Batch [20/50], Loss: 0.2629\n",
            "Epoch [3/200], Batch [25/50], Loss: 0.0364\n",
            "Epoch [3/200], Batch [30/50], Loss: 0.1259\n",
            "Epoch [3/200], Batch [35/50], Loss: 0.1016\n",
            "Epoch [3/200], Batch [40/50], Loss: 0.1278\n",
            "Epoch [3/200], Batch [45/50], Loss: 0.1544\n",
            "Epoch [3/200], Batch [50/50], Loss: 0.3020\n",
            "Epoch 3: Train Loss = 7.9020, Accuracy = 94.62%\n",
            "Epoch [4/200], Batch [5/50], Loss: 0.1231\n",
            "Epoch [4/200], Batch [10/50], Loss: 0.0180\n",
            "Epoch [4/200], Batch [15/50], Loss: 0.0323\n",
            "Epoch [4/200], Batch [20/50], Loss: 0.1935\n",
            "Epoch [4/200], Batch [25/50], Loss: 0.1035\n",
            "Epoch [4/200], Batch [30/50], Loss: 0.1035\n",
            "Epoch [4/200], Batch [35/50], Loss: 0.3386\n",
            "Epoch [4/200], Batch [40/50], Loss: 0.1340\n",
            "Epoch [4/200], Batch [45/50], Loss: 0.1639\n",
            "Epoch [4/200], Batch [50/50], Loss: 0.0686\n",
            "Epoch 4: Train Loss = 7.1271, Accuracy = 95.00%\n",
            "Epoch [5/200], Batch [5/50], Loss: 0.0548\n",
            "Epoch [5/200], Batch [10/50], Loss: 0.3359\n",
            "Epoch [5/200], Batch [15/50], Loss: 0.0584\n",
            "Epoch [5/200], Batch [20/50], Loss: 0.1205\n",
            "Epoch [5/200], Batch [25/50], Loss: 0.1035\n",
            "Epoch [5/200], Batch [30/50], Loss: 0.0922\n",
            "Epoch [5/200], Batch [35/50], Loss: 0.1252\n",
            "Epoch [5/200], Batch [40/50], Loss: 0.3730\n",
            "Epoch [5/200], Batch [45/50], Loss: 0.0690\n",
            "Epoch [5/200], Batch [50/50], Loss: 0.0529\n",
            "Epoch 5: Train Loss = 6.2746, Accuracy = 95.62%\n",
            "Epoch [6/200], Batch [5/50], Loss: 0.0923\n",
            "Epoch [6/200], Batch [10/50], Loss: 0.1011\n",
            "Epoch [6/200], Batch [15/50], Loss: 0.2769\n",
            "Epoch [6/200], Batch [20/50], Loss: 0.0474\n",
            "Epoch [6/200], Batch [25/50], Loss: 0.0487\n",
            "Epoch [6/200], Batch [30/50], Loss: 0.0813\n",
            "Epoch [6/200], Batch [35/50], Loss: 0.0574\n",
            "Epoch [6/200], Batch [40/50], Loss: 0.0456\n",
            "Epoch [6/200], Batch [45/50], Loss: 0.0820\n",
            "Epoch [6/200], Batch [50/50], Loss: 0.0717\n",
            "Epoch 6: Train Loss = 5.7676, Accuracy = 96.00%\n",
            "Epoch [7/200], Batch [5/50], Loss: 0.0582\n",
            "Epoch [7/200], Batch [10/50], Loss: 0.0533\n",
            "Epoch [7/200], Batch [15/50], Loss: 0.1211\n",
            "Epoch [7/200], Batch [20/50], Loss: 0.1047\n",
            "Epoch [7/200], Batch [25/50], Loss: 0.1006\n",
            "Epoch [7/200], Batch [30/50], Loss: 0.1396\n",
            "Epoch [7/200], Batch [35/50], Loss: 0.1047\n",
            "Epoch [7/200], Batch [40/50], Loss: 0.0127\n",
            "Epoch [7/200], Batch [45/50], Loss: 0.1170\n",
            "Epoch [7/200], Batch [50/50], Loss: 0.1677\n",
            "Epoch 7: Train Loss = 5.0463, Accuracy = 96.69%\n",
            "Epoch [8/200], Batch [5/50], Loss: 0.2140\n",
            "Epoch [8/200], Batch [10/50], Loss: 0.1760\n",
            "Epoch [8/200], Batch [15/50], Loss: 0.1410\n",
            "Epoch [8/200], Batch [20/50], Loss: 0.0954\n",
            "Epoch [8/200], Batch [25/50], Loss: 0.0764\n",
            "Epoch [8/200], Batch [30/50], Loss: 0.0469\n",
            "Epoch [8/200], Batch [35/50], Loss: 0.1416\n",
            "Epoch [8/200], Batch [40/50], Loss: 0.0361\n",
            "Epoch [8/200], Batch [45/50], Loss: 0.0528\n",
            "Epoch [8/200], Batch [50/50], Loss: 0.0452\n",
            "Epoch 8: Train Loss = 4.7134, Accuracy = 97.06%\n",
            "Epoch [9/200], Batch [5/50], Loss: 0.1132\n",
            "Epoch [9/200], Batch [10/50], Loss: 0.0616\n",
            "Epoch [9/200], Batch [15/50], Loss: 0.0091\n",
            "Epoch [9/200], Batch [20/50], Loss: 0.0610\n",
            "Epoch [9/200], Batch [25/50], Loss: 0.0271\n",
            "Epoch [9/200], Batch [30/50], Loss: 0.0843\n",
            "Epoch [9/200], Batch [35/50], Loss: 0.1047\n",
            "Epoch [9/200], Batch [40/50], Loss: 0.0334\n",
            "Epoch [9/200], Batch [45/50], Loss: 0.0897\n",
            "Epoch [9/200], Batch [50/50], Loss: 0.1945\n",
            "Epoch 9: Train Loss = 4.3497, Accuracy = 97.06%\n",
            "Epoch [10/200], Batch [5/50], Loss: 0.0126\n",
            "Epoch [10/200], Batch [10/50], Loss: 0.1384\n",
            "Epoch [10/200], Batch [15/50], Loss: 0.0585\n",
            "Epoch [10/200], Batch [20/50], Loss: 0.1033\n",
            "Epoch [10/200], Batch [25/50], Loss: 0.0410\n",
            "Epoch [10/200], Batch [30/50], Loss: 0.0455\n",
            "Epoch [10/200], Batch [35/50], Loss: 0.1133\n",
            "Epoch [10/200], Batch [40/50], Loss: 0.0343\n",
            "Epoch [10/200], Batch [45/50], Loss: 0.0298\n",
            "Epoch [10/200], Batch [50/50], Loss: 0.0416\n",
            "Epoch 10: Train Loss = 3.8583, Accuracy = 97.44%\n",
            "Epoch [11/200], Batch [5/50], Loss: 0.0632\n",
            "Epoch [11/200], Batch [10/50], Loss: 0.0838\n",
            "Epoch [11/200], Batch [15/50], Loss: 0.0967\n",
            "Epoch [11/200], Batch [20/50], Loss: 0.0753\n",
            "Epoch [11/200], Batch [25/50], Loss: 0.0710\n",
            "Epoch [11/200], Batch [30/50], Loss: 0.1302\n",
            "Epoch [11/200], Batch [35/50], Loss: 0.0630\n",
            "Epoch [11/200], Batch [40/50], Loss: 0.0776\n",
            "Epoch [11/200], Batch [45/50], Loss: 0.0114\n",
            "Epoch [11/200], Batch [50/50], Loss: 0.0890\n",
            "Epoch 11: Train Loss = 3.6107, Accuracy = 98.38%\n",
            "Epoch [12/200], Batch [5/50], Loss: 0.1383\n",
            "Epoch [12/200], Batch [10/50], Loss: 0.0581\n",
            "Epoch [12/200], Batch [15/50], Loss: 0.0564\n",
            "Epoch [12/200], Batch [20/50], Loss: 0.1130\n",
            "Epoch [12/200], Batch [25/50], Loss: 0.0350\n",
            "Epoch [12/200], Batch [30/50], Loss: 0.0249\n",
            "Epoch [12/200], Batch [35/50], Loss: 0.0298\n",
            "Epoch [12/200], Batch [40/50], Loss: 0.1030\n",
            "Epoch [12/200], Batch [45/50], Loss: 0.0903\n",
            "Epoch [12/200], Batch [50/50], Loss: 0.0248\n",
            "Epoch 12: Train Loss = 3.2522, Accuracy = 98.25%\n",
            "Epoch [13/200], Batch [5/50], Loss: 0.0467\n",
            "Epoch [13/200], Batch [10/50], Loss: 0.0415\n",
            "Epoch [13/200], Batch [15/50], Loss: 0.0656\n",
            "Epoch [13/200], Batch [20/50], Loss: 0.0817\n",
            "Epoch [13/200], Batch [25/50], Loss: 0.0720\n",
            "Epoch [13/200], Batch [30/50], Loss: 0.1064\n",
            "Epoch [13/200], Batch [35/50], Loss: 0.0297\n",
            "Epoch [13/200], Batch [40/50], Loss: 0.0400\n",
            "Epoch [13/200], Batch [45/50], Loss: 0.0984\n",
            "Epoch [13/200], Batch [50/50], Loss: 0.0719\n",
            "Epoch 13: Train Loss = 3.1102, Accuracy = 98.06%\n",
            "Epoch [14/200], Batch [5/50], Loss: 0.0744\n",
            "Epoch [14/200], Batch [10/50], Loss: 0.0335\n",
            "Epoch [14/200], Batch [15/50], Loss: 0.0315\n",
            "Epoch [14/200], Batch [20/50], Loss: 0.0330\n",
            "Epoch [14/200], Batch [25/50], Loss: 0.0421\n",
            "Epoch [14/200], Batch [30/50], Loss: 0.0538\n",
            "Epoch [14/200], Batch [35/50], Loss: 0.0949\n",
            "Epoch [14/200], Batch [40/50], Loss: 0.0217\n",
            "Epoch [14/200], Batch [45/50], Loss: 0.0415\n",
            "Epoch [14/200], Batch [50/50], Loss: 0.0420\n",
            "Epoch 14: Train Loss = 2.8553, Accuracy = 98.31%\n",
            "Epoch [15/200], Batch [5/50], Loss: 0.0091\n",
            "Epoch [15/200], Batch [10/50], Loss: 0.0731\n",
            "Epoch [15/200], Batch [15/50], Loss: 0.0794\n",
            "Epoch [15/200], Batch [20/50], Loss: 0.0267\n",
            "Epoch [15/200], Batch [25/50], Loss: 0.0781\n",
            "Epoch [15/200], Batch [30/50], Loss: 0.0341\n",
            "Epoch [15/200], Batch [35/50], Loss: 0.0820\n",
            "Epoch [15/200], Batch [40/50], Loss: 0.0945\n",
            "Epoch [15/200], Batch [45/50], Loss: 0.0570\n",
            "Epoch [15/200], Batch [50/50], Loss: 0.0554\n",
            "Epoch 15: Train Loss = 2.7922, Accuracy = 98.50%\n",
            "Epoch [16/200], Batch [5/50], Loss: 0.0455\n",
            "Epoch [16/200], Batch [10/50], Loss: 0.0257\n",
            "Epoch [16/200], Batch [15/50], Loss: 0.0927\n",
            "Epoch [16/200], Batch [20/50], Loss: 0.0207\n",
            "Epoch [16/200], Batch [25/50], Loss: 0.0149\n",
            "Epoch [16/200], Batch [30/50], Loss: 0.0362\n",
            "Epoch [16/200], Batch [35/50], Loss: 0.1028\n",
            "Epoch [16/200], Batch [40/50], Loss: 0.0232\n",
            "Epoch [16/200], Batch [45/50], Loss: 0.0142\n",
            "Epoch [16/200], Batch [50/50], Loss: 0.0857\n",
            "Epoch 16: Train Loss = 2.5505, Accuracy = 98.62%\n",
            "Epoch [17/200], Batch [5/50], Loss: 0.0121\n",
            "Epoch [17/200], Batch [10/50], Loss: 0.0195\n",
            "Epoch [17/200], Batch [15/50], Loss: 0.0410\n",
            "Epoch [17/200], Batch [20/50], Loss: 0.0262\n",
            "Epoch [17/200], Batch [25/50], Loss: 0.1068\n",
            "Epoch [17/200], Batch [30/50], Loss: 0.0578\n",
            "Epoch [17/200], Batch [35/50], Loss: 0.0608\n",
            "Epoch [17/200], Batch [40/50], Loss: 0.0566\n",
            "Epoch [17/200], Batch [45/50], Loss: 0.0580\n",
            "Epoch [17/200], Batch [50/50], Loss: 0.0365\n",
            "Epoch 17: Train Loss = 2.5910, Accuracy = 98.75%\n",
            "Epoch [18/200], Batch [5/50], Loss: 0.0250\n",
            "Epoch [18/200], Batch [10/50], Loss: 0.0999\n",
            "Epoch [18/200], Batch [15/50], Loss: 0.1013\n",
            "Epoch [18/200], Batch [20/50], Loss: 0.0319\n",
            "Epoch [18/200], Batch [25/50], Loss: 0.0334\n",
            "Epoch [18/200], Batch [30/50], Loss: 0.1024\n",
            "Epoch [18/200], Batch [35/50], Loss: 0.1158\n",
            "Epoch [18/200], Batch [40/50], Loss: 0.0748\n",
            "Epoch [18/200], Batch [45/50], Loss: 0.0374\n",
            "Epoch [18/200], Batch [50/50], Loss: 0.0643\n",
            "Epoch 18: Train Loss = 2.5656, Accuracy = 98.56%\n",
            "Epoch [19/200], Batch [5/50], Loss: 0.0210\n",
            "Epoch [19/200], Batch [10/50], Loss: 0.0570\n",
            "Epoch [19/200], Batch [15/50], Loss: 0.0550\n",
            "Epoch [19/200], Batch [20/50], Loss: 0.0136\n",
            "Epoch [19/200], Batch [25/50], Loss: 0.0624\n",
            "Epoch [19/200], Batch [30/50], Loss: 0.0115\n",
            "Epoch [19/200], Batch [35/50], Loss: 0.0711\n",
            "Epoch [19/200], Batch [40/50], Loss: 0.0121\n",
            "Epoch [19/200], Batch [45/50], Loss: 0.0618\n",
            "Epoch [19/200], Batch [50/50], Loss: 0.0280\n",
            "Epoch 19: Train Loss = 2.2947, Accuracy = 99.00%\n",
            "Epoch [20/200], Batch [5/50], Loss: 0.0300\n",
            "Epoch [20/200], Batch [10/50], Loss: 0.0656\n",
            "Epoch [20/200], Batch [15/50], Loss: 0.0432\n",
            "Epoch [20/200], Batch [20/50], Loss: 0.0498\n",
            "Epoch [20/200], Batch [25/50], Loss: 0.0204\n",
            "Epoch [20/200], Batch [30/50], Loss: 0.0329\n",
            "Epoch [20/200], Batch [35/50], Loss: 0.0483\n",
            "Epoch [20/200], Batch [40/50], Loss: 0.0029\n",
            "Epoch [20/200], Batch [45/50], Loss: 0.0352\n",
            "Epoch [20/200], Batch [50/50], Loss: 0.0159\n",
            "Epoch 20: Train Loss = 2.0007, Accuracy = 99.25%\n",
            "Epoch [21/200], Batch [5/50], Loss: 0.0432\n",
            "Epoch [21/200], Batch [10/50], Loss: 0.0266\n",
            "Epoch [21/200], Batch [15/50], Loss: 0.0671\n",
            "Epoch [21/200], Batch [20/50], Loss: 0.0478\n",
            "Epoch [21/200], Batch [25/50], Loss: 0.0859\n",
            "Epoch [21/200], Batch [30/50], Loss: 0.0233\n",
            "Epoch [21/200], Batch [35/50], Loss: 0.0555\n",
            "Epoch [21/200], Batch [40/50], Loss: 0.0939\n",
            "Epoch [21/200], Batch [45/50], Loss: 0.0286\n",
            "Epoch [21/200], Batch [50/50], Loss: 0.0689\n",
            "Epoch 21: Train Loss = 2.1137, Accuracy = 98.94%\n",
            "Epoch [22/200], Batch [5/50], Loss: 0.0265\n",
            "Epoch [22/200], Batch [10/50], Loss: 0.0337\n",
            "Epoch [22/200], Batch [15/50], Loss: 0.0732\n",
            "Epoch [22/200], Batch [20/50], Loss: 0.0361\n",
            "Epoch [22/200], Batch [25/50], Loss: 0.0375\n",
            "Epoch [22/200], Batch [30/50], Loss: 0.0112\n",
            "Epoch [22/200], Batch [35/50], Loss: 0.0390\n",
            "Epoch [22/200], Batch [40/50], Loss: 0.0709\n",
            "Epoch [22/200], Batch [45/50], Loss: 0.0090\n",
            "Epoch [22/200], Batch [50/50], Loss: 0.0419\n",
            "Epoch 22: Train Loss = 1.8823, Accuracy = 99.25%\n",
            "Epoch [23/200], Batch [5/50], Loss: 0.0864\n",
            "Epoch [23/200], Batch [10/50], Loss: 0.0535\n",
            "Epoch [23/200], Batch [15/50], Loss: 0.0247\n",
            "Epoch [23/200], Batch [20/50], Loss: 0.0804\n",
            "Epoch [23/200], Batch [25/50], Loss: 0.0163\n",
            "Epoch [23/200], Batch [30/50], Loss: 0.0732\n",
            "Epoch [23/200], Batch [35/50], Loss: 0.0604\n",
            "Epoch [23/200], Batch [40/50], Loss: 0.0592\n",
            "Epoch [23/200], Batch [45/50], Loss: 0.0531\n",
            "Epoch [23/200], Batch [50/50], Loss: 0.0518\n",
            "Epoch 23: Train Loss = 1.8381, Accuracy = 99.06%\n",
            "Epoch [24/200], Batch [5/50], Loss: 0.0705\n",
            "Epoch [24/200], Batch [10/50], Loss: 0.0255\n",
            "Epoch [24/200], Batch [15/50], Loss: 0.0172\n",
            "Epoch [24/200], Batch [20/50], Loss: 0.0284\n",
            "Epoch [24/200], Batch [25/50], Loss: 0.0929\n",
            "Epoch [24/200], Batch [30/50], Loss: 0.0563\n",
            "Epoch [24/200], Batch [35/50], Loss: 0.0423\n",
            "Epoch [24/200], Batch [40/50], Loss: 0.0395\n",
            "Epoch [24/200], Batch [45/50], Loss: 0.0168\n",
            "Epoch [24/200], Batch [50/50], Loss: 0.0471\n",
            "Epoch 24: Train Loss = 1.8134, Accuracy = 99.38%\n",
            "Epoch [25/200], Batch [5/50], Loss: 0.0151\n",
            "Epoch [25/200], Batch [10/50], Loss: 0.0266\n",
            "Epoch [25/200], Batch [15/50], Loss: 0.0583\n",
            "Epoch [25/200], Batch [20/50], Loss: 0.0530\n",
            "Epoch [25/200], Batch [25/50], Loss: 0.0614\n",
            "Epoch [25/200], Batch [30/50], Loss: 0.0729\n",
            "Epoch [25/200], Batch [35/50], Loss: 0.0097\n",
            "Epoch [25/200], Batch [40/50], Loss: 0.0348\n",
            "Epoch [25/200], Batch [45/50], Loss: 0.0343\n",
            "Epoch [25/200], Batch [50/50], Loss: 0.0589\n",
            "Epoch 25: Train Loss = 1.8684, Accuracy = 98.94%\n",
            "Epoch [26/200], Batch [5/50], Loss: 0.0308\n",
            "Epoch [26/200], Batch [10/50], Loss: 0.0181\n",
            "Epoch [26/200], Batch [15/50], Loss: 0.0112\n",
            "Epoch [26/200], Batch [20/50], Loss: 0.0172\n",
            "Epoch [26/200], Batch [25/50], Loss: 0.0241\n",
            "Epoch [26/200], Batch [30/50], Loss: 0.0844\n",
            "Epoch [26/200], Batch [35/50], Loss: 0.0251\n",
            "Epoch [26/200], Batch [40/50], Loss: 0.0340\n",
            "Epoch [26/200], Batch [45/50], Loss: 0.0275\n",
            "Epoch [26/200], Batch [50/50], Loss: 0.0222\n",
            "Epoch 26: Train Loss = 1.7267, Accuracy = 99.25%\n",
            "Epoch [27/200], Batch [5/50], Loss: 0.0424\n",
            "Epoch [27/200], Batch [10/50], Loss: 0.0169\n",
            "Epoch [27/200], Batch [15/50], Loss: 0.0529\n",
            "Epoch [27/200], Batch [20/50], Loss: 0.0184\n",
            "Epoch [27/200], Batch [25/50], Loss: 0.0146\n",
            "Epoch [27/200], Batch [30/50], Loss: 0.0061\n",
            "Epoch [27/200], Batch [35/50], Loss: 0.0270\n",
            "Epoch [27/200], Batch [40/50], Loss: 0.0528\n",
            "Epoch [27/200], Batch [45/50], Loss: 0.0351\n",
            "Epoch [27/200], Batch [50/50], Loss: 0.0041\n",
            "Epoch 27: Train Loss = 1.6129, Accuracy = 99.38%\n",
            "Epoch [28/200], Batch [5/50], Loss: 0.0309\n",
            "Epoch [28/200], Batch [10/50], Loss: 0.0235\n",
            "Epoch [28/200], Batch [15/50], Loss: 0.0153\n",
            "Epoch [28/200], Batch [20/50], Loss: 0.0406\n",
            "Epoch [28/200], Batch [25/50], Loss: 0.0455\n",
            "Epoch [28/200], Batch [30/50], Loss: 0.0749\n",
            "Epoch [28/200], Batch [35/50], Loss: 0.0111\n",
            "Epoch [28/200], Batch [40/50], Loss: 0.0261\n",
            "Epoch [28/200], Batch [45/50], Loss: 0.0246\n",
            "Epoch [28/200], Batch [50/50], Loss: 0.0140\n",
            "Epoch 28: Train Loss = 1.5413, Accuracy = 99.19%\n",
            "Epoch [29/200], Batch [5/50], Loss: 0.0184\n",
            "Epoch [29/200], Batch [10/50], Loss: 0.0219\n",
            "Epoch [29/200], Batch [15/50], Loss: 0.0316\n",
            "Epoch [29/200], Batch [20/50], Loss: 0.0640\n",
            "Epoch [29/200], Batch [25/50], Loss: 0.0098\n",
            "Epoch [29/200], Batch [30/50], Loss: 0.0325\n",
            "Epoch [29/200], Batch [35/50], Loss: 0.0336\n",
            "Epoch [29/200], Batch [40/50], Loss: 0.0417\n",
            "Epoch [29/200], Batch [45/50], Loss: 0.0260\n",
            "Epoch [29/200], Batch [50/50], Loss: 0.0092\n",
            "Epoch 29: Train Loss = 1.4135, Accuracy = 99.50%\n",
            "Epoch [30/200], Batch [5/50], Loss: 0.0063\n",
            "Epoch [30/200], Batch [10/50], Loss: 0.0290\n",
            "Epoch [30/200], Batch [15/50], Loss: 0.0091\n",
            "Epoch [30/200], Batch [20/50], Loss: 0.0502\n",
            "Epoch [30/200], Batch [25/50], Loss: 0.0250\n",
            "Epoch [30/200], Batch [30/50], Loss: 0.0120\n",
            "Epoch [30/200], Batch [35/50], Loss: 0.0315\n",
            "Epoch [30/200], Batch [40/50], Loss: 0.0255\n",
            "Epoch [30/200], Batch [45/50], Loss: 0.0179\n",
            "Epoch [30/200], Batch [50/50], Loss: 0.0446\n",
            "Epoch 30: Train Loss = 1.3678, Accuracy = 99.69%\n",
            "Epoch [31/200], Batch [5/50], Loss: 0.0414\n",
            "Epoch [31/200], Batch [10/50], Loss: 0.0135\n",
            "Epoch [31/200], Batch [15/50], Loss: 0.0411\n",
            "Epoch [31/200], Batch [20/50], Loss: 0.0419\n",
            "Epoch [31/200], Batch [25/50], Loss: 0.0117\n",
            "Epoch [31/200], Batch [30/50], Loss: 0.0284\n",
            "Epoch [31/200], Batch [35/50], Loss: 0.0199\n",
            "Epoch [31/200], Batch [40/50], Loss: 0.0339\n",
            "Epoch [31/200], Batch [45/50], Loss: 0.0531\n",
            "Epoch [31/200], Batch [50/50], Loss: 0.0480\n",
            "Epoch 31: Train Loss = 1.4254, Accuracy = 99.38%\n",
            "Epoch [32/200], Batch [5/50], Loss: 0.0181\n",
            "Epoch [32/200], Batch [10/50], Loss: 0.0193\n",
            "Epoch [32/200], Batch [15/50], Loss: 0.0887\n",
            "Epoch [32/200], Batch [20/50], Loss: 0.0136\n",
            "Epoch [32/200], Batch [25/50], Loss: 0.0298\n",
            "Epoch [32/200], Batch [30/50], Loss: 0.0127\n",
            "Epoch [32/200], Batch [35/50], Loss: 0.0119\n",
            "Epoch [32/200], Batch [40/50], Loss: 0.0197\n",
            "Epoch [32/200], Batch [45/50], Loss: 0.0200\n",
            "Epoch [32/200], Batch [50/50], Loss: 0.0401\n",
            "Epoch 32: Train Loss = 1.4003, Accuracy = 99.56%\n",
            "Epoch [33/200], Batch [5/50], Loss: 0.0157\n",
            "Epoch [33/200], Batch [10/50], Loss: 0.0183\n",
            "Epoch [33/200], Batch [15/50], Loss: 0.0221\n",
            "Epoch [33/200], Batch [20/50], Loss: 0.0550\n",
            "Epoch [33/200], Batch [25/50], Loss: 0.0386\n",
            "Epoch [33/200], Batch [30/50], Loss: 0.0309\n",
            "Epoch [33/200], Batch [35/50], Loss: 0.0170\n",
            "Epoch [33/200], Batch [40/50], Loss: 0.0176\n",
            "Epoch [33/200], Batch [45/50], Loss: 0.0589\n",
            "Epoch [33/200], Batch [50/50], Loss: 0.0076\n",
            "Epoch 33: Train Loss = 1.4219, Accuracy = 99.56%\n",
            "Epoch [34/200], Batch [5/50], Loss: 0.0124\n",
            "Epoch [34/200], Batch [10/50], Loss: 0.0062\n",
            "Epoch [34/200], Batch [15/50], Loss: 0.0235\n",
            "Epoch [34/200], Batch [20/50], Loss: 0.0508\n",
            "Epoch [34/200], Batch [25/50], Loss: 0.0070\n",
            "Epoch [34/200], Batch [30/50], Loss: 0.0348\n",
            "Epoch [34/200], Batch [35/50], Loss: 0.0078\n",
            "Epoch [34/200], Batch [40/50], Loss: 0.0218\n",
            "Epoch [34/200], Batch [45/50], Loss: 0.0219\n",
            "Epoch [34/200], Batch [50/50], Loss: 0.0637\n",
            "Epoch 34: Train Loss = 1.3249, Accuracy = 99.69%\n",
            "Epoch [35/200], Batch [5/50], Loss: 0.0192\n",
            "Epoch [35/200], Batch [10/50], Loss: 0.0269\n",
            "Epoch [35/200], Batch [15/50], Loss: 0.0478\n",
            "Epoch [35/200], Batch [20/50], Loss: 0.0078\n",
            "Epoch [35/200], Batch [25/50], Loss: 0.0141\n",
            "Epoch [35/200], Batch [30/50], Loss: 0.0275\n",
            "Epoch [35/200], Batch [35/50], Loss: 0.0129\n",
            "Epoch [35/200], Batch [40/50], Loss: 0.0261\n",
            "Epoch [35/200], Batch [45/50], Loss: 0.0117\n",
            "Epoch [35/200], Batch [50/50], Loss: 0.0246\n",
            "Epoch 35: Train Loss = 1.1781, Accuracy = 99.69%\n",
            "Epoch [36/200], Batch [5/50], Loss: 0.0149\n",
            "Epoch [36/200], Batch [10/50], Loss: 0.0146\n",
            "Epoch [36/200], Batch [15/50], Loss: 0.0213\n",
            "Epoch [36/200], Batch [20/50], Loss: 0.0233\n",
            "Epoch [36/200], Batch [25/50], Loss: 0.0170\n",
            "Epoch [36/200], Batch [30/50], Loss: 0.0105\n",
            "Epoch [36/200], Batch [35/50], Loss: 0.0227\n",
            "Epoch [36/200], Batch [40/50], Loss: 0.0316\n",
            "Epoch [36/200], Batch [45/50], Loss: 0.0639\n",
            "Epoch [36/200], Batch [50/50], Loss: 0.0212\n",
            "Epoch 36: Train Loss = 1.2440, Accuracy = 99.75%\n",
            "Epoch [37/200], Batch [5/50], Loss: 0.0449\n",
            "Epoch [37/200], Batch [10/50], Loss: 0.0307\n",
            "Epoch [37/200], Batch [15/50], Loss: 0.0121\n",
            "Epoch [37/200], Batch [20/50], Loss: 0.0223\n",
            "Epoch [37/200], Batch [25/50], Loss: 0.0280\n",
            "Epoch [37/200], Batch [30/50], Loss: 0.0238\n",
            "Epoch [37/200], Batch [35/50], Loss: 0.0199\n",
            "Epoch [37/200], Batch [40/50], Loss: 0.0141\n",
            "Epoch [37/200], Batch [45/50], Loss: 0.0122\n",
            "Epoch [37/200], Batch [50/50], Loss: 0.0192\n",
            "Epoch 37: Train Loss = 1.1303, Accuracy = 99.81%\n",
            "Epoch [38/200], Batch [5/50], Loss: 0.0094\n",
            "Epoch [38/200], Batch [10/50], Loss: 0.0205\n",
            "Epoch [38/200], Batch [15/50], Loss: 0.0124\n",
            "Epoch [38/200], Batch [20/50], Loss: 0.0199\n",
            "Epoch [38/200], Batch [25/50], Loss: 0.0187\n",
            "Epoch [38/200], Batch [30/50], Loss: 0.0191\n",
            "Epoch [38/200], Batch [35/50], Loss: 0.0257\n",
            "Epoch [38/200], Batch [40/50], Loss: 0.0165\n",
            "Epoch [38/200], Batch [45/50], Loss: 0.0507\n",
            "Epoch [38/200], Batch [50/50], Loss: 0.0091\n",
            "Epoch 38: Train Loss = 1.1561, Accuracy = 99.75%\n",
            "Epoch [39/200], Batch [5/50], Loss: 0.0177\n",
            "Epoch [39/200], Batch [10/50], Loss: 0.0251\n",
            "Epoch [39/200], Batch [15/50], Loss: 0.0121\n",
            "Epoch [39/200], Batch [20/50], Loss: 0.0235\n",
            "Epoch [39/200], Batch [25/50], Loss: 0.0064\n",
            "Epoch [39/200], Batch [30/50], Loss: 0.0061\n",
            "Epoch [39/200], Batch [35/50], Loss: 0.0164\n",
            "Epoch [39/200], Batch [40/50], Loss: 0.0264\n",
            "Epoch [39/200], Batch [45/50], Loss: 0.0118\n",
            "Epoch [39/200], Batch [50/50], Loss: 0.0383\n",
            "Epoch 39: Train Loss = 1.0546, Accuracy = 99.81%\n",
            "Epoch [40/200], Batch [5/50], Loss: 0.0189\n",
            "Epoch [40/200], Batch [10/50], Loss: 0.0152\n",
            "Epoch [40/200], Batch [15/50], Loss: 0.0100\n",
            "Epoch [40/200], Batch [20/50], Loss: 0.0129\n",
            "Epoch [40/200], Batch [25/50], Loss: 0.0411\n",
            "Epoch [40/200], Batch [30/50], Loss: 0.0102\n",
            "Epoch [40/200], Batch [35/50], Loss: 0.0438\n",
            "Epoch [40/200], Batch [40/50], Loss: 0.0130\n",
            "Epoch [40/200], Batch [45/50], Loss: 0.0262\n",
            "Epoch [40/200], Batch [50/50], Loss: 0.0207\n",
            "Epoch 40: Train Loss = 0.9947, Accuracy = 99.94%\n",
            "Epoch [41/200], Batch [5/50], Loss: 0.0148\n",
            "Epoch [41/200], Batch [10/50], Loss: 0.0086\n",
            "Epoch [41/200], Batch [15/50], Loss: 0.0272\n",
            "Epoch [41/200], Batch [20/50], Loss: 0.0099\n",
            "Epoch [41/200], Batch [25/50], Loss: 0.0136\n",
            "Epoch [41/200], Batch [30/50], Loss: 0.0281\n",
            "Epoch [41/200], Batch [35/50], Loss: 0.0034\n",
            "Epoch [41/200], Batch [40/50], Loss: 0.0254\n",
            "Epoch [41/200], Batch [45/50], Loss: 0.0072\n",
            "Epoch [41/200], Batch [50/50], Loss: 0.0121\n",
            "Epoch 41: Train Loss = 0.9620, Accuracy = 99.88%\n",
            "Epoch [42/200], Batch [5/50], Loss: 0.0100\n",
            "Epoch [42/200], Batch [10/50], Loss: 0.0277\n",
            "Epoch [42/200], Batch [15/50], Loss: 0.0164\n",
            "Epoch [42/200], Batch [20/50], Loss: 0.0224\n",
            "Epoch [42/200], Batch [25/50], Loss: 0.0363\n",
            "Epoch [42/200], Batch [30/50], Loss: 0.0303\n",
            "Epoch [42/200], Batch [35/50], Loss: 0.0080\n",
            "Epoch [42/200], Batch [40/50], Loss: 0.0116\n",
            "Epoch [42/200], Batch [45/50], Loss: 0.0258\n",
            "Epoch [42/200], Batch [50/50], Loss: 0.0120\n",
            "Epoch 42: Train Loss = 0.9703, Accuracy = 99.81%\n",
            "Epoch [43/200], Batch [5/50], Loss: 0.0234\n",
            "Epoch [43/200], Batch [10/50], Loss: 0.0085\n",
            "Epoch [43/200], Batch [15/50], Loss: 0.0137\n",
            "Epoch [43/200], Batch [20/50], Loss: 0.0475\n",
            "Epoch [43/200], Batch [25/50], Loss: 0.0121\n",
            "Epoch [43/200], Batch [30/50], Loss: 0.0170\n",
            "Epoch [43/200], Batch [35/50], Loss: 0.0230\n",
            "Epoch [43/200], Batch [40/50], Loss: 0.0222\n",
            "Epoch [43/200], Batch [45/50], Loss: 0.0089\n",
            "Epoch [43/200], Batch [50/50], Loss: 0.0703\n",
            "Epoch 43: Train Loss = 0.9465, Accuracy = 99.75%\n",
            "Epoch [44/200], Batch [5/50], Loss: 0.0205\n",
            "Epoch [44/200], Batch [10/50], Loss: 0.0097\n",
            "Epoch [44/200], Batch [15/50], Loss: 0.0167\n",
            "Epoch [44/200], Batch [20/50], Loss: 0.0519\n",
            "Epoch [44/200], Batch [25/50], Loss: 0.0082\n",
            "Epoch [44/200], Batch [30/50], Loss: 0.0322\n",
            "Epoch [44/200], Batch [35/50], Loss: 0.0300\n",
            "Epoch [44/200], Batch [40/50], Loss: 0.0105\n",
            "Epoch [44/200], Batch [45/50], Loss: 0.0145\n",
            "Epoch [44/200], Batch [50/50], Loss: 0.0127\n",
            "Epoch 44: Train Loss = 0.9873, Accuracy = 99.81%\n",
            "Epoch [45/200], Batch [5/50], Loss: 0.0078\n",
            "Epoch [45/200], Batch [10/50], Loss: 0.0132\n",
            "Epoch [45/200], Batch [15/50], Loss: 0.0128\n",
            "Epoch [45/200], Batch [20/50], Loss: 0.0147\n",
            "Epoch [45/200], Batch [25/50], Loss: 0.0245\n",
            "Epoch [45/200], Batch [30/50], Loss: 0.0295\n",
            "Epoch [45/200], Batch [35/50], Loss: 0.0182\n",
            "Epoch [45/200], Batch [40/50], Loss: 0.0249\n",
            "Epoch [45/200], Batch [45/50], Loss: 0.0111\n",
            "Epoch [45/200], Batch [50/50], Loss: 0.0096\n",
            "Epoch 45: Train Loss = 0.8852, Accuracy = 99.88%\n",
            "Epoch [46/200], Batch [5/50], Loss: 0.0144\n",
            "Epoch [46/200], Batch [10/50], Loss: 0.0199\n",
            "Epoch [46/200], Batch [15/50], Loss: 0.0122\n",
            "Epoch [46/200], Batch [20/50], Loss: 0.0108\n",
            "Epoch [46/200], Batch [25/50], Loss: 0.0113\n",
            "Epoch [46/200], Batch [30/50], Loss: 0.0162\n",
            "Epoch [46/200], Batch [35/50], Loss: 0.0496\n",
            "Epoch [46/200], Batch [40/50], Loss: 0.0083\n",
            "Epoch [46/200], Batch [45/50], Loss: 0.0291\n",
            "Epoch [46/200], Batch [50/50], Loss: 0.0399\n",
            "Epoch 46: Train Loss = 1.0364, Accuracy = 99.75%\n",
            "Epoch [47/200], Batch [5/50], Loss: 0.0197\n",
            "Epoch [47/200], Batch [10/50], Loss: 0.0180\n",
            "Epoch [47/200], Batch [15/50], Loss: 0.0128\n",
            "Epoch [47/200], Batch [20/50], Loss: 0.0207\n",
            "Epoch [47/200], Batch [25/50], Loss: 0.0178\n",
            "Epoch [47/200], Batch [30/50], Loss: 0.0275\n",
            "Epoch [47/200], Batch [35/50], Loss: 0.0351\n",
            "Epoch [47/200], Batch [40/50], Loss: 0.0083\n",
            "Epoch [47/200], Batch [45/50], Loss: 0.0247\n",
            "Epoch [47/200], Batch [50/50], Loss: 0.0214\n",
            "Epoch 47: Train Loss = 0.8604, Accuracy = 99.94%\n",
            "Epoch [48/200], Batch [5/50], Loss: 0.0191\n",
            "Epoch [48/200], Batch [10/50], Loss: 0.0099\n",
            "Epoch [48/200], Batch [15/50], Loss: 0.0105\n",
            "Epoch [48/200], Batch [20/50], Loss: 0.0074\n",
            "Epoch [48/200], Batch [25/50], Loss: 0.0157\n",
            "Epoch [48/200], Batch [30/50], Loss: 0.0181\n",
            "Epoch [48/200], Batch [35/50], Loss: 0.0127\n",
            "Epoch [48/200], Batch [40/50], Loss: 0.0131\n",
            "Epoch [48/200], Batch [45/50], Loss: 0.0077\n",
            "Epoch [48/200], Batch [50/50], Loss: 0.0171\n",
            "Epoch 48: Train Loss = 0.8053, Accuracy = 99.88%\n",
            "Epoch [49/200], Batch [5/50], Loss: 0.0131\n",
            "Epoch [49/200], Batch [10/50], Loss: 0.0113\n",
            "Epoch [49/200], Batch [15/50], Loss: 0.0480\n",
            "Epoch [49/200], Batch [20/50], Loss: 0.0190\n",
            "Epoch [49/200], Batch [25/50], Loss: 0.0138\n",
            "Epoch [49/200], Batch [30/50], Loss: 0.0137\n",
            "Epoch [49/200], Batch [35/50], Loss: 0.0215\n",
            "Epoch [49/200], Batch [40/50], Loss: 0.0516\n",
            "Epoch [49/200], Batch [45/50], Loss: 0.0487\n",
            "Epoch [49/200], Batch [50/50], Loss: 0.0209\n",
            "Epoch 49: Train Loss = 0.7640, Accuracy = 99.88%\n",
            "Epoch [50/200], Batch [5/50], Loss: 0.0113\n",
            "Epoch [50/200], Batch [10/50], Loss: 0.0199\n",
            "Epoch [50/200], Batch [15/50], Loss: 0.0207\n",
            "Epoch [50/200], Batch [20/50], Loss: 0.0214\n",
            "Epoch [50/200], Batch [25/50], Loss: 0.0202\n",
            "Epoch [50/200], Batch [30/50], Loss: 0.0142\n",
            "Epoch [50/200], Batch [35/50], Loss: 0.0071\n",
            "Epoch [50/200], Batch [40/50], Loss: 0.0287\n",
            "Epoch [50/200], Batch [45/50], Loss: 0.0155\n",
            "Epoch [50/200], Batch [50/50], Loss: 0.0196\n",
            "Epoch 50: Train Loss = 0.7849, Accuracy = 99.94%\n",
            "Epoch [51/200], Batch [5/50], Loss: 0.0117\n",
            "Epoch [51/200], Batch [10/50], Loss: 0.0149\n",
            "Epoch [51/200], Batch [15/50], Loss: 0.0219\n",
            "Epoch [51/200], Batch [20/50], Loss: 0.0191\n",
            "Epoch [51/200], Batch [25/50], Loss: 0.0081\n",
            "Epoch [51/200], Batch [30/50], Loss: 0.0077\n",
            "Epoch [51/200], Batch [35/50], Loss: 0.0135\n",
            "Epoch [51/200], Batch [40/50], Loss: 0.0079\n",
            "Epoch [51/200], Batch [45/50], Loss: 0.0101\n",
            "Epoch [51/200], Batch [50/50], Loss: 0.0123\n",
            "Epoch 51: Train Loss = 0.6892, Accuracy = 100.00%\n",
            "Epoch [52/200], Batch [5/50], Loss: 0.0232\n",
            "Epoch [52/200], Batch [10/50], Loss: 0.0169\n",
            "Epoch [52/200], Batch [15/50], Loss: 0.0303\n",
            "Epoch [52/200], Batch [20/50], Loss: 0.0226\n",
            "Epoch [52/200], Batch [25/50], Loss: 0.0121\n",
            "Epoch [52/200], Batch [30/50], Loss: 0.0208\n",
            "Epoch [52/200], Batch [35/50], Loss: 0.0042\n",
            "Epoch [52/200], Batch [40/50], Loss: 0.0099\n",
            "Epoch [52/200], Batch [45/50], Loss: 0.0247\n",
            "Epoch [52/200], Batch [50/50], Loss: 0.0374\n",
            "Epoch 52: Train Loss = 0.6687, Accuracy = 99.94%\n",
            "Epoch [53/200], Batch [5/50], Loss: 0.0164\n",
            "Epoch [53/200], Batch [10/50], Loss: 0.0098\n",
            "Epoch [53/200], Batch [15/50], Loss: 0.0103\n",
            "Epoch [53/200], Batch [20/50], Loss: 0.0111\n",
            "Epoch [53/200], Batch [25/50], Loss: 0.0072\n",
            "Epoch [53/200], Batch [30/50], Loss: 0.0203\n",
            "Epoch [53/200], Batch [35/50], Loss: 0.0144\n",
            "Epoch [53/200], Batch [40/50], Loss: 0.0150\n",
            "Epoch [53/200], Batch [45/50], Loss: 0.0100\n",
            "Epoch [53/200], Batch [50/50], Loss: 0.0111\n",
            "Epoch 53: Train Loss = 0.6948, Accuracy = 99.94%\n",
            "Epoch [54/200], Batch [5/50], Loss: 0.0849\n",
            "Epoch [54/200], Batch [10/50], Loss: 0.0171\n",
            "Epoch [54/200], Batch [15/50], Loss: 0.0190\n",
            "Epoch [54/200], Batch [20/50], Loss: 0.0222\n",
            "Epoch [54/200], Batch [25/50], Loss: 0.0057\n",
            "Epoch [54/200], Batch [30/50], Loss: 0.0323\n",
            "Epoch [54/200], Batch [35/50], Loss: 0.0053\n",
            "Epoch [54/200], Batch [40/50], Loss: 0.0221\n",
            "Epoch [54/200], Batch [45/50], Loss: 0.0142\n",
            "Epoch [54/200], Batch [50/50], Loss: 0.0106\n",
            "Epoch 54: Train Loss = 0.9405, Accuracy = 99.81%\n",
            "Epoch [55/200], Batch [5/50], Loss: 0.0164\n",
            "Epoch [55/200], Batch [10/50], Loss: 0.0106\n",
            "Epoch [55/200], Batch [15/50], Loss: 0.0242\n",
            "Epoch [55/200], Batch [20/50], Loss: 0.0079\n",
            "Epoch [55/200], Batch [25/50], Loss: 0.0188\n",
            "Epoch [55/200], Batch [30/50], Loss: 0.0051\n",
            "Epoch [55/200], Batch [35/50], Loss: 0.0096\n",
            "Epoch [55/200], Batch [40/50], Loss: 0.0105\n",
            "Epoch [55/200], Batch [45/50], Loss: 0.0096\n",
            "Epoch [55/200], Batch [50/50], Loss: 0.0092\n",
            "Epoch 55: Train Loss = 0.6899, Accuracy = 100.00%\n",
            "Epoch [56/200], Batch [5/50], Loss: 0.0086\n",
            "Epoch [56/200], Batch [10/50], Loss: 0.0097\n",
            "Epoch [56/200], Batch [15/50], Loss: 0.0143\n",
            "Epoch [56/200], Batch [20/50], Loss: 0.0138\n",
            "Epoch [56/200], Batch [25/50], Loss: 0.0067\n",
            "Epoch [56/200], Batch [30/50], Loss: 0.0118\n",
            "Epoch [56/200], Batch [35/50], Loss: 0.0105\n",
            "Epoch [56/200], Batch [40/50], Loss: 0.0255\n",
            "Epoch [56/200], Batch [45/50], Loss: 0.0044\n",
            "Epoch [56/200], Batch [50/50], Loss: 0.0106\n",
            "Epoch 56: Train Loss = 0.6297, Accuracy = 100.00%\n",
            "Epoch [57/200], Batch [5/50], Loss: 0.0156\n",
            "Epoch [57/200], Batch [10/50], Loss: 0.0123\n",
            "Epoch [57/200], Batch [15/50], Loss: 0.0116\n",
            "Epoch [57/200], Batch [20/50], Loss: 0.0296\n",
            "Epoch [57/200], Batch [25/50], Loss: 0.0170\n",
            "Epoch [57/200], Batch [30/50], Loss: 0.0291\n",
            "Epoch [57/200], Batch [35/50], Loss: 0.0108\n",
            "Epoch [57/200], Batch [40/50], Loss: 0.0185\n",
            "Epoch [57/200], Batch [45/50], Loss: 0.0218\n",
            "Epoch [57/200], Batch [50/50], Loss: 0.0307\n",
            "Epoch 57: Train Loss = 0.6738, Accuracy = 100.00%\n",
            "Epoch [58/200], Batch [5/50], Loss: 0.0085\n",
            "Epoch [58/200], Batch [10/50], Loss: 0.0120\n",
            "Epoch [58/200], Batch [15/50], Loss: 0.0075\n",
            "Epoch [58/200], Batch [20/50], Loss: 0.0114\n",
            "Epoch [58/200], Batch [25/50], Loss: 0.0050\n",
            "Epoch [58/200], Batch [30/50], Loss: 0.0144\n",
            "Epoch [58/200], Batch [35/50], Loss: 0.0086\n",
            "Epoch [58/200], Batch [40/50], Loss: 0.0083\n",
            "Epoch [58/200], Batch [45/50], Loss: 0.0100\n",
            "Epoch [58/200], Batch [50/50], Loss: 0.0144\n",
            "Epoch 58: Train Loss = 0.5854, Accuracy = 99.94%\n",
            "Epoch [59/200], Batch [5/50], Loss: 0.0128\n",
            "Epoch [59/200], Batch [10/50], Loss: 0.0192\n",
            "Epoch [59/200], Batch [15/50], Loss: 0.0147\n",
            "Epoch [59/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [59/200], Batch [25/50], Loss: 0.0133\n",
            "Epoch [59/200], Batch [30/50], Loss: 0.0145\n",
            "Epoch [59/200], Batch [35/50], Loss: 0.0092\n",
            "Epoch [59/200], Batch [40/50], Loss: 0.0224\n",
            "Epoch [59/200], Batch [45/50], Loss: 0.0065\n",
            "Epoch [59/200], Batch [50/50], Loss: 0.0119\n",
            "Epoch 59: Train Loss = 0.5881, Accuracy = 100.00%\n",
            "Epoch [60/200], Batch [5/50], Loss: 0.0070\n",
            "Epoch [60/200], Batch [10/50], Loss: 0.0077\n",
            "Epoch [60/200], Batch [15/50], Loss: 0.0130\n",
            "Epoch [60/200], Batch [20/50], Loss: 0.0422\n",
            "Epoch [60/200], Batch [25/50], Loss: 0.0059\n",
            "Epoch [60/200], Batch [30/50], Loss: 0.0105\n",
            "Epoch [60/200], Batch [35/50], Loss: 0.0059\n",
            "Epoch [60/200], Batch [40/50], Loss: 0.0082\n",
            "Epoch [60/200], Batch [45/50], Loss: 0.0056\n",
            "Epoch [60/200], Batch [50/50], Loss: 0.0118\n",
            "Epoch 60: Train Loss = 0.5988, Accuracy = 99.88%\n",
            "Epoch [61/200], Batch [5/50], Loss: 0.0116\n",
            "Epoch [61/200], Batch [10/50], Loss: 0.0072\n",
            "Epoch [61/200], Batch [15/50], Loss: 0.0074\n",
            "Epoch [61/200], Batch [20/50], Loss: 0.0230\n",
            "Epoch [61/200], Batch [25/50], Loss: 0.0074\n",
            "Epoch [61/200], Batch [30/50], Loss: 0.0251\n",
            "Epoch [61/200], Batch [35/50], Loss: 0.0122\n",
            "Epoch [61/200], Batch [40/50], Loss: 0.0096\n",
            "Epoch [61/200], Batch [45/50], Loss: 0.0219\n",
            "Epoch [61/200], Batch [50/50], Loss: 0.0146\n",
            "Epoch 61: Train Loss = 0.5898, Accuracy = 99.94%\n",
            "Epoch [62/200], Batch [5/50], Loss: 0.0302\n",
            "Epoch [62/200], Batch [10/50], Loss: 0.0192\n",
            "Epoch [62/200], Batch [15/50], Loss: 0.0083\n",
            "Epoch [62/200], Batch [20/50], Loss: 0.0099\n",
            "Epoch [62/200], Batch [25/50], Loss: 0.0084\n",
            "Epoch [62/200], Batch [30/50], Loss: 0.0202\n",
            "Epoch [62/200], Batch [35/50], Loss: 0.0157\n",
            "Epoch [62/200], Batch [40/50], Loss: 0.0030\n",
            "Epoch [62/200], Batch [45/50], Loss: 0.0058\n",
            "Epoch [62/200], Batch [50/50], Loss: 0.0078\n",
            "Epoch 62: Train Loss = 0.5733, Accuracy = 100.00%\n",
            "Epoch [63/200], Batch [5/50], Loss: 0.0091\n",
            "Epoch [63/200], Batch [10/50], Loss: 0.0089\n",
            "Epoch [63/200], Batch [15/50], Loss: 0.0112\n",
            "Epoch [63/200], Batch [20/50], Loss: 0.0144\n",
            "Epoch [63/200], Batch [25/50], Loss: 0.0128\n",
            "Epoch [63/200], Batch [30/50], Loss: 0.0067\n",
            "Epoch [63/200], Batch [35/50], Loss: 0.0118\n",
            "Epoch [63/200], Batch [40/50], Loss: 0.0240\n",
            "Epoch [63/200], Batch [45/50], Loss: 0.0079\n",
            "Epoch [63/200], Batch [50/50], Loss: 0.0172\n",
            "Epoch 63: Train Loss = 0.5060, Accuracy = 100.00%\n",
            "Epoch [64/200], Batch [5/50], Loss: 0.0048\n",
            "Epoch [64/200], Batch [10/50], Loss: 0.0056\n",
            "Epoch [64/200], Batch [15/50], Loss: 0.0169\n",
            "Epoch [64/200], Batch [20/50], Loss: 0.0073\n",
            "Epoch [64/200], Batch [25/50], Loss: 0.0019\n",
            "Epoch [64/200], Batch [30/50], Loss: 0.0072\n",
            "Epoch [64/200], Batch [35/50], Loss: 0.0142\n",
            "Epoch [64/200], Batch [40/50], Loss: 0.0056\n",
            "Epoch [64/200], Batch [45/50], Loss: 0.0058\n",
            "Epoch [64/200], Batch [50/50], Loss: 0.0074\n",
            "Epoch 64: Train Loss = 0.5212, Accuracy = 100.00%\n",
            "Epoch [65/200], Batch [5/50], Loss: 0.0051\n",
            "Epoch [65/200], Batch [10/50], Loss: 0.0104\n",
            "Epoch [65/200], Batch [15/50], Loss: 0.0043\n",
            "Epoch [65/200], Batch [20/50], Loss: 0.0124\n",
            "Epoch [65/200], Batch [25/50], Loss: 0.0066\n",
            "Epoch [65/200], Batch [30/50], Loss: 0.0164\n",
            "Epoch [65/200], Batch [35/50], Loss: 0.0064\n",
            "Epoch [65/200], Batch [40/50], Loss: 0.0058\n",
            "Epoch [65/200], Batch [45/50], Loss: 0.0073\n",
            "Epoch [65/200], Batch [50/50], Loss: 0.0069\n",
            "Epoch 65: Train Loss = 0.4967, Accuracy = 100.00%\n",
            "Epoch [66/200], Batch [5/50], Loss: 0.0071\n",
            "Epoch [66/200], Batch [10/50], Loss: 0.0089\n",
            "Epoch [66/200], Batch [15/50], Loss: 0.0197\n",
            "Epoch [66/200], Batch [20/50], Loss: 0.0084\n",
            "Epoch [66/200], Batch [25/50], Loss: 0.0154\n",
            "Epoch [66/200], Batch [30/50], Loss: 0.0100\n",
            "Epoch [66/200], Batch [35/50], Loss: 0.0098\n",
            "Epoch [66/200], Batch [40/50], Loss: 0.0081\n",
            "Epoch [66/200], Batch [45/50], Loss: 0.0153\n",
            "Epoch [66/200], Batch [50/50], Loss: 0.0043\n",
            "Epoch 66: Train Loss = 0.4902, Accuracy = 100.00%\n",
            "Epoch [67/200], Batch [5/50], Loss: 0.0059\n",
            "Epoch [67/200], Batch [10/50], Loss: 0.0088\n",
            "Epoch [67/200], Batch [15/50], Loss: 0.0047\n",
            "Epoch [67/200], Batch [20/50], Loss: 0.0097\n",
            "Epoch [67/200], Batch [25/50], Loss: 0.0079\n",
            "Epoch [67/200], Batch [30/50], Loss: 0.0108\n",
            "Epoch [67/200], Batch [35/50], Loss: 0.0146\n",
            "Epoch [67/200], Batch [40/50], Loss: 0.0098\n",
            "Epoch [67/200], Batch [45/50], Loss: 0.0083\n",
            "Epoch [67/200], Batch [50/50], Loss: 0.0078\n",
            "Epoch 67: Train Loss = 0.4617, Accuracy = 100.00%\n",
            "Epoch [68/200], Batch [5/50], Loss: 0.0096\n",
            "Epoch [68/200], Batch [10/50], Loss: 0.0099\n",
            "Epoch [68/200], Batch [15/50], Loss: 0.0025\n",
            "Epoch [68/200], Batch [20/50], Loss: 0.0052\n",
            "Epoch [68/200], Batch [25/50], Loss: 0.0155\n",
            "Epoch [68/200], Batch [30/50], Loss: 0.0166\n",
            "Epoch [68/200], Batch [35/50], Loss: 0.0051\n",
            "Epoch [68/200], Batch [40/50], Loss: 0.0173\n",
            "Epoch [68/200], Batch [45/50], Loss: 0.0053\n",
            "Epoch [68/200], Batch [50/50], Loss: 0.0254\n",
            "Epoch 68: Train Loss = 0.4755, Accuracy = 100.00%\n",
            "Epoch [69/200], Batch [5/50], Loss: 0.0100\n",
            "Epoch [69/200], Batch [10/50], Loss: 0.0185\n",
            "Epoch [69/200], Batch [15/50], Loss: 0.0072\n",
            "Epoch [69/200], Batch [20/50], Loss: 0.0045\n",
            "Epoch [69/200], Batch [25/50], Loss: 0.0088\n",
            "Epoch [69/200], Batch [30/50], Loss: 0.0070\n",
            "Epoch [69/200], Batch [35/50], Loss: 0.0078\n",
            "Epoch [69/200], Batch [40/50], Loss: 0.0082\n",
            "Epoch [69/200], Batch [45/50], Loss: 0.0086\n",
            "Epoch [69/200], Batch [50/50], Loss: 0.0134\n",
            "Epoch 69: Train Loss = 0.4492, Accuracy = 100.00%\n",
            "Epoch [70/200], Batch [5/50], Loss: 0.0077\n",
            "Epoch [70/200], Batch [10/50], Loss: 0.0497\n",
            "Epoch [70/200], Batch [15/50], Loss: 0.0046\n",
            "Epoch [70/200], Batch [20/50], Loss: 0.0131\n",
            "Epoch [70/200], Batch [25/50], Loss: 0.0152\n",
            "Epoch [70/200], Batch [30/50], Loss: 0.0041\n",
            "Epoch [70/200], Batch [35/50], Loss: 0.0182\n",
            "Epoch [70/200], Batch [40/50], Loss: 0.0056\n",
            "Epoch [70/200], Batch [45/50], Loss: 0.0254\n",
            "Epoch [70/200], Batch [50/50], Loss: 0.0109\n",
            "Epoch 70: Train Loss = 0.5947, Accuracy = 99.88%\n",
            "Epoch [71/200], Batch [5/50], Loss: 0.0117\n",
            "Epoch [71/200], Batch [10/50], Loss: 0.0046\n",
            "Epoch [71/200], Batch [15/50], Loss: 0.0102\n",
            "Epoch [71/200], Batch [20/50], Loss: 0.0096\n",
            "Epoch [71/200], Batch [25/50], Loss: 0.0057\n",
            "Epoch [71/200], Batch [30/50], Loss: 0.0153\n",
            "Epoch [71/200], Batch [35/50], Loss: 0.0122\n",
            "Epoch [71/200], Batch [40/50], Loss: 0.0126\n",
            "Epoch [71/200], Batch [45/50], Loss: 0.0102\n",
            "Epoch [71/200], Batch [50/50], Loss: 0.0083\n",
            "Epoch 71: Train Loss = 0.4984, Accuracy = 100.00%\n",
            "Epoch [72/200], Batch [5/50], Loss: 0.0092\n",
            "Epoch [72/200], Batch [10/50], Loss: 0.0097\n",
            "Epoch [72/200], Batch [15/50], Loss: 0.0082\n",
            "Epoch [72/200], Batch [20/50], Loss: 0.0107\n",
            "Epoch [72/200], Batch [25/50], Loss: 0.0058\n",
            "Epoch [72/200], Batch [30/50], Loss: 0.0091\n",
            "Epoch [72/200], Batch [35/50], Loss: 0.0169\n",
            "Epoch [72/200], Batch [40/50], Loss: 0.0071\n",
            "Epoch [72/200], Batch [45/50], Loss: 0.0213\n",
            "Epoch [72/200], Batch [50/50], Loss: 0.0142\n",
            "Epoch 72: Train Loss = 0.4470, Accuracy = 100.00%\n",
            "Epoch [73/200], Batch [5/50], Loss: 0.0051\n",
            "Epoch [73/200], Batch [10/50], Loss: 0.0089\n",
            "Epoch [73/200], Batch [15/50], Loss: 0.0088\n",
            "Epoch [73/200], Batch [20/50], Loss: 0.0073\n",
            "Epoch [73/200], Batch [25/50], Loss: 0.0066\n",
            "Epoch [73/200], Batch [30/50], Loss: 0.0105\n",
            "Epoch [73/200], Batch [35/50], Loss: 0.0142\n",
            "Epoch [73/200], Batch [40/50], Loss: 0.0055\n",
            "Epoch [73/200], Batch [45/50], Loss: 0.0215\n",
            "Epoch [73/200], Batch [50/50], Loss: 0.0066\n",
            "Epoch 73: Train Loss = 0.4245, Accuracy = 100.00%\n",
            "Epoch [74/200], Batch [5/50], Loss: 0.0088\n",
            "Epoch [74/200], Batch [10/50], Loss: 0.0081\n",
            "Epoch [74/200], Batch [15/50], Loss: 0.0062\n",
            "Epoch [74/200], Batch [20/50], Loss: 0.0064\n",
            "Epoch [74/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [74/200], Batch [30/50], Loss: 0.0093\n",
            "Epoch [74/200], Batch [35/50], Loss: 0.0036\n",
            "Epoch [74/200], Batch [40/50], Loss: 0.0060\n",
            "Epoch [74/200], Batch [45/50], Loss: 0.0029\n",
            "Epoch [74/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 74: Train Loss = 0.4605, Accuracy = 100.00%\n",
            "Epoch [75/200], Batch [5/50], Loss: 0.0091\n",
            "Epoch [75/200], Batch [10/50], Loss: 0.0043\n",
            "Epoch [75/200], Batch [15/50], Loss: 0.0104\n",
            "Epoch [75/200], Batch [20/50], Loss: 0.0084\n",
            "Epoch [75/200], Batch [25/50], Loss: 0.0040\n",
            "Epoch [75/200], Batch [30/50], Loss: 0.0169\n",
            "Epoch [75/200], Batch [35/50], Loss: 0.0056\n",
            "Epoch [75/200], Batch [40/50], Loss: 0.0062\n",
            "Epoch [75/200], Batch [45/50], Loss: 0.0063\n",
            "Epoch [75/200], Batch [50/50], Loss: 0.0072\n",
            "Epoch 75: Train Loss = 0.4270, Accuracy = 100.00%\n",
            "Epoch [76/200], Batch [5/50], Loss: 0.0058\n",
            "Epoch [76/200], Batch [10/50], Loss: 0.0041\n",
            "Epoch [76/200], Batch [15/50], Loss: 0.0065\n",
            "Epoch [76/200], Batch [20/50], Loss: 0.0087\n",
            "Epoch [76/200], Batch [25/50], Loss: 0.0115\n",
            "Epoch [76/200], Batch [30/50], Loss: 0.0065\n",
            "Epoch [76/200], Batch [35/50], Loss: 0.0099\n",
            "Epoch [76/200], Batch [40/50], Loss: 0.0094\n",
            "Epoch [76/200], Batch [45/50], Loss: 0.0132\n",
            "Epoch [76/200], Batch [50/50], Loss: 0.0065\n",
            "Epoch 76: Train Loss = 0.4108, Accuracy = 100.00%\n",
            "Epoch [77/200], Batch [5/50], Loss: 0.0074\n",
            "Epoch [77/200], Batch [10/50], Loss: 0.0056\n",
            "Epoch [77/200], Batch [15/50], Loss: 0.0091\n",
            "Epoch [77/200], Batch [20/50], Loss: 0.0111\n",
            "Epoch [77/200], Batch [25/50], Loss: 0.0064\n",
            "Epoch [77/200], Batch [30/50], Loss: 0.0100\n",
            "Epoch [77/200], Batch [35/50], Loss: 0.0033\n",
            "Epoch [77/200], Batch [40/50], Loss: 0.0086\n",
            "Epoch [77/200], Batch [45/50], Loss: 0.0067\n",
            "Epoch [77/200], Batch [50/50], Loss: 0.0092\n",
            "Epoch 77: Train Loss = 0.4271, Accuracy = 100.00%\n",
            "Epoch [78/200], Batch [5/50], Loss: 0.0079\n",
            "Epoch [78/200], Batch [10/50], Loss: 0.0072\n",
            "Epoch [78/200], Batch [15/50], Loss: 0.0074\n",
            "Epoch [78/200], Batch [20/50], Loss: 0.0039\n",
            "Epoch [78/200], Batch [25/50], Loss: 0.0033\n",
            "Epoch [78/200], Batch [30/50], Loss: 0.0028\n",
            "Epoch [78/200], Batch [35/50], Loss: 0.0030\n",
            "Epoch [78/200], Batch [40/50], Loss: 0.0089\n",
            "Epoch [78/200], Batch [45/50], Loss: 0.0084\n",
            "Epoch [78/200], Batch [50/50], Loss: 0.0094\n",
            "Epoch 78: Train Loss = 0.4075, Accuracy = 100.00%\n",
            "Epoch [79/200], Batch [5/50], Loss: 0.0044\n",
            "Epoch [79/200], Batch [10/50], Loss: 0.0070\n",
            "Epoch [79/200], Batch [15/50], Loss: 0.0063\n",
            "Epoch [79/200], Batch [20/50], Loss: 0.0176\n",
            "Epoch [79/200], Batch [25/50], Loss: 0.0122\n",
            "Epoch [79/200], Batch [30/50], Loss: 0.0114\n",
            "Epoch [79/200], Batch [35/50], Loss: 0.0075\n",
            "Epoch [79/200], Batch [40/50], Loss: 0.0084\n",
            "Epoch [79/200], Batch [45/50], Loss: 0.0139\n",
            "Epoch [79/200], Batch [50/50], Loss: 0.0131\n",
            "Epoch 79: Train Loss = 0.4195, Accuracy = 100.00%\n",
            "Epoch [80/200], Batch [5/50], Loss: 0.0167\n",
            "Epoch [80/200], Batch [10/50], Loss: 0.0106\n",
            "Epoch [80/200], Batch [15/50], Loss: 0.0118\n",
            "Epoch [80/200], Batch [20/50], Loss: 0.0025\n",
            "Epoch [80/200], Batch [25/50], Loss: 0.0101\n",
            "Epoch [80/200], Batch [30/50], Loss: 0.0032\n",
            "Epoch [80/200], Batch [35/50], Loss: 0.0038\n",
            "Epoch [80/200], Batch [40/50], Loss: 0.0143\n",
            "Epoch [80/200], Batch [45/50], Loss: 0.0081\n",
            "Epoch [80/200], Batch [50/50], Loss: 0.0088\n",
            "Epoch 80: Train Loss = 0.3939, Accuracy = 100.00%\n",
            "Epoch [81/200], Batch [5/50], Loss: 0.0076\n",
            "Epoch [81/200], Batch [10/50], Loss: 0.0116\n",
            "Epoch [81/200], Batch [15/50], Loss: 0.0054\n",
            "Epoch [81/200], Batch [20/50], Loss: 0.0114\n",
            "Epoch [81/200], Batch [25/50], Loss: 0.0031\n",
            "Epoch [81/200], Batch [30/50], Loss: 0.0116\n",
            "Epoch [81/200], Batch [35/50], Loss: 0.0062\n",
            "Epoch [81/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [81/200], Batch [45/50], Loss: 0.0059\n",
            "Epoch [81/200], Batch [50/50], Loss: 0.0071\n",
            "Epoch 81: Train Loss = 0.3983, Accuracy = 100.00%\n",
            "Epoch [82/200], Batch [5/50], Loss: 0.0089\n",
            "Epoch [82/200], Batch [10/50], Loss: 0.0209\n",
            "Epoch [82/200], Batch [15/50], Loss: 0.0049\n",
            "Epoch [82/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [82/200], Batch [25/50], Loss: 0.0061\n",
            "Epoch [82/200], Batch [30/50], Loss: 0.0082\n",
            "Epoch [82/200], Batch [35/50], Loss: 0.0076\n",
            "Epoch [82/200], Batch [40/50], Loss: 0.0079\n",
            "Epoch [82/200], Batch [45/50], Loss: 0.0045\n",
            "Epoch [82/200], Batch [50/50], Loss: 0.0047\n",
            "Epoch 82: Train Loss = 0.4011, Accuracy = 100.00%\n",
            "Epoch [83/200], Batch [5/50], Loss: 0.0119\n",
            "Epoch [83/200], Batch [10/50], Loss: 0.0029\n",
            "Epoch [83/200], Batch [15/50], Loss: 0.0060\n",
            "Epoch [83/200], Batch [20/50], Loss: 0.0072\n",
            "Epoch [83/200], Batch [25/50], Loss: 0.0068\n",
            "Epoch [83/200], Batch [30/50], Loss: 0.0058\n",
            "Epoch [83/200], Batch [35/50], Loss: 0.0047\n",
            "Epoch [83/200], Batch [40/50], Loss: 0.0078\n",
            "Epoch [83/200], Batch [45/50], Loss: 0.0026\n",
            "Epoch [83/200], Batch [50/50], Loss: 0.0096\n",
            "Epoch 83: Train Loss = 0.4128, Accuracy = 100.00%\n",
            "Epoch [84/200], Batch [5/50], Loss: 0.0098\n",
            "Epoch [84/200], Batch [10/50], Loss: 0.0021\n",
            "Epoch [84/200], Batch [15/50], Loss: 0.0051\n",
            "Epoch [84/200], Batch [20/50], Loss: 0.0022\n",
            "Epoch [84/200], Batch [25/50], Loss: 0.0102\n",
            "Epoch [84/200], Batch [30/50], Loss: 0.0083\n",
            "Epoch [84/200], Batch [35/50], Loss: 0.0068\n",
            "Epoch [84/200], Batch [40/50], Loss: 0.0089\n",
            "Epoch [84/200], Batch [45/50], Loss: 0.0054\n",
            "Epoch [84/200], Batch [50/50], Loss: 0.0045\n",
            "Epoch 84: Train Loss = 0.3782, Accuracy = 100.00%\n",
            "Epoch [85/200], Batch [5/50], Loss: 0.0035\n",
            "Epoch [85/200], Batch [10/50], Loss: 0.0036\n",
            "Epoch [85/200], Batch [15/50], Loss: 0.0051\n",
            "Epoch [85/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [85/200], Batch [25/50], Loss: 0.0104\n",
            "Epoch [85/200], Batch [30/50], Loss: 0.0082\n",
            "Epoch [85/200], Batch [35/50], Loss: 0.0037\n",
            "Epoch [85/200], Batch [40/50], Loss: 0.0062\n",
            "Epoch [85/200], Batch [45/50], Loss: 0.0145\n",
            "Epoch [85/200], Batch [50/50], Loss: 0.0185\n",
            "Epoch 85: Train Loss = 0.3718, Accuracy = 100.00%\n",
            "Epoch [86/200], Batch [5/50], Loss: 0.0080\n",
            "Epoch [86/200], Batch [10/50], Loss: 0.0046\n",
            "Epoch [86/200], Batch [15/50], Loss: 0.0054\n",
            "Epoch [86/200], Batch [20/50], Loss: 0.0070\n",
            "Epoch [86/200], Batch [25/50], Loss: 0.0045\n",
            "Epoch [86/200], Batch [30/50], Loss: 0.0042\n",
            "Epoch [86/200], Batch [35/50], Loss: 0.0087\n",
            "Epoch [86/200], Batch [40/50], Loss: 0.0086\n",
            "Epoch [86/200], Batch [45/50], Loss: 0.0077\n",
            "Epoch [86/200], Batch [50/50], Loss: 0.0049\n",
            "Epoch 86: Train Loss = 0.3559, Accuracy = 100.00%\n",
            "Epoch [87/200], Batch [5/50], Loss: 0.0046\n",
            "Epoch [87/200], Batch [10/50], Loss: 0.0042\n",
            "Epoch [87/200], Batch [15/50], Loss: 0.0055\n",
            "Epoch [87/200], Batch [20/50], Loss: 0.0076\n",
            "Epoch [87/200], Batch [25/50], Loss: 0.0058\n",
            "Epoch [87/200], Batch [30/50], Loss: 0.0089\n",
            "Epoch [87/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [87/200], Batch [40/50], Loss: 0.0081\n",
            "Epoch [87/200], Batch [45/50], Loss: 0.0043\n",
            "Epoch [87/200], Batch [50/50], Loss: 0.0030\n",
            "Epoch 87: Train Loss = 0.3447, Accuracy = 100.00%\n",
            "Epoch [88/200], Batch [5/50], Loss: 0.0091\n",
            "Epoch [88/200], Batch [10/50], Loss: 0.0056\n",
            "Epoch [88/200], Batch [15/50], Loss: 0.0029\n",
            "Epoch [88/200], Batch [20/50], Loss: 0.0050\n",
            "Epoch [88/200], Batch [25/50], Loss: 0.0070\n",
            "Epoch [88/200], Batch [30/50], Loss: 0.0062\n",
            "Epoch [88/200], Batch [35/50], Loss: 0.0039\n",
            "Epoch [88/200], Batch [40/50], Loss: 0.0127\n",
            "Epoch [88/200], Batch [45/50], Loss: 0.0074\n",
            "Epoch [88/200], Batch [50/50], Loss: 0.0037\n",
            "Epoch 88: Train Loss = 0.3371, Accuracy = 100.00%\n",
            "Epoch [89/200], Batch [5/50], Loss: 0.0072\n",
            "Epoch [89/200], Batch [10/50], Loss: 0.0046\n",
            "Epoch [89/200], Batch [15/50], Loss: 0.0075\n",
            "Epoch [89/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [89/200], Batch [25/50], Loss: 0.0082\n",
            "Epoch [89/200], Batch [30/50], Loss: 0.0086\n",
            "Epoch [89/200], Batch [35/50], Loss: 0.0032\n",
            "Epoch [89/200], Batch [40/50], Loss: 0.0165\n",
            "Epoch [89/200], Batch [45/50], Loss: 0.0094\n",
            "Epoch [89/200], Batch [50/50], Loss: 0.0021\n",
            "Epoch 89: Train Loss = 0.3443, Accuracy = 100.00%\n",
            "Epoch [90/200], Batch [5/50], Loss: 0.0067\n",
            "Epoch [90/200], Batch [10/50], Loss: 0.0063\n",
            "Epoch [90/200], Batch [15/50], Loss: 0.0098\n",
            "Epoch [90/200], Batch [20/50], Loss: 0.0081\n",
            "Epoch [90/200], Batch [25/50], Loss: 0.0050\n",
            "Epoch [90/200], Batch [30/50], Loss: 0.0117\n",
            "Epoch [90/200], Batch [35/50], Loss: 0.0024\n",
            "Epoch [90/200], Batch [40/50], Loss: 0.0076\n",
            "Epoch [90/200], Batch [45/50], Loss: 0.0081\n",
            "Epoch [90/200], Batch [50/50], Loss: 0.0129\n",
            "Epoch 90: Train Loss = 0.3526, Accuracy = 100.00%\n",
            "Epoch [91/200], Batch [5/50], Loss: 0.0058\n",
            "Epoch [91/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [91/200], Batch [15/50], Loss: 0.0088\n",
            "Epoch [91/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [91/200], Batch [25/50], Loss: 0.0040\n",
            "Epoch [91/200], Batch [30/50], Loss: 0.0047\n",
            "Epoch [91/200], Batch [35/50], Loss: 0.0039\n",
            "Epoch [91/200], Batch [40/50], Loss: 0.0084\n",
            "Epoch [91/200], Batch [45/50], Loss: 0.0056\n",
            "Epoch [91/200], Batch [50/50], Loss: 0.0103\n",
            "Epoch 91: Train Loss = 0.3458, Accuracy = 100.00%\n",
            "Epoch [92/200], Batch [5/50], Loss: 0.0044\n",
            "Epoch [92/200], Batch [10/50], Loss: 0.0039\n",
            "Epoch [92/200], Batch [15/50], Loss: 0.0183\n",
            "Epoch [92/200], Batch [20/50], Loss: 0.0029\n",
            "Epoch [92/200], Batch [25/50], Loss: 0.0085\n",
            "Epoch [92/200], Batch [30/50], Loss: 0.0054\n",
            "Epoch [92/200], Batch [35/50], Loss: 0.0106\n",
            "Epoch [92/200], Batch [40/50], Loss: 0.0029\n",
            "Epoch [92/200], Batch [45/50], Loss: 0.0143\n",
            "Epoch [92/200], Batch [50/50], Loss: 0.0056\n",
            "Epoch 92: Train Loss = 0.3491, Accuracy = 100.00%\n",
            "Epoch [93/200], Batch [5/50], Loss: 0.0098\n",
            "Epoch [93/200], Batch [10/50], Loss: 0.0117\n",
            "Epoch [93/200], Batch [15/50], Loss: 0.0042\n",
            "Epoch [93/200], Batch [20/50], Loss: 0.0078\n",
            "Epoch [93/200], Batch [25/50], Loss: 0.0091\n",
            "Epoch [93/200], Batch [30/50], Loss: 0.0062\n",
            "Epoch [93/200], Batch [35/50], Loss: 0.0052\n",
            "Epoch [93/200], Batch [40/50], Loss: 0.0058\n",
            "Epoch [93/200], Batch [45/50], Loss: 0.0018\n",
            "Epoch [93/200], Batch [50/50], Loss: 0.0075\n",
            "Epoch 93: Train Loss = 0.3561, Accuracy = 100.00%\n",
            "Epoch [94/200], Batch [5/50], Loss: 0.0059\n",
            "Epoch [94/200], Batch [10/50], Loss: 0.0202\n",
            "Epoch [94/200], Batch [15/50], Loss: 0.0037\n",
            "Epoch [94/200], Batch [20/50], Loss: 0.0045\n",
            "Epoch [94/200], Batch [25/50], Loss: 0.0068\n",
            "Epoch [94/200], Batch [30/50], Loss: 0.0035\n",
            "Epoch [94/200], Batch [35/50], Loss: 0.0102\n",
            "Epoch [94/200], Batch [40/50], Loss: 0.0079\n",
            "Epoch [94/200], Batch [45/50], Loss: 0.0092\n",
            "Epoch [94/200], Batch [50/50], Loss: 0.0050\n",
            "Epoch 94: Train Loss = 0.3394, Accuracy = 100.00%\n",
            "Epoch [95/200], Batch [5/50], Loss: 0.0042\n",
            "Epoch [95/200], Batch [10/50], Loss: 0.0063\n",
            "Epoch [95/200], Batch [15/50], Loss: 0.0073\n",
            "Epoch [95/200], Batch [20/50], Loss: 0.0067\n",
            "Epoch [95/200], Batch [25/50], Loss: 0.0119\n",
            "Epoch [95/200], Batch [30/50], Loss: 0.0081\n",
            "Epoch [95/200], Batch [35/50], Loss: 0.0023\n",
            "Epoch [95/200], Batch [40/50], Loss: 0.0065\n",
            "Epoch [95/200], Batch [45/50], Loss: 0.0043\n",
            "Epoch [95/200], Batch [50/50], Loss: 0.0081\n",
            "Epoch 95: Train Loss = 0.3357, Accuracy = 100.00%\n",
            "Epoch [96/200], Batch [5/50], Loss: 0.0066\n",
            "Epoch [96/200], Batch [10/50], Loss: 0.0070\n",
            "Epoch [96/200], Batch [15/50], Loss: 0.0067\n",
            "Epoch [96/200], Batch [20/50], Loss: 0.0071\n",
            "Epoch [96/200], Batch [25/50], Loss: 0.0064\n",
            "Epoch [96/200], Batch [30/50], Loss: 0.0057\n",
            "Epoch [96/200], Batch [35/50], Loss: 0.0082\n",
            "Epoch [96/200], Batch [40/50], Loss: 0.0048\n",
            "Epoch [96/200], Batch [45/50], Loss: 0.0052\n",
            "Epoch [96/200], Batch [50/50], Loss: 0.0080\n",
            "Epoch 96: Train Loss = 0.3225, Accuracy = 100.00%\n",
            "Epoch [97/200], Batch [5/50], Loss: 0.0081\n",
            "Epoch [97/200], Batch [10/50], Loss: 0.0076\n",
            "Epoch [97/200], Batch [15/50], Loss: 0.0059\n",
            "Epoch [97/200], Batch [20/50], Loss: 0.0076\n",
            "Epoch [97/200], Batch [25/50], Loss: 0.0046\n",
            "Epoch [97/200], Batch [30/50], Loss: 0.0103\n",
            "Epoch [97/200], Batch [35/50], Loss: 0.0071\n",
            "Epoch [97/200], Batch [40/50], Loss: 0.0069\n",
            "Epoch [97/200], Batch [45/50], Loss: 0.0138\n",
            "Epoch [97/200], Batch [50/50], Loss: 0.0062\n",
            "Epoch 97: Train Loss = 0.3148, Accuracy = 100.00%\n",
            "Epoch [98/200], Batch [5/50], Loss: 0.0039\n",
            "Epoch [98/200], Batch [10/50], Loss: 0.0083\n",
            "Epoch [98/200], Batch [15/50], Loss: 0.0059\n",
            "Epoch [98/200], Batch [20/50], Loss: 0.0145\n",
            "Epoch [98/200], Batch [25/50], Loss: 0.0050\n",
            "Epoch [98/200], Batch [30/50], Loss: 0.0059\n",
            "Epoch [98/200], Batch [35/50], Loss: 0.0046\n",
            "Epoch [98/200], Batch [40/50], Loss: 0.0038\n",
            "Epoch [98/200], Batch [45/50], Loss: 0.0139\n",
            "Epoch [98/200], Batch [50/50], Loss: 0.0065\n",
            "Epoch 98: Train Loss = 0.3131, Accuracy = 100.00%\n",
            "Epoch [99/200], Batch [5/50], Loss: 0.0055\n",
            "Epoch [99/200], Batch [10/50], Loss: 0.0074\n",
            "Epoch [99/200], Batch [15/50], Loss: 0.0074\n",
            "Epoch [99/200], Batch [20/50], Loss: 0.0117\n",
            "Epoch [99/200], Batch [25/50], Loss: 0.0075\n",
            "Epoch [99/200], Batch [30/50], Loss: 0.0044\n",
            "Epoch [99/200], Batch [35/50], Loss: 0.0075\n",
            "Epoch [99/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [99/200], Batch [45/50], Loss: 0.0013\n",
            "Epoch [99/200], Batch [50/50], Loss: 0.0051\n",
            "Epoch 99: Train Loss = 0.3061, Accuracy = 100.00%\n",
            "Epoch [100/200], Batch [5/50], Loss: 0.0107\n",
            "Epoch [100/200], Batch [10/50], Loss: 0.0073\n",
            "Epoch [100/200], Batch [15/50], Loss: 0.0029\n",
            "Epoch [100/200], Batch [20/50], Loss: 0.0081\n",
            "Epoch [100/200], Batch [25/50], Loss: 0.0047\n",
            "Epoch [100/200], Batch [30/50], Loss: 0.0128\n",
            "Epoch [100/200], Batch [35/50], Loss: 0.0059\n",
            "Epoch [100/200], Batch [40/50], Loss: 0.0160\n",
            "Epoch [100/200], Batch [45/50], Loss: 0.0045\n",
            "Epoch [100/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 100: Train Loss = 0.3030, Accuracy = 100.00%\n",
            "Epoch [101/200], Batch [5/50], Loss: 0.0030\n",
            "Epoch [101/200], Batch [10/50], Loss: 0.0076\n",
            "Epoch [101/200], Batch [15/50], Loss: 0.0043\n",
            "Epoch [101/200], Batch [20/50], Loss: 0.0075\n",
            "Epoch [101/200], Batch [25/50], Loss: 0.0093\n",
            "Epoch [101/200], Batch [30/50], Loss: 0.0074\n",
            "Epoch [101/200], Batch [35/50], Loss: 0.0055\n",
            "Epoch [101/200], Batch [40/50], Loss: 0.0044\n",
            "Epoch [101/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [101/200], Batch [50/50], Loss: 0.0040\n",
            "Epoch 101: Train Loss = 0.2834, Accuracy = 100.00%\n",
            "Epoch [102/200], Batch [5/50], Loss: 0.0040\n",
            "Epoch [102/200], Batch [10/50], Loss: 0.0095\n",
            "Epoch [102/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [102/200], Batch [20/50], Loss: 0.0042\n",
            "Epoch [102/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [102/200], Batch [30/50], Loss: 0.0037\n",
            "Epoch [102/200], Batch [35/50], Loss: 0.0046\n",
            "Epoch [102/200], Batch [40/50], Loss: 0.0059\n",
            "Epoch [102/200], Batch [45/50], Loss: 0.0123\n",
            "Epoch [102/200], Batch [50/50], Loss: 0.0030\n",
            "Epoch 102: Train Loss = 0.2912, Accuracy = 100.00%\n",
            "Epoch [103/200], Batch [5/50], Loss: 0.0063\n",
            "Epoch [103/200], Batch [10/50], Loss: 0.0078\n",
            "Epoch [103/200], Batch [15/50], Loss: 0.0031\n",
            "Epoch [103/200], Batch [20/50], Loss: 0.0058\n",
            "Epoch [103/200], Batch [25/50], Loss: 0.0069\n",
            "Epoch [103/200], Batch [30/50], Loss: 0.0105\n",
            "Epoch [103/200], Batch [35/50], Loss: 0.0092\n",
            "Epoch [103/200], Batch [40/50], Loss: 0.0052\n",
            "Epoch [103/200], Batch [45/50], Loss: 0.0053\n",
            "Epoch [103/200], Batch [50/50], Loss: 0.0044\n",
            "Epoch 103: Train Loss = 0.2918, Accuracy = 100.00%\n",
            "Epoch [104/200], Batch [5/50], Loss: 0.0068\n",
            "Epoch [104/200], Batch [10/50], Loss: 0.0064\n",
            "Epoch [104/200], Batch [15/50], Loss: 0.0113\n",
            "Epoch [104/200], Batch [20/50], Loss: 0.0047\n",
            "Epoch [104/200], Batch [25/50], Loss: 0.0052\n",
            "Epoch [104/200], Batch [30/50], Loss: 0.0057\n",
            "Epoch [104/200], Batch [35/50], Loss: 0.0079\n",
            "Epoch [104/200], Batch [40/50], Loss: 0.0048\n",
            "Epoch [104/200], Batch [45/50], Loss: 0.0033\n",
            "Epoch [104/200], Batch [50/50], Loss: 0.0065\n",
            "Epoch 104: Train Loss = 0.2914, Accuracy = 100.00%\n",
            "Epoch [105/200], Batch [5/50], Loss: 0.0067\n",
            "Epoch [105/200], Batch [10/50], Loss: 0.0047\n",
            "Epoch [105/200], Batch [15/50], Loss: 0.0055\n",
            "Epoch [105/200], Batch [20/50], Loss: 0.0030\n",
            "Epoch [105/200], Batch [25/50], Loss: 0.0056\n",
            "Epoch [105/200], Batch [30/50], Loss: 0.0099\n",
            "Epoch [105/200], Batch [35/50], Loss: 0.0099\n",
            "Epoch [105/200], Batch [40/50], Loss: 0.0051\n",
            "Epoch [105/200], Batch [45/50], Loss: 0.0065\n",
            "Epoch [105/200], Batch [50/50], Loss: 0.0107\n",
            "Epoch 105: Train Loss = 0.2841, Accuracy = 100.00%\n",
            "Epoch [106/200], Batch [5/50], Loss: 0.0080\n",
            "Epoch [106/200], Batch [10/50], Loss: 0.0053\n",
            "Epoch [106/200], Batch [15/50], Loss: 0.0075\n",
            "Epoch [106/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [106/200], Batch [25/50], Loss: 0.0049\n",
            "Epoch [106/200], Batch [30/50], Loss: 0.0039\n",
            "Epoch [106/200], Batch [35/50], Loss: 0.0027\n",
            "Epoch [106/200], Batch [40/50], Loss: 0.0022\n",
            "Epoch [106/200], Batch [45/50], Loss: 0.0141\n",
            "Epoch [106/200], Batch [50/50], Loss: 0.0037\n",
            "Epoch 106: Train Loss = 0.2734, Accuracy = 100.00%\n",
            "Epoch [107/200], Batch [5/50], Loss: 0.0058\n",
            "Epoch [107/200], Batch [10/50], Loss: 0.0045\n",
            "Epoch [107/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [107/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [107/200], Batch [25/50], Loss: 0.0046\n",
            "Epoch [107/200], Batch [30/50], Loss: 0.0067\n",
            "Epoch [107/200], Batch [35/50], Loss: 0.0024\n",
            "Epoch [107/200], Batch [40/50], Loss: 0.0034\n",
            "Epoch [107/200], Batch [45/50], Loss: 0.0067\n",
            "Epoch [107/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 107: Train Loss = 0.2736, Accuracy = 100.00%\n",
            "Epoch [108/200], Batch [5/50], Loss: 0.0128\n",
            "Epoch [108/200], Batch [10/50], Loss: 0.0024\n",
            "Epoch [108/200], Batch [15/50], Loss: 0.0034\n",
            "Epoch [108/200], Batch [20/50], Loss: 0.0041\n",
            "Epoch [108/200], Batch [25/50], Loss: 0.0043\n",
            "Epoch [108/200], Batch [30/50], Loss: 0.0043\n",
            "Epoch [108/200], Batch [35/50], Loss: 0.0058\n",
            "Epoch [108/200], Batch [40/50], Loss: 0.0062\n",
            "Epoch [108/200], Batch [45/50], Loss: 0.0050\n",
            "Epoch [108/200], Batch [50/50], Loss: 0.0086\n",
            "Epoch 108: Train Loss = 0.2835, Accuracy = 100.00%\n",
            "Epoch [109/200], Batch [5/50], Loss: 0.0055\n",
            "Epoch [109/200], Batch [10/50], Loss: 0.0053\n",
            "Epoch [109/200], Batch [15/50], Loss: 0.0138\n",
            "Epoch [109/200], Batch [20/50], Loss: 0.0054\n",
            "Epoch [109/200], Batch [25/50], Loss: 0.0045\n",
            "Epoch [109/200], Batch [30/50], Loss: 0.0079\n",
            "Epoch [109/200], Batch [35/50], Loss: 0.0107\n",
            "Epoch [109/200], Batch [40/50], Loss: 0.0141\n",
            "Epoch [109/200], Batch [45/50], Loss: 0.0090\n",
            "Epoch [109/200], Batch [50/50], Loss: 0.0035\n",
            "Epoch 109: Train Loss = 0.2925, Accuracy = 100.00%\n",
            "Epoch [110/200], Batch [5/50], Loss: 0.0032\n",
            "Epoch [110/200], Batch [10/50], Loss: 0.0053\n",
            "Epoch [110/200], Batch [15/50], Loss: 0.0062\n",
            "Epoch [110/200], Batch [20/50], Loss: 0.0044\n",
            "Epoch [110/200], Batch [25/50], Loss: 0.0038\n",
            "Epoch [110/200], Batch [30/50], Loss: 0.0030\n",
            "Epoch [110/200], Batch [35/50], Loss: 0.0064\n",
            "Epoch [110/200], Batch [40/50], Loss: 0.0081\n",
            "Epoch [110/200], Batch [45/50], Loss: 0.0050\n",
            "Epoch [110/200], Batch [50/50], Loss: 0.0047\n",
            "Epoch 110: Train Loss = 0.2825, Accuracy = 100.00%\n",
            "Epoch [111/200], Batch [5/50], Loss: 0.0062\n",
            "Epoch [111/200], Batch [10/50], Loss: 0.0146\n",
            "Epoch [111/200], Batch [15/50], Loss: 0.0061\n",
            "Epoch [111/200], Batch [20/50], Loss: 0.0053\n",
            "Epoch [111/200], Batch [25/50], Loss: 0.0060\n",
            "Epoch [111/200], Batch [30/50], Loss: 0.0060\n",
            "Epoch [111/200], Batch [35/50], Loss: 0.0032\n",
            "Epoch [111/200], Batch [40/50], Loss: 0.0034\n",
            "Epoch [111/200], Batch [45/50], Loss: 0.0065\n",
            "Epoch [111/200], Batch [50/50], Loss: 0.0061\n",
            "Epoch 111: Train Loss = 0.2696, Accuracy = 100.00%\n",
            "Epoch [112/200], Batch [5/50], Loss: 0.0044\n",
            "Epoch [112/200], Batch [10/50], Loss: 0.0040\n",
            "Epoch [112/200], Batch [15/50], Loss: 0.0070\n",
            "Epoch [112/200], Batch [20/50], Loss: 0.0058\n",
            "Epoch [112/200], Batch [25/50], Loss: 0.0042\n",
            "Epoch [112/200], Batch [30/50], Loss: 0.0085\n",
            "Epoch [112/200], Batch [35/50], Loss: 0.0056\n",
            "Epoch [112/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [112/200], Batch [45/50], Loss: 0.0041\n",
            "Epoch [112/200], Batch [50/50], Loss: 0.0046\n",
            "Epoch 112: Train Loss = 0.2609, Accuracy = 100.00%\n",
            "Epoch [113/200], Batch [5/50], Loss: 0.0032\n",
            "Epoch [113/200], Batch [10/50], Loss: 0.0042\n",
            "Epoch [113/200], Batch [15/50], Loss: 0.0047\n",
            "Epoch [113/200], Batch [20/50], Loss: 0.0044\n",
            "Epoch [113/200], Batch [25/50], Loss: 0.0021\n",
            "Epoch [113/200], Batch [30/50], Loss: 0.0036\n",
            "Epoch [113/200], Batch [35/50], Loss: 0.0065\n",
            "Epoch [113/200], Batch [40/50], Loss: 0.0094\n",
            "Epoch [113/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [113/200], Batch [50/50], Loss: 0.0016\n",
            "Epoch 113: Train Loss = 0.2624, Accuracy = 100.00%\n",
            "Epoch [114/200], Batch [5/50], Loss: 0.0055\n",
            "Epoch [114/200], Batch [10/50], Loss: 0.0055\n",
            "Epoch [114/200], Batch [15/50], Loss: 0.0092\n",
            "Epoch [114/200], Batch [20/50], Loss: 0.0039\n",
            "Epoch [114/200], Batch [25/50], Loss: 0.0040\n",
            "Epoch [114/200], Batch [30/50], Loss: 0.0074\n",
            "Epoch [114/200], Batch [35/50], Loss: 0.0017\n",
            "Epoch [114/200], Batch [40/50], Loss: 0.0040\n",
            "Epoch [114/200], Batch [45/50], Loss: 0.0047\n",
            "Epoch [114/200], Batch [50/50], Loss: 0.0061\n",
            "Epoch 114: Train Loss = 0.2648, Accuracy = 100.00%\n",
            "Epoch [115/200], Batch [5/50], Loss: 0.0037\n",
            "Epoch [115/200], Batch [10/50], Loss: 0.0068\n",
            "Epoch [115/200], Batch [15/50], Loss: 0.0041\n",
            "Epoch [115/200], Batch [20/50], Loss: 0.0041\n",
            "Epoch [115/200], Batch [25/50], Loss: 0.0020\n",
            "Epoch [115/200], Batch [30/50], Loss: 0.0089\n",
            "Epoch [115/200], Batch [35/50], Loss: 0.0089\n",
            "Epoch [115/200], Batch [40/50], Loss: 0.0037\n",
            "Epoch [115/200], Batch [45/50], Loss: 0.0102\n",
            "Epoch [115/200], Batch [50/50], Loss: 0.0052\n",
            "Epoch 115: Train Loss = 0.2623, Accuracy = 100.00%\n",
            "Epoch [116/200], Batch [5/50], Loss: 0.0040\n",
            "Epoch [116/200], Batch [10/50], Loss: 0.0063\n",
            "Epoch [116/200], Batch [15/50], Loss: 0.0061\n",
            "Epoch [116/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [116/200], Batch [25/50], Loss: 0.0055\n",
            "Epoch [116/200], Batch [30/50], Loss: 0.0054\n",
            "Epoch [116/200], Batch [35/50], Loss: 0.0042\n",
            "Epoch [116/200], Batch [40/50], Loss: 0.0018\n",
            "Epoch [116/200], Batch [45/50], Loss: 0.0040\n",
            "Epoch [116/200], Batch [50/50], Loss: 0.0052\n",
            "Epoch 116: Train Loss = 0.2590, Accuracy = 100.00%\n",
            "Epoch [117/200], Batch [5/50], Loss: 0.0045\n",
            "Epoch [117/200], Batch [10/50], Loss: 0.0041\n",
            "Epoch [117/200], Batch [15/50], Loss: 0.0050\n",
            "Epoch [117/200], Batch [20/50], Loss: 0.0067\n",
            "Epoch [117/200], Batch [25/50], Loss: 0.0034\n",
            "Epoch [117/200], Batch [30/50], Loss: 0.0047\n",
            "Epoch [117/200], Batch [35/50], Loss: 0.0032\n",
            "Epoch [117/200], Batch [40/50], Loss: 0.0054\n",
            "Epoch [117/200], Batch [45/50], Loss: 0.0078\n",
            "Epoch [117/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 117: Train Loss = 0.2630, Accuracy = 100.00%\n",
            "Epoch [118/200], Batch [5/50], Loss: 0.0080\n",
            "Epoch [118/200], Batch [10/50], Loss: 0.0024\n",
            "Epoch [118/200], Batch [15/50], Loss: 0.0061\n",
            "Epoch [118/200], Batch [20/50], Loss: 0.0087\n",
            "Epoch [118/200], Batch [25/50], Loss: 0.0058\n",
            "Epoch [118/200], Batch [30/50], Loss: 0.0023\n",
            "Epoch [118/200], Batch [35/50], Loss: 0.0038\n",
            "Epoch [118/200], Batch [40/50], Loss: 0.0085\n",
            "Epoch [118/200], Batch [45/50], Loss: 0.0045\n",
            "Epoch [118/200], Batch [50/50], Loss: 0.0026\n",
            "Epoch 118: Train Loss = 0.2612, Accuracy = 100.00%\n",
            "Epoch [119/200], Batch [5/50], Loss: 0.0031\n",
            "Epoch [119/200], Batch [10/50], Loss: 0.0066\n",
            "Epoch [119/200], Batch [15/50], Loss: 0.0035\n",
            "Epoch [119/200], Batch [20/50], Loss: 0.0040\n",
            "Epoch [119/200], Batch [25/50], Loss: 0.0033\n",
            "Epoch [119/200], Batch [30/50], Loss: 0.0042\n",
            "Epoch [119/200], Batch [35/50], Loss: 0.0032\n",
            "Epoch [119/200], Batch [40/50], Loss: 0.0031\n",
            "Epoch [119/200], Batch [45/50], Loss: 0.0021\n",
            "Epoch [119/200], Batch [50/50], Loss: 0.0081\n",
            "Epoch 119: Train Loss = 0.2517, Accuracy = 100.00%\n",
            "Epoch [120/200], Batch [5/50], Loss: 0.0035\n",
            "Epoch [120/200], Batch [10/50], Loss: 0.0042\n",
            "Epoch [120/200], Batch [15/50], Loss: 0.0041\n",
            "Epoch [120/200], Batch [20/50], Loss: 0.0059\n",
            "Epoch [120/200], Batch [25/50], Loss: 0.0064\n",
            "Epoch [120/200], Batch [30/50], Loss: 0.0057\n",
            "Epoch [120/200], Batch [35/50], Loss: 0.0072\n",
            "Epoch [120/200], Batch [40/50], Loss: 0.0040\n",
            "Epoch [120/200], Batch [45/50], Loss: 0.0094\n",
            "Epoch [120/200], Batch [50/50], Loss: 0.0029\n",
            "Epoch 120: Train Loss = 0.2525, Accuracy = 100.00%\n",
            "Epoch [121/200], Batch [5/50], Loss: 0.0053\n",
            "Epoch [121/200], Batch [10/50], Loss: 0.0032\n",
            "Epoch [121/200], Batch [15/50], Loss: 0.0053\n",
            "Epoch [121/200], Batch [20/50], Loss: 0.0066\n",
            "Epoch [121/200], Batch [25/50], Loss: 0.0044\n",
            "Epoch [121/200], Batch [30/50], Loss: 0.0044\n",
            "Epoch [121/200], Batch [35/50], Loss: 0.0052\n",
            "Epoch [121/200], Batch [40/50], Loss: 0.0035\n",
            "Epoch [121/200], Batch [45/50], Loss: 0.0043\n",
            "Epoch [121/200], Batch [50/50], Loss: 0.0073\n",
            "Epoch 121: Train Loss = 0.2506, Accuracy = 100.00%\n",
            "Epoch [122/200], Batch [5/50], Loss: 0.0066\n",
            "Epoch [122/200], Batch [10/50], Loss: 0.0035\n",
            "Epoch [122/200], Batch [15/50], Loss: 0.0027\n",
            "Epoch [122/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [122/200], Batch [25/50], Loss: 0.0028\n",
            "Epoch [122/200], Batch [30/50], Loss: 0.0060\n",
            "Epoch [122/200], Batch [35/50], Loss: 0.0068\n",
            "Epoch [122/200], Batch [40/50], Loss: 0.0058\n",
            "Epoch [122/200], Batch [45/50], Loss: 0.0066\n",
            "Epoch [122/200], Batch [50/50], Loss: 0.0037\n",
            "Epoch 122: Train Loss = 0.2588, Accuracy = 100.00%\n",
            "Epoch [123/200], Batch [5/50], Loss: 0.0092\n",
            "Epoch [123/200], Batch [10/50], Loss: 0.0051\n",
            "Epoch [123/200], Batch [15/50], Loss: 0.0038\n",
            "Epoch [123/200], Batch [20/50], Loss: 0.0068\n",
            "Epoch [123/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [123/200], Batch [30/50], Loss: 0.0043\n",
            "Epoch [123/200], Batch [35/50], Loss: 0.0069\n",
            "Epoch [123/200], Batch [40/50], Loss: 0.0011\n",
            "Epoch [123/200], Batch [45/50], Loss: 0.0030\n",
            "Epoch [123/200], Batch [50/50], Loss: 0.0053\n",
            "Epoch 123: Train Loss = 0.2589, Accuracy = 100.00%\n",
            "Epoch [124/200], Batch [5/50], Loss: 0.0075\n",
            "Epoch [124/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [124/200], Batch [15/50], Loss: 0.0046\n",
            "Epoch [124/200], Batch [20/50], Loss: 0.0060\n",
            "Epoch [124/200], Batch [25/50], Loss: 0.0046\n",
            "Epoch [124/200], Batch [30/50], Loss: 0.0050\n",
            "Epoch [124/200], Batch [35/50], Loss: 0.0104\n",
            "Epoch [124/200], Batch [40/50], Loss: 0.0034\n",
            "Epoch [124/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [124/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 124: Train Loss = 0.2516, Accuracy = 100.00%\n",
            "Epoch [125/200], Batch [5/50], Loss: 0.0034\n",
            "Epoch [125/200], Batch [10/50], Loss: 0.0049\n",
            "Epoch [125/200], Batch [15/50], Loss: 0.0076\n",
            "Epoch [125/200], Batch [20/50], Loss: 0.0017\n",
            "Epoch [125/200], Batch [25/50], Loss: 0.0060\n",
            "Epoch [125/200], Batch [30/50], Loss: 0.0048\n",
            "Epoch [125/200], Batch [35/50], Loss: 0.0040\n",
            "Epoch [125/200], Batch [40/50], Loss: 0.0038\n",
            "Epoch [125/200], Batch [45/50], Loss: 0.0063\n",
            "Epoch [125/200], Batch [50/50], Loss: 0.0082\n",
            "Epoch 125: Train Loss = 0.2524, Accuracy = 100.00%\n",
            "Epoch [126/200], Batch [5/50], Loss: 0.0037\n",
            "Epoch [126/200], Batch [10/50], Loss: 0.0047\n",
            "Epoch [126/200], Batch [15/50], Loss: 0.0046\n",
            "Epoch [126/200], Batch [20/50], Loss: 0.0018\n",
            "Epoch [126/200], Batch [25/50], Loss: 0.0019\n",
            "Epoch [126/200], Batch [30/50], Loss: 0.0028\n",
            "Epoch [126/200], Batch [35/50], Loss: 0.0058\n",
            "Epoch [126/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [126/200], Batch [45/50], Loss: 0.0027\n",
            "Epoch [126/200], Batch [50/50], Loss: 0.0047\n",
            "Epoch 126: Train Loss = 0.2552, Accuracy = 100.00%\n",
            "Epoch [127/200], Batch [5/50], Loss: 0.0033\n",
            "Epoch [127/200], Batch [10/50], Loss: 0.0075\n",
            "Epoch [127/200], Batch [15/50], Loss: 0.0045\n",
            "Epoch [127/200], Batch [20/50], Loss: 0.0042\n",
            "Epoch [127/200], Batch [25/50], Loss: 0.0025\n",
            "Epoch [127/200], Batch [30/50], Loss: 0.0041\n",
            "Epoch [127/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [127/200], Batch [40/50], Loss: 0.0052\n",
            "Epoch [127/200], Batch [45/50], Loss: 0.0033\n",
            "Epoch [127/200], Batch [50/50], Loss: 0.0050\n",
            "Epoch 127: Train Loss = 0.2446, Accuracy = 100.00%\n",
            "Epoch [128/200], Batch [5/50], Loss: 0.0048\n",
            "Epoch [128/200], Batch [10/50], Loss: 0.0038\n",
            "Epoch [128/200], Batch [15/50], Loss: 0.0056\n",
            "Epoch [128/200], Batch [20/50], Loss: 0.0030\n",
            "Epoch [128/200], Batch [25/50], Loss: 0.0031\n",
            "Epoch [128/200], Batch [30/50], Loss: 0.0070\n",
            "Epoch [128/200], Batch [35/50], Loss: 0.0033\n",
            "Epoch [128/200], Batch [40/50], Loss: 0.0045\n",
            "Epoch [128/200], Batch [45/50], Loss: 0.0060\n",
            "Epoch [128/200], Batch [50/50], Loss: 0.0119\n",
            "Epoch 128: Train Loss = 0.2419, Accuracy = 100.00%\n",
            "Epoch [129/200], Batch [5/50], Loss: 0.0071\n",
            "Epoch [129/200], Batch [10/50], Loss: 0.0061\n",
            "Epoch [129/200], Batch [15/50], Loss: 0.0056\n",
            "Epoch [129/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [129/200], Batch [25/50], Loss: 0.0013\n",
            "Epoch [129/200], Batch [30/50], Loss: 0.0037\n",
            "Epoch [129/200], Batch [35/50], Loss: 0.0063\n",
            "Epoch [129/200], Batch [40/50], Loss: 0.0066\n",
            "Epoch [129/200], Batch [45/50], Loss: 0.0038\n",
            "Epoch [129/200], Batch [50/50], Loss: 0.0063\n",
            "Epoch 129: Train Loss = 0.2418, Accuracy = 100.00%\n",
            "Epoch [130/200], Batch [5/50], Loss: 0.0063\n",
            "Epoch [130/200], Batch [10/50], Loss: 0.0078\n",
            "Epoch [130/200], Batch [15/50], Loss: 0.0053\n",
            "Epoch [130/200], Batch [20/50], Loss: 0.0048\n",
            "Epoch [130/200], Batch [25/50], Loss: 0.0045\n",
            "Epoch [130/200], Batch [30/50], Loss: 0.0021\n",
            "Epoch [130/200], Batch [35/50], Loss: 0.0045\n",
            "Epoch [130/200], Batch [40/50], Loss: 0.0058\n",
            "Epoch [130/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [130/200], Batch [50/50], Loss: 0.0044\n",
            "Epoch 130: Train Loss = 0.2416, Accuracy = 100.00%\n",
            "Epoch [131/200], Batch [5/50], Loss: 0.0048\n",
            "Epoch [131/200], Batch [10/50], Loss: 0.0017\n",
            "Epoch [131/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [131/200], Batch [20/50], Loss: 0.0083\n",
            "Epoch [131/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [131/200], Batch [30/50], Loss: 0.0045\n",
            "Epoch [131/200], Batch [35/50], Loss: 0.0047\n",
            "Epoch [131/200], Batch [40/50], Loss: 0.0050\n",
            "Epoch [131/200], Batch [45/50], Loss: 0.0025\n",
            "Epoch [131/200], Batch [50/50], Loss: 0.0047\n",
            "Epoch 131: Train Loss = 0.2367, Accuracy = 100.00%\n",
            "Epoch [132/200], Batch [5/50], Loss: 0.0057\n",
            "Epoch [132/200], Batch [10/50], Loss: 0.0045\n",
            "Epoch [132/200], Batch [15/50], Loss: 0.0047\n",
            "Epoch [132/200], Batch [20/50], Loss: 0.0067\n",
            "Epoch [132/200], Batch [25/50], Loss: 0.0068\n",
            "Epoch [132/200], Batch [30/50], Loss: 0.0050\n",
            "Epoch [132/200], Batch [35/50], Loss: 0.0039\n",
            "Epoch [132/200], Batch [40/50], Loss: 0.0067\n",
            "Epoch [132/200], Batch [45/50], Loss: 0.0049\n",
            "Epoch [132/200], Batch [50/50], Loss: 0.0052\n",
            "Epoch 132: Train Loss = 0.2425, Accuracy = 100.00%\n",
            "Epoch [133/200], Batch [5/50], Loss: 0.0077\n",
            "Epoch [133/200], Batch [10/50], Loss: 0.0050\n",
            "Epoch [133/200], Batch [15/50], Loss: 0.0037\n",
            "Epoch [133/200], Batch [20/50], Loss: 0.0055\n",
            "Epoch [133/200], Batch [25/50], Loss: 0.0068\n",
            "Epoch [133/200], Batch [30/50], Loss: 0.0049\n",
            "Epoch [133/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [133/200], Batch [40/50], Loss: 0.0037\n",
            "Epoch [133/200], Batch [45/50], Loss: 0.0041\n",
            "Epoch [133/200], Batch [50/50], Loss: 0.0056\n",
            "Epoch 133: Train Loss = 0.2358, Accuracy = 100.00%\n",
            "Epoch [134/200], Batch [5/50], Loss: 0.0053\n",
            "Epoch [134/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [134/200], Batch [15/50], Loss: 0.0079\n",
            "Epoch [134/200], Batch [20/50], Loss: 0.0045\n",
            "Epoch [134/200], Batch [25/50], Loss: 0.0047\n",
            "Epoch [134/200], Batch [30/50], Loss: 0.0036\n",
            "Epoch [134/200], Batch [35/50], Loss: 0.0036\n",
            "Epoch [134/200], Batch [40/50], Loss: 0.0039\n",
            "Epoch [134/200], Batch [45/50], Loss: 0.0023\n",
            "Epoch [134/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 134: Train Loss = 0.2335, Accuracy = 100.00%\n",
            "Epoch [135/200], Batch [5/50], Loss: 0.0039\n",
            "Epoch [135/200], Batch [10/50], Loss: 0.0032\n",
            "Epoch [135/200], Batch [15/50], Loss: 0.0048\n",
            "Epoch [135/200], Batch [20/50], Loss: 0.0032\n",
            "Epoch [135/200], Batch [25/50], Loss: 0.0036\n",
            "Epoch [135/200], Batch [30/50], Loss: 0.0071\n",
            "Epoch [135/200], Batch [35/50], Loss: 0.0018\n",
            "Epoch [135/200], Batch [40/50], Loss: 0.0071\n",
            "Epoch [135/200], Batch [45/50], Loss: 0.0066\n",
            "Epoch [135/200], Batch [50/50], Loss: 0.0063\n",
            "Epoch 135: Train Loss = 0.2331, Accuracy = 100.00%\n",
            "Epoch [136/200], Batch [5/50], Loss: 0.0049\n",
            "Epoch [136/200], Batch [10/50], Loss: 0.0040\n",
            "Epoch [136/200], Batch [15/50], Loss: 0.0047\n",
            "Epoch [136/200], Batch [20/50], Loss: 0.0032\n",
            "Epoch [136/200], Batch [25/50], Loss: 0.0025\n",
            "Epoch [136/200], Batch [30/50], Loss: 0.0037\n",
            "Epoch [136/200], Batch [35/50], Loss: 0.0061\n",
            "Epoch [136/200], Batch [40/50], Loss: 0.0032\n",
            "Epoch [136/200], Batch [45/50], Loss: 0.0057\n",
            "Epoch [136/200], Batch [50/50], Loss: 0.0050\n",
            "Epoch 136: Train Loss = 0.2348, Accuracy = 100.00%\n",
            "Epoch [137/200], Batch [5/50], Loss: 0.0063\n",
            "Epoch [137/200], Batch [10/50], Loss: 0.0057\n",
            "Epoch [137/200], Batch [15/50], Loss: 0.0017\n",
            "Epoch [137/200], Batch [20/50], Loss: 0.0030\n",
            "Epoch [137/200], Batch [25/50], Loss: 0.0045\n",
            "Epoch [137/200], Batch [30/50], Loss: 0.0021\n",
            "Epoch [137/200], Batch [35/50], Loss: 0.0047\n",
            "Epoch [137/200], Batch [40/50], Loss: 0.0026\n",
            "Epoch [137/200], Batch [45/50], Loss: 0.0061\n",
            "Epoch [137/200], Batch [50/50], Loss: 0.0073\n",
            "Epoch 137: Train Loss = 0.2331, Accuracy = 100.00%\n",
            "Epoch [138/200], Batch [5/50], Loss: 0.0043\n",
            "Epoch [138/200], Batch [10/50], Loss: 0.0044\n",
            "Epoch [138/200], Batch [15/50], Loss: 0.0044\n",
            "Epoch [138/200], Batch [20/50], Loss: 0.0056\n",
            "Epoch [138/200], Batch [25/50], Loss: 0.0053\n",
            "Epoch [138/200], Batch [30/50], Loss: 0.0031\n",
            "Epoch [138/200], Batch [35/50], Loss: 0.0051\n",
            "Epoch [138/200], Batch [40/50], Loss: 0.0073\n",
            "Epoch [138/200], Batch [45/50], Loss: 0.0034\n",
            "Epoch [138/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 138: Train Loss = 0.2299, Accuracy = 100.00%\n",
            "Epoch [139/200], Batch [5/50], Loss: 0.0031\n",
            "Epoch [139/200], Batch [10/50], Loss: 0.0055\n",
            "Epoch [139/200], Batch [15/50], Loss: 0.0055\n",
            "Epoch [139/200], Batch [20/50], Loss: 0.0035\n",
            "Epoch [139/200], Batch [25/50], Loss: 0.0028\n",
            "Epoch [139/200], Batch [30/50], Loss: 0.0048\n",
            "Epoch [139/200], Batch [35/50], Loss: 0.0085\n",
            "Epoch [139/200], Batch [40/50], Loss: 0.0057\n",
            "Epoch [139/200], Batch [45/50], Loss: 0.0073\n",
            "Epoch [139/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 139: Train Loss = 0.2306, Accuracy = 100.00%\n",
            "Epoch [140/200], Batch [5/50], Loss: 0.0068\n",
            "Epoch [140/200], Batch [10/50], Loss: 0.0059\n",
            "Epoch [140/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [140/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [140/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [140/200], Batch [30/50], Loss: 0.0030\n",
            "Epoch [140/200], Batch [35/50], Loss: 0.0020\n",
            "Epoch [140/200], Batch [40/50], Loss: 0.0037\n",
            "Epoch [140/200], Batch [45/50], Loss: 0.0019\n",
            "Epoch [140/200], Batch [50/50], Loss: 0.0051\n",
            "Epoch 140: Train Loss = 0.2276, Accuracy = 100.00%\n",
            "Epoch [141/200], Batch [5/50], Loss: 0.0042\n",
            "Epoch [141/200], Batch [10/50], Loss: 0.0026\n",
            "Epoch [141/200], Batch [15/50], Loss: 0.0025\n",
            "Epoch [141/200], Batch [20/50], Loss: 0.0031\n",
            "Epoch [141/200], Batch [25/50], Loss: 0.0063\n",
            "Epoch [141/200], Batch [30/50], Loss: 0.0040\n",
            "Epoch [141/200], Batch [35/50], Loss: 0.0037\n",
            "Epoch [141/200], Batch [40/50], Loss: 0.0023\n",
            "Epoch [141/200], Batch [45/50], Loss: 0.0047\n",
            "Epoch [141/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 141: Train Loss = 0.2297, Accuracy = 100.00%\n",
            "Epoch [142/200], Batch [5/50], Loss: 0.0033\n",
            "Epoch [142/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [142/200], Batch [15/50], Loss: 0.0034\n",
            "Epoch [142/200], Batch [20/50], Loss: 0.0044\n",
            "Epoch [142/200], Batch [25/50], Loss: 0.0061\n",
            "Epoch [142/200], Batch [30/50], Loss: 0.0038\n",
            "Epoch [142/200], Batch [35/50], Loss: 0.0023\n",
            "Epoch [142/200], Batch [40/50], Loss: 0.0067\n",
            "Epoch [142/200], Batch [45/50], Loss: 0.0047\n",
            "Epoch [142/200], Batch [50/50], Loss: 0.0036\n",
            "Epoch 142: Train Loss = 0.2287, Accuracy = 100.00%\n",
            "Epoch [143/200], Batch [5/50], Loss: 0.0024\n",
            "Epoch [143/200], Batch [10/50], Loss: 0.0034\n",
            "Epoch [143/200], Batch [15/50], Loss: 0.0061\n",
            "Epoch [143/200], Batch [20/50], Loss: 0.0045\n",
            "Epoch [143/200], Batch [25/50], Loss: 0.0079\n",
            "Epoch [143/200], Batch [30/50], Loss: 0.0060\n",
            "Epoch [143/200], Batch [35/50], Loss: 0.0066\n",
            "Epoch [143/200], Batch [40/50], Loss: 0.0027\n",
            "Epoch [143/200], Batch [45/50], Loss: 0.0030\n",
            "Epoch [143/200], Batch [50/50], Loss: 0.0082\n",
            "Epoch 143: Train Loss = 0.2311, Accuracy = 100.00%\n",
            "Epoch [144/200], Batch [5/50], Loss: 0.0041\n",
            "Epoch [144/200], Batch [10/50], Loss: 0.0052\n",
            "Epoch [144/200], Batch [15/50], Loss: 0.0093\n",
            "Epoch [144/200], Batch [20/50], Loss: 0.0037\n",
            "Epoch [144/200], Batch [25/50], Loss: 0.0042\n",
            "Epoch [144/200], Batch [30/50], Loss: 0.0042\n",
            "Epoch [144/200], Batch [35/50], Loss: 0.0037\n",
            "Epoch [144/200], Batch [40/50], Loss: 0.0044\n",
            "Epoch [144/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [144/200], Batch [50/50], Loss: 0.0064\n",
            "Epoch 144: Train Loss = 0.2306, Accuracy = 100.00%\n",
            "Epoch [145/200], Batch [5/50], Loss: 0.0043\n",
            "Epoch [145/200], Batch [10/50], Loss: 0.0047\n",
            "Epoch [145/200], Batch [15/50], Loss: 0.0063\n",
            "Epoch [145/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [145/200], Batch [25/50], Loss: 0.0027\n",
            "Epoch [145/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [145/200], Batch [35/50], Loss: 0.0022\n",
            "Epoch [145/200], Batch [40/50], Loss: 0.0039\n",
            "Epoch [145/200], Batch [45/50], Loss: 0.0037\n",
            "Epoch [145/200], Batch [50/50], Loss: 0.0077\n",
            "Epoch 145: Train Loss = 0.2242, Accuracy = 100.00%\n",
            "Epoch [146/200], Batch [5/50], Loss: 0.0063\n",
            "Epoch [146/200], Batch [10/50], Loss: 0.0028\n",
            "Epoch [146/200], Batch [15/50], Loss: 0.0063\n",
            "Epoch [146/200], Batch [20/50], Loss: 0.0049\n",
            "Epoch [146/200], Batch [25/50], Loss: 0.0075\n",
            "Epoch [146/200], Batch [30/50], Loss: 0.0035\n",
            "Epoch [146/200], Batch [35/50], Loss: 0.0041\n",
            "Epoch [146/200], Batch [40/50], Loss: 0.0023\n",
            "Epoch [146/200], Batch [45/50], Loss: 0.0022\n",
            "Epoch [146/200], Batch [50/50], Loss: 0.0041\n",
            "Epoch 146: Train Loss = 0.2275, Accuracy = 100.00%\n",
            "Epoch [147/200], Batch [5/50], Loss: 0.0082\n",
            "Epoch [147/200], Batch [10/50], Loss: 0.0050\n",
            "Epoch [147/200], Batch [15/50], Loss: 0.0043\n",
            "Epoch [147/200], Batch [20/50], Loss: 0.0066\n",
            "Epoch [147/200], Batch [25/50], Loss: 0.0075\n",
            "Epoch [147/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [147/200], Batch [35/50], Loss: 0.0030\n",
            "Epoch [147/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [147/200], Batch [45/50], Loss: 0.0032\n",
            "Epoch [147/200], Batch [50/50], Loss: 0.0022\n",
            "Epoch 147: Train Loss = 0.2236, Accuracy = 100.00%\n",
            "Epoch [148/200], Batch [5/50], Loss: 0.0040\n",
            "Epoch [148/200], Batch [10/50], Loss: 0.0064\n",
            "Epoch [148/200], Batch [15/50], Loss: 0.0064\n",
            "Epoch [148/200], Batch [20/50], Loss: 0.0041\n",
            "Epoch [148/200], Batch [25/50], Loss: 0.0065\n",
            "Epoch [148/200], Batch [30/50], Loss: 0.0060\n",
            "Epoch [148/200], Batch [35/50], Loss: 0.0036\n",
            "Epoch [148/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [148/200], Batch [45/50], Loss: 0.0058\n",
            "Epoch [148/200], Batch [50/50], Loss: 0.0030\n",
            "Epoch 148: Train Loss = 0.2258, Accuracy = 100.00%\n",
            "Epoch [149/200], Batch [5/50], Loss: 0.0021\n",
            "Epoch [149/200], Batch [10/50], Loss: 0.0073\n",
            "Epoch [149/200], Batch [15/50], Loss: 0.0024\n",
            "Epoch [149/200], Batch [20/50], Loss: 0.0039\n",
            "Epoch [149/200], Batch [25/50], Loss: 0.0068\n",
            "Epoch [149/200], Batch [30/50], Loss: 0.0039\n",
            "Epoch [149/200], Batch [35/50], Loss: 0.0046\n",
            "Epoch [149/200], Batch [40/50], Loss: 0.0044\n",
            "Epoch [149/200], Batch [45/50], Loss: 0.0024\n",
            "Epoch [149/200], Batch [50/50], Loss: 0.0023\n",
            "Epoch 149: Train Loss = 0.2234, Accuracy = 100.00%\n",
            "Epoch [150/200], Batch [5/50], Loss: 0.0029\n",
            "Epoch [150/200], Batch [10/50], Loss: 0.0024\n",
            "Epoch [150/200], Batch [15/50], Loss: 0.0061\n",
            "Epoch [150/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [150/200], Batch [25/50], Loss: 0.0052\n",
            "Epoch [150/200], Batch [30/50], Loss: 0.0050\n",
            "Epoch [150/200], Batch [35/50], Loss: 0.0047\n",
            "Epoch [150/200], Batch [40/50], Loss: 0.0056\n",
            "Epoch [150/200], Batch [45/50], Loss: 0.0059\n",
            "Epoch [150/200], Batch [50/50], Loss: 0.0036\n",
            "Epoch 150: Train Loss = 0.2239, Accuracy = 100.00%\n",
            "Epoch [151/200], Batch [5/50], Loss: 0.0060\n",
            "Epoch [151/200], Batch [10/50], Loss: 0.0045\n",
            "Epoch [151/200], Batch [15/50], Loss: 0.0036\n",
            "Epoch [151/200], Batch [20/50], Loss: 0.0047\n",
            "Epoch [151/200], Batch [25/50], Loss: 0.0036\n",
            "Epoch [151/200], Batch [30/50], Loss: 0.0055\n",
            "Epoch [151/200], Batch [35/50], Loss: 0.0051\n",
            "Epoch [151/200], Batch [40/50], Loss: 0.0058\n",
            "Epoch [151/200], Batch [45/50], Loss: 0.0051\n",
            "Epoch [151/200], Batch [50/50], Loss: 0.0078\n",
            "Epoch 151: Train Loss = 0.2224, Accuracy = 100.00%\n",
            "Epoch [152/200], Batch [5/50], Loss: 0.0023\n",
            "Epoch [152/200], Batch [10/50], Loss: 0.0033\n",
            "Epoch [152/200], Batch [15/50], Loss: 0.0038\n",
            "Epoch [152/200], Batch [20/50], Loss: 0.0039\n",
            "Epoch [152/200], Batch [25/50], Loss: 0.0032\n",
            "Epoch [152/200], Batch [30/50], Loss: 0.0081\n",
            "Epoch [152/200], Batch [35/50], Loss: 0.0027\n",
            "Epoch [152/200], Batch [40/50], Loss: 0.0031\n",
            "Epoch [152/200], Batch [45/50], Loss: 0.0044\n",
            "Epoch [152/200], Batch [50/50], Loss: 0.0065\n",
            "Epoch 152: Train Loss = 0.2217, Accuracy = 100.00%\n",
            "Epoch [153/200], Batch [5/50], Loss: 0.0042\n",
            "Epoch [153/200], Batch [10/50], Loss: 0.0032\n",
            "Epoch [153/200], Batch [15/50], Loss: 0.0020\n",
            "Epoch [153/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [153/200], Batch [25/50], Loss: 0.0033\n",
            "Epoch [153/200], Batch [30/50], Loss: 0.0035\n",
            "Epoch [153/200], Batch [35/50], Loss: 0.0054\n",
            "Epoch [153/200], Batch [40/50], Loss: 0.0050\n",
            "Epoch [153/200], Batch [45/50], Loss: 0.0037\n",
            "Epoch [153/200], Batch [50/50], Loss: 0.0057\n",
            "Epoch 153: Train Loss = 0.2207, Accuracy = 100.00%\n",
            "Epoch [154/200], Batch [5/50], Loss: 0.0050\n",
            "Epoch [154/200], Batch [10/50], Loss: 0.0030\n",
            "Epoch [154/200], Batch [15/50], Loss: 0.0038\n",
            "Epoch [154/200], Batch [20/50], Loss: 0.0048\n",
            "Epoch [154/200], Batch [25/50], Loss: 0.0034\n",
            "Epoch [154/200], Batch [30/50], Loss: 0.0044\n",
            "Epoch [154/200], Batch [35/50], Loss: 0.0056\n",
            "Epoch [154/200], Batch [40/50], Loss: 0.0047\n",
            "Epoch [154/200], Batch [45/50], Loss: 0.0049\n",
            "Epoch [154/200], Batch [50/50], Loss: 0.0061\n",
            "Epoch 154: Train Loss = 0.2211, Accuracy = 100.00%\n",
            "Epoch [155/200], Batch [5/50], Loss: 0.0031\n",
            "Epoch [155/200], Batch [10/50], Loss: 0.0036\n",
            "Epoch [155/200], Batch [15/50], Loss: 0.0044\n",
            "Epoch [155/200], Batch [20/50], Loss: 0.0026\n",
            "Epoch [155/200], Batch [25/50], Loss: 0.0040\n",
            "Epoch [155/200], Batch [30/50], Loss: 0.0049\n",
            "Epoch [155/200], Batch [35/50], Loss: 0.0025\n",
            "Epoch [155/200], Batch [40/50], Loss: 0.0047\n",
            "Epoch [155/200], Batch [45/50], Loss: 0.0043\n",
            "Epoch [155/200], Batch [50/50], Loss: 0.0074\n",
            "Epoch 155: Train Loss = 0.2207, Accuracy = 100.00%\n",
            "Epoch [156/200], Batch [5/50], Loss: 0.0022\n",
            "Epoch [156/200], Batch [10/50], Loss: 0.0037\n",
            "Epoch [156/200], Batch [15/50], Loss: 0.0066\n",
            "Epoch [156/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [156/200], Batch [25/50], Loss: 0.0048\n",
            "Epoch [156/200], Batch [30/50], Loss: 0.0039\n",
            "Epoch [156/200], Batch [35/50], Loss: 0.0046\n",
            "Epoch [156/200], Batch [40/50], Loss: 0.0060\n",
            "Epoch [156/200], Batch [45/50], Loss: 0.0071\n",
            "Epoch [156/200], Batch [50/50], Loss: 0.0081\n",
            "Epoch 156: Train Loss = 0.2204, Accuracy = 100.00%\n",
            "Epoch [157/200], Batch [5/50], Loss: 0.0027\n",
            "Epoch [157/200], Batch [10/50], Loss: 0.0052\n",
            "Epoch [157/200], Batch [15/50], Loss: 0.0058\n",
            "Epoch [157/200], Batch [20/50], Loss: 0.0061\n",
            "Epoch [157/200], Batch [25/50], Loss: 0.0045\n",
            "Epoch [157/200], Batch [30/50], Loss: 0.0034\n",
            "Epoch [157/200], Batch [35/50], Loss: 0.0034\n",
            "Epoch [157/200], Batch [40/50], Loss: 0.0042\n",
            "Epoch [157/200], Batch [45/50], Loss: 0.0068\n",
            "Epoch [157/200], Batch [50/50], Loss: 0.0051\n",
            "Epoch 157: Train Loss = 0.2207, Accuracy = 100.00%\n",
            "Epoch [158/200], Batch [5/50], Loss: 0.0027\n",
            "Epoch [158/200], Batch [10/50], Loss: 0.0038\n",
            "Epoch [158/200], Batch [15/50], Loss: 0.0020\n",
            "Epoch [158/200], Batch [20/50], Loss: 0.0065\n",
            "Epoch [158/200], Batch [25/50], Loss: 0.0065\n",
            "Epoch [158/200], Batch [30/50], Loss: 0.0045\n",
            "Epoch [158/200], Batch [35/50], Loss: 0.0077\n",
            "Epoch [158/200], Batch [40/50], Loss: 0.0041\n",
            "Epoch [158/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [158/200], Batch [50/50], Loss: 0.0027\n",
            "Epoch 158: Train Loss = 0.2202, Accuracy = 100.00%\n",
            "Epoch [159/200], Batch [5/50], Loss: 0.0043\n",
            "Epoch [159/200], Batch [10/50], Loss: 0.0024\n",
            "Epoch [159/200], Batch [15/50], Loss: 0.0050\n",
            "Epoch [159/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [159/200], Batch [25/50], Loss: 0.0021\n",
            "Epoch [159/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [159/200], Batch [35/50], Loss: 0.0070\n",
            "Epoch [159/200], Batch [40/50], Loss: 0.0018\n",
            "Epoch [159/200], Batch [45/50], Loss: 0.0037\n",
            "Epoch [159/200], Batch [50/50], Loss: 0.0034\n",
            "Epoch 159: Train Loss = 0.2194, Accuracy = 100.00%\n",
            "Epoch [160/200], Batch [5/50], Loss: 0.0055\n",
            "Epoch [160/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [160/200], Batch [15/50], Loss: 0.0023\n",
            "Epoch [160/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [160/200], Batch [25/50], Loss: 0.0050\n",
            "Epoch [160/200], Batch [30/50], Loss: 0.0047\n",
            "Epoch [160/200], Batch [35/50], Loss: 0.0042\n",
            "Epoch [160/200], Batch [40/50], Loss: 0.0034\n",
            "Epoch [160/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [160/200], Batch [50/50], Loss: 0.0036\n",
            "Epoch 160: Train Loss = 0.2187, Accuracy = 100.00%\n",
            "Epoch [161/200], Batch [5/50], Loss: 0.0026\n",
            "Epoch [161/200], Batch [10/50], Loss: 0.0041\n",
            "Epoch [161/200], Batch [15/50], Loss: 0.0040\n",
            "Epoch [161/200], Batch [20/50], Loss: 0.0031\n",
            "Epoch [161/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [161/200], Batch [30/50], Loss: 0.0084\n",
            "Epoch [161/200], Batch [35/50], Loss: 0.0060\n",
            "Epoch [161/200], Batch [40/50], Loss: 0.0044\n",
            "Epoch [161/200], Batch [45/50], Loss: 0.0022\n",
            "Epoch [161/200], Batch [50/50], Loss: 0.0033\n",
            "Epoch 161: Train Loss = 0.2195, Accuracy = 100.00%\n",
            "Epoch [162/200], Batch [5/50], Loss: 0.0052\n",
            "Epoch [162/200], Batch [10/50], Loss: 0.0035\n",
            "Epoch [162/200], Batch [15/50], Loss: 0.0069\n",
            "Epoch [162/200], Batch [20/50], Loss: 0.0050\n",
            "Epoch [162/200], Batch [25/50], Loss: 0.0042\n",
            "Epoch [162/200], Batch [30/50], Loss: 0.0039\n",
            "Epoch [162/200], Batch [35/50], Loss: 0.0030\n",
            "Epoch [162/200], Batch [40/50], Loss: 0.0047\n",
            "Epoch [162/200], Batch [45/50], Loss: 0.0053\n",
            "Epoch [162/200], Batch [50/50], Loss: 0.0043\n",
            "Epoch 162: Train Loss = 0.2197, Accuracy = 100.00%\n",
            "Epoch [163/200], Batch [5/50], Loss: 0.0060\n",
            "Epoch [163/200], Batch [10/50], Loss: 0.0082\n",
            "Epoch [163/200], Batch [15/50], Loss: 0.0028\n",
            "Epoch [163/200], Batch [20/50], Loss: 0.0058\n",
            "Epoch [163/200], Batch [25/50], Loss: 0.0049\n",
            "Epoch [163/200], Batch [30/50], Loss: 0.0057\n",
            "Epoch [163/200], Batch [35/50], Loss: 0.0028\n",
            "Epoch [163/200], Batch [40/50], Loss: 0.0054\n",
            "Epoch [163/200], Batch [45/50], Loss: 0.0034\n",
            "Epoch [163/200], Batch [50/50], Loss: 0.0062\n",
            "Epoch 163: Train Loss = 0.2190, Accuracy = 100.00%\n",
            "Epoch [164/200], Batch [5/50], Loss: 0.0045\n",
            "Epoch [164/200], Batch [10/50], Loss: 0.0039\n",
            "Epoch [164/200], Batch [15/50], Loss: 0.0020\n",
            "Epoch [164/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [164/200], Batch [25/50], Loss: 0.0030\n",
            "Epoch [164/200], Batch [30/50], Loss: 0.0053\n",
            "Epoch [164/200], Batch [35/50], Loss: 0.0039\n",
            "Epoch [164/200], Batch [40/50], Loss: 0.0062\n",
            "Epoch [164/200], Batch [45/50], Loss: 0.0062\n",
            "Epoch [164/200], Batch [50/50], Loss: 0.0085\n",
            "Epoch 164: Train Loss = 0.2184, Accuracy = 100.00%\n",
            "Epoch [165/200], Batch [5/50], Loss: 0.0050\n",
            "Epoch [165/200], Batch [10/50], Loss: 0.0045\n",
            "Epoch [165/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [165/200], Batch [20/50], Loss: 0.0024\n",
            "Epoch [165/200], Batch [25/50], Loss: 0.0056\n",
            "Epoch [165/200], Batch [30/50], Loss: 0.0067\n",
            "Epoch [165/200], Batch [35/50], Loss: 0.0043\n",
            "Epoch [165/200], Batch [40/50], Loss: 0.0051\n",
            "Epoch [165/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [165/200], Batch [50/50], Loss: 0.0037\n",
            "Epoch 165: Train Loss = 0.2180, Accuracy = 100.00%\n",
            "Epoch [166/200], Batch [5/50], Loss: 0.0064\n",
            "Epoch [166/200], Batch [10/50], Loss: 0.0084\n",
            "Epoch [166/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [166/200], Batch [20/50], Loss: 0.0041\n",
            "Epoch [166/200], Batch [25/50], Loss: 0.0075\n",
            "Epoch [166/200], Batch [30/50], Loss: 0.0065\n",
            "Epoch [166/200], Batch [35/50], Loss: 0.0024\n",
            "Epoch [166/200], Batch [40/50], Loss: 0.0029\n",
            "Epoch [166/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [166/200], Batch [50/50], Loss: 0.0058\n",
            "Epoch 166: Train Loss = 0.2182, Accuracy = 100.00%\n",
            "Epoch [167/200], Batch [5/50], Loss: 0.0027\n",
            "Epoch [167/200], Batch [10/50], Loss: 0.0029\n",
            "Epoch [167/200], Batch [15/50], Loss: 0.0055\n",
            "Epoch [167/200], Batch [20/50], Loss: 0.0059\n",
            "Epoch [167/200], Batch [25/50], Loss: 0.0028\n",
            "Epoch [167/200], Batch [30/50], Loss: 0.0038\n",
            "Epoch [167/200], Batch [35/50], Loss: 0.0048\n",
            "Epoch [167/200], Batch [40/50], Loss: 0.0072\n",
            "Epoch [167/200], Batch [45/50], Loss: 0.0055\n",
            "Epoch [167/200], Batch [50/50], Loss: 0.0029\n",
            "Epoch 167: Train Loss = 0.2184, Accuracy = 100.00%\n",
            "Epoch [168/200], Batch [5/50], Loss: 0.0028\n",
            "Epoch [168/200], Batch [10/50], Loss: 0.0039\n",
            "Epoch [168/200], Batch [15/50], Loss: 0.0044\n",
            "Epoch [168/200], Batch [20/50], Loss: 0.0037\n",
            "Epoch [168/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [168/200], Batch [30/50], Loss: 0.0040\n",
            "Epoch [168/200], Batch [35/50], Loss: 0.0063\n",
            "Epoch [168/200], Batch [40/50], Loss: 0.0048\n",
            "Epoch [168/200], Batch [45/50], Loss: 0.0030\n",
            "Epoch [168/200], Batch [50/50], Loss: 0.0070\n",
            "Epoch 168: Train Loss = 0.2182, Accuracy = 100.00%\n",
            "Epoch [169/200], Batch [5/50], Loss: 0.0045\n",
            "Epoch [169/200], Batch [10/50], Loss: 0.0029\n",
            "Epoch [169/200], Batch [15/50], Loss: 0.0049\n",
            "Epoch [169/200], Batch [20/50], Loss: 0.0038\n",
            "Epoch [169/200], Batch [25/50], Loss: 0.0039\n",
            "Epoch [169/200], Batch [30/50], Loss: 0.0080\n",
            "Epoch [169/200], Batch [35/50], Loss: 0.0037\n",
            "Epoch [169/200], Batch [40/50], Loss: 0.0056\n",
            "Epoch [169/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [169/200], Batch [50/50], Loss: 0.0035\n",
            "Epoch 169: Train Loss = 0.2177, Accuracy = 100.00%\n",
            "Epoch [170/200], Batch [5/50], Loss: 0.0018\n",
            "Epoch [170/200], Batch [10/50], Loss: 0.0025\n",
            "Epoch [170/200], Batch [15/50], Loss: 0.0048\n",
            "Epoch [170/200], Batch [20/50], Loss: 0.0066\n",
            "Epoch [170/200], Batch [25/50], Loss: 0.0051\n",
            "Epoch [170/200], Batch [30/50], Loss: 0.0043\n",
            "Epoch [170/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [170/200], Batch [40/50], Loss: 0.0038\n",
            "Epoch [170/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [170/200], Batch [50/50], Loss: 0.0023\n",
            "Epoch 170: Train Loss = 0.2176, Accuracy = 100.00%\n",
            "Epoch [171/200], Batch [5/50], Loss: 0.0061\n",
            "Epoch [171/200], Batch [10/50], Loss: 0.0078\n",
            "Epoch [171/200], Batch [15/50], Loss: 0.0037\n",
            "Epoch [171/200], Batch [20/50], Loss: 0.0042\n",
            "Epoch [171/200], Batch [25/50], Loss: 0.0043\n",
            "Epoch [171/200], Batch [30/50], Loss: 0.0041\n",
            "Epoch [171/200], Batch [35/50], Loss: 0.0029\n",
            "Epoch [171/200], Batch [40/50], Loss: 0.0036\n",
            "Epoch [171/200], Batch [45/50], Loss: 0.0039\n",
            "Epoch [171/200], Batch [50/50], Loss: 0.0033\n",
            "Epoch 171: Train Loss = 0.2177, Accuracy = 100.00%\n",
            "Epoch [172/200], Batch [5/50], Loss: 0.0033\n",
            "Epoch [172/200], Batch [10/50], Loss: 0.0039\n",
            "Epoch [172/200], Batch [15/50], Loss: 0.0041\n",
            "Epoch [172/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [172/200], Batch [25/50], Loss: 0.0052\n",
            "Epoch [172/200], Batch [30/50], Loss: 0.0026\n",
            "Epoch [172/200], Batch [35/50], Loss: 0.0065\n",
            "Epoch [172/200], Batch [40/50], Loss: 0.0031\n",
            "Epoch [172/200], Batch [45/50], Loss: 0.0104\n",
            "Epoch [172/200], Batch [50/50], Loss: 0.0034\n",
            "Epoch 172: Train Loss = 0.2174, Accuracy = 100.00%\n",
            "Epoch [173/200], Batch [5/50], Loss: 0.0025\n",
            "Epoch [173/200], Batch [10/50], Loss: 0.0033\n",
            "Epoch [173/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [173/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [173/200], Batch [25/50], Loss: 0.0017\n",
            "Epoch [173/200], Batch [30/50], Loss: 0.0035\n",
            "Epoch [173/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [173/200], Batch [40/50], Loss: 0.0070\n",
            "Epoch [173/200], Batch [45/50], Loss: 0.0028\n",
            "Epoch [173/200], Batch [50/50], Loss: 0.0051\n",
            "Epoch 173: Train Loss = 0.2174, Accuracy = 100.00%\n",
            "Epoch [174/200], Batch [5/50], Loss: 0.0034\n",
            "Epoch [174/200], Batch [10/50], Loss: 0.0034\n",
            "Epoch [174/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [174/200], Batch [20/50], Loss: 0.0041\n",
            "Epoch [174/200], Batch [25/50], Loss: 0.0037\n",
            "Epoch [174/200], Batch [30/50], Loss: 0.0029\n",
            "Epoch [174/200], Batch [35/50], Loss: 0.0053\n",
            "Epoch [174/200], Batch [40/50], Loss: 0.0054\n",
            "Epoch [174/200], Batch [45/50], Loss: 0.0041\n",
            "Epoch [174/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 174: Train Loss = 0.2173, Accuracy = 100.00%\n",
            "Epoch [175/200], Batch [5/50], Loss: 0.0052\n",
            "Epoch [175/200], Batch [10/50], Loss: 0.0051\n",
            "Epoch [175/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [175/200], Batch [20/50], Loss: 0.0031\n",
            "Epoch [175/200], Batch [25/50], Loss: 0.0054\n",
            "Epoch [175/200], Batch [30/50], Loss: 0.0029\n",
            "Epoch [175/200], Batch [35/50], Loss: 0.0031\n",
            "Epoch [175/200], Batch [40/50], Loss: 0.0046\n",
            "Epoch [175/200], Batch [45/50], Loss: 0.0042\n",
            "Epoch [175/200], Batch [50/50], Loss: 0.0032\n",
            "Epoch 175: Train Loss = 0.2172, Accuracy = 100.00%\n",
            "Epoch [176/200], Batch [5/50], Loss: 0.0033\n",
            "Epoch [176/200], Batch [10/50], Loss: 0.0021\n",
            "Epoch [176/200], Batch [15/50], Loss: 0.0030\n",
            "Epoch [176/200], Batch [20/50], Loss: 0.0029\n",
            "Epoch [176/200], Batch [25/50], Loss: 0.0034\n",
            "Epoch [176/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [176/200], Batch [35/50], Loss: 0.0076\n",
            "Epoch [176/200], Batch [40/50], Loss: 0.0045\n",
            "Epoch [176/200], Batch [45/50], Loss: 0.0081\n",
            "Epoch [176/200], Batch [50/50], Loss: 0.0038\n",
            "Epoch 176: Train Loss = 0.2173, Accuracy = 100.00%\n",
            "Epoch [177/200], Batch [5/50], Loss: 0.0030\n",
            "Epoch [177/200], Batch [10/50], Loss: 0.0029\n",
            "Epoch [177/200], Batch [15/50], Loss: 0.0056\n",
            "Epoch [177/200], Batch [20/50], Loss: 0.0024\n",
            "Epoch [177/200], Batch [25/50], Loss: 0.0022\n",
            "Epoch [177/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [177/200], Batch [35/50], Loss: 0.0029\n",
            "Epoch [177/200], Batch [40/50], Loss: 0.0055\n",
            "Epoch [177/200], Batch [45/50], Loss: 0.0059\n",
            "Epoch [177/200], Batch [50/50], Loss: 0.0023\n",
            "Epoch 177: Train Loss = 0.2170, Accuracy = 100.00%\n",
            "Epoch [178/200], Batch [5/50], Loss: 0.0024\n",
            "Epoch [178/200], Batch [10/50], Loss: 0.0035\n",
            "Epoch [178/200], Batch [15/50], Loss: 0.0067\n",
            "Epoch [178/200], Batch [20/50], Loss: 0.0038\n",
            "Epoch [178/200], Batch [25/50], Loss: 0.0066\n",
            "Epoch [178/200], Batch [30/50], Loss: 0.0033\n",
            "Epoch [178/200], Batch [35/50], Loss: 0.0016\n",
            "Epoch [178/200], Batch [40/50], Loss: 0.0056\n",
            "Epoch [178/200], Batch [45/50], Loss: 0.0072\n",
            "Epoch [178/200], Batch [50/50], Loss: 0.0030\n",
            "Epoch 178: Train Loss = 0.2176, Accuracy = 100.00%\n",
            "Epoch [179/200], Batch [5/50], Loss: 0.0040\n",
            "Epoch [179/200], Batch [10/50], Loss: 0.0046\n",
            "Epoch [179/200], Batch [15/50], Loss: 0.0012\n",
            "Epoch [179/200], Batch [20/50], Loss: 0.0054\n",
            "Epoch [179/200], Batch [25/50], Loss: 0.0059\n",
            "Epoch [179/200], Batch [30/50], Loss: 0.0053\n",
            "Epoch [179/200], Batch [35/50], Loss: 0.0033\n",
            "Epoch [179/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [179/200], Batch [45/50], Loss: 0.0027\n",
            "Epoch [179/200], Batch [50/50], Loss: 0.0040\n",
            "Epoch 179: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [180/200], Batch [5/50], Loss: 0.0047\n",
            "Epoch [180/200], Batch [10/50], Loss: 0.0052\n",
            "Epoch [180/200], Batch [15/50], Loss: 0.0065\n",
            "Epoch [180/200], Batch [20/50], Loss: 0.0060\n",
            "Epoch [180/200], Batch [25/50], Loss: 0.0031\n",
            "Epoch [180/200], Batch [30/50], Loss: 0.0022\n",
            "Epoch [180/200], Batch [35/50], Loss: 0.0045\n",
            "Epoch [180/200], Batch [40/50], Loss: 0.0038\n",
            "Epoch [180/200], Batch [45/50], Loss: 0.0054\n",
            "Epoch [180/200], Batch [50/50], Loss: 0.0041\n",
            "Epoch 180: Train Loss = 0.2172, Accuracy = 100.00%\n",
            "Epoch [181/200], Batch [5/50], Loss: 0.0034\n",
            "Epoch [181/200], Batch [10/50], Loss: 0.0035\n",
            "Epoch [181/200], Batch [15/50], Loss: 0.0041\n",
            "Epoch [181/200], Batch [20/50], Loss: 0.0047\n",
            "Epoch [181/200], Batch [25/50], Loss: 0.0039\n",
            "Epoch [181/200], Batch [30/50], Loss: 0.0040\n",
            "Epoch [181/200], Batch [35/50], Loss: 0.0062\n",
            "Epoch [181/200], Batch [40/50], Loss: 0.0059\n",
            "Epoch [181/200], Batch [45/50], Loss: 0.0056\n",
            "Epoch [181/200], Batch [50/50], Loss: 0.0040\n",
            "Epoch 181: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [182/200], Batch [5/50], Loss: 0.0032\n",
            "Epoch [182/200], Batch [10/50], Loss: 0.0047\n",
            "Epoch [182/200], Batch [15/50], Loss: 0.0058\n",
            "Epoch [182/200], Batch [20/50], Loss: 0.0034\n",
            "Epoch [182/200], Batch [25/50], Loss: 0.0058\n",
            "Epoch [182/200], Batch [30/50], Loss: 0.0047\n",
            "Epoch [182/200], Batch [35/50], Loss: 0.0032\n",
            "Epoch [182/200], Batch [40/50], Loss: 0.0035\n",
            "Epoch [182/200], Batch [45/50], Loss: 0.0044\n",
            "Epoch [182/200], Batch [50/50], Loss: 0.0047\n",
            "Epoch 182: Train Loss = 0.2170, Accuracy = 100.00%\n",
            "Epoch [183/200], Batch [5/50], Loss: 0.0066\n",
            "Epoch [183/200], Batch [10/50], Loss: 0.0033\n",
            "Epoch [183/200], Batch [15/50], Loss: 0.0054\n",
            "Epoch [183/200], Batch [20/50], Loss: 0.0028\n",
            "Epoch [183/200], Batch [25/50], Loss: 0.0050\n",
            "Epoch [183/200], Batch [30/50], Loss: 0.0059\n",
            "Epoch [183/200], Batch [35/50], Loss: 0.0076\n",
            "Epoch [183/200], Batch [40/50], Loss: 0.0072\n",
            "Epoch [183/200], Batch [45/50], Loss: 0.0038\n",
            "Epoch [183/200], Batch [50/50], Loss: 0.0042\n",
            "Epoch 183: Train Loss = 0.2175, Accuracy = 100.00%\n",
            "Epoch [184/200], Batch [5/50], Loss: 0.0030\n",
            "Epoch [184/200], Batch [10/50], Loss: 0.0016\n",
            "Epoch [184/200], Batch [15/50], Loss: 0.0036\n",
            "Epoch [184/200], Batch [20/50], Loss: 0.0046\n",
            "Epoch [184/200], Batch [25/50], Loss: 0.0040\n",
            "Epoch [184/200], Batch [30/50], Loss: 0.0055\n",
            "Epoch [184/200], Batch [35/50], Loss: 0.0044\n",
            "Epoch [184/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [184/200], Batch [45/50], Loss: 0.0026\n",
            "Epoch [184/200], Batch [50/50], Loss: 0.0033\n",
            "Epoch 184: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [185/200], Batch [5/50], Loss: 0.0047\n",
            "Epoch [185/200], Batch [10/50], Loss: 0.0038\n",
            "Epoch [185/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [185/200], Batch [20/50], Loss: 0.0031\n",
            "Epoch [185/200], Batch [25/50], Loss: 0.0055\n",
            "Epoch [185/200], Batch [30/50], Loss: 0.0046\n",
            "Epoch [185/200], Batch [35/50], Loss: 0.0022\n",
            "Epoch [185/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [185/200], Batch [45/50], Loss: 0.0029\n",
            "Epoch [185/200], Batch [50/50], Loss: 0.0073\n",
            "Epoch 185: Train Loss = 0.2172, Accuracy = 100.00%\n",
            "Epoch [186/200], Batch [5/50], Loss: 0.0023\n",
            "Epoch [186/200], Batch [10/50], Loss: 0.0038\n",
            "Epoch [186/200], Batch [15/50], Loss: 0.0028\n",
            "Epoch [186/200], Batch [20/50], Loss: 0.0023\n",
            "Epoch [186/200], Batch [25/50], Loss: 0.0060\n",
            "Epoch [186/200], Batch [30/50], Loss: 0.0032\n",
            "Epoch [186/200], Batch [35/50], Loss: 0.0034\n",
            "Epoch [186/200], Batch [40/50], Loss: 0.0038\n",
            "Epoch [186/200], Batch [45/50], Loss: 0.0034\n",
            "Epoch [186/200], Batch [50/50], Loss: 0.0037\n",
            "Epoch 186: Train Loss = 0.2172, Accuracy = 100.00%\n",
            "Epoch [187/200], Batch [5/50], Loss: 0.0041\n",
            "Epoch [187/200], Batch [10/50], Loss: 0.0044\n",
            "Epoch [187/200], Batch [15/50], Loss: 0.0037\n",
            "Epoch [187/200], Batch [20/50], Loss: 0.0057\n",
            "Epoch [187/200], Batch [25/50], Loss: 0.0034\n",
            "Epoch [187/200], Batch [30/50], Loss: 0.0031\n",
            "Epoch [187/200], Batch [35/50], Loss: 0.0049\n",
            "Epoch [187/200], Batch [40/50], Loss: 0.0054\n",
            "Epoch [187/200], Batch [45/50], Loss: 0.0063\n",
            "Epoch [187/200], Batch [50/50], Loss: 0.0025\n",
            "Epoch 187: Train Loss = 0.2172, Accuracy = 100.00%\n",
            "Epoch [188/200], Batch [5/50], Loss: 0.0023\n",
            "Epoch [188/200], Batch [10/50], Loss: 0.0055\n",
            "Epoch [188/200], Batch [15/50], Loss: 0.0037\n",
            "Epoch [188/200], Batch [20/50], Loss: 0.0029\n",
            "Epoch [188/200], Batch [25/50], Loss: 0.0013\n",
            "Epoch [188/200], Batch [30/50], Loss: 0.0058\n",
            "Epoch [188/200], Batch [35/50], Loss: 0.0031\n",
            "Epoch [188/200], Batch [40/50], Loss: 0.0077\n",
            "Epoch [188/200], Batch [45/50], Loss: 0.0032\n",
            "Epoch [188/200], Batch [50/50], Loss: 0.0054\n",
            "Epoch 188: Train Loss = 0.2169, Accuracy = 100.00%\n",
            "Epoch [189/200], Batch [5/50], Loss: 0.0032\n",
            "Epoch [189/200], Batch [10/50], Loss: 0.0056\n",
            "Epoch [189/200], Batch [15/50], Loss: 0.0057\n",
            "Epoch [189/200], Batch [20/50], Loss: 0.0035\n",
            "Epoch [189/200], Batch [25/50], Loss: 0.0029\n",
            "Epoch [189/200], Batch [30/50], Loss: 0.0074\n",
            "Epoch [189/200], Batch [35/50], Loss: 0.0035\n",
            "Epoch [189/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [189/200], Batch [45/50], Loss: 0.0051\n",
            "Epoch [189/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 189: Train Loss = 0.2168, Accuracy = 100.00%\n",
            "Epoch [190/200], Batch [5/50], Loss: 0.0065\n",
            "Epoch [190/200], Batch [10/50], Loss: 0.0050\n",
            "Epoch [190/200], Batch [15/50], Loss: 0.0030\n",
            "Epoch [190/200], Batch [20/50], Loss: 0.0024\n",
            "Epoch [190/200], Batch [25/50], Loss: 0.0030\n",
            "Epoch [190/200], Batch [30/50], Loss: 0.0061\n",
            "Epoch [190/200], Batch [35/50], Loss: 0.0044\n",
            "Epoch [190/200], Batch [40/50], Loss: 0.0111\n",
            "Epoch [190/200], Batch [45/50], Loss: 0.0061\n",
            "Epoch [190/200], Batch [50/50], Loss: 0.0036\n",
            "Epoch 190: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [191/200], Batch [5/50], Loss: 0.0029\n",
            "Epoch [191/200], Batch [10/50], Loss: 0.0037\n",
            "Epoch [191/200], Batch [15/50], Loss: 0.0047\n",
            "Epoch [191/200], Batch [20/50], Loss: 0.0044\n",
            "Epoch [191/200], Batch [25/50], Loss: 0.0023\n",
            "Epoch [191/200], Batch [30/50], Loss: 0.0036\n",
            "Epoch [191/200], Batch [35/50], Loss: 0.0024\n",
            "Epoch [191/200], Batch [40/50], Loss: 0.0033\n",
            "Epoch [191/200], Batch [45/50], Loss: 0.0061\n",
            "Epoch [191/200], Batch [50/50], Loss: 0.0034\n",
            "Epoch 191: Train Loss = 0.2173, Accuracy = 100.00%\n",
            "Epoch [192/200], Batch [5/50], Loss: 0.0071\n",
            "Epoch [192/200], Batch [10/50], Loss: 0.0033\n",
            "Epoch [192/200], Batch [15/50], Loss: 0.0033\n",
            "Epoch [192/200], Batch [20/50], Loss: 0.0056\n",
            "Epoch [192/200], Batch [25/50], Loss: 0.0061\n",
            "Epoch [192/200], Batch [30/50], Loss: 0.0043\n",
            "Epoch [192/200], Batch [35/50], Loss: 0.0044\n",
            "Epoch [192/200], Batch [40/50], Loss: 0.0029\n",
            "Epoch [192/200], Batch [45/50], Loss: 0.0036\n",
            "Epoch [192/200], Batch [50/50], Loss: 0.0039\n",
            "Epoch 192: Train Loss = 0.2168, Accuracy = 100.00%\n",
            "Epoch [193/200], Batch [5/50], Loss: 0.0043\n",
            "Epoch [193/200], Batch [10/50], Loss: 0.0020\n",
            "Epoch [193/200], Batch [15/50], Loss: 0.0050\n",
            "Epoch [193/200], Batch [20/50], Loss: 0.0052\n",
            "Epoch [193/200], Batch [25/50], Loss: 0.0053\n",
            "Epoch [193/200], Batch [30/50], Loss: 0.0038\n",
            "Epoch [193/200], Batch [35/50], Loss: 0.0030\n",
            "Epoch [193/200], Batch [40/50], Loss: 0.0037\n",
            "Epoch [193/200], Batch [45/50], Loss: 0.0061\n",
            "Epoch [193/200], Batch [50/50], Loss: 0.0041\n",
            "Epoch 193: Train Loss = 0.2174, Accuracy = 100.00%\n",
            "Epoch [194/200], Batch [5/50], Loss: 0.0046\n",
            "Epoch [194/200], Batch [10/50], Loss: 0.0069\n",
            "Epoch [194/200], Batch [15/50], Loss: 0.0051\n",
            "Epoch [194/200], Batch [20/50], Loss: 0.0034\n",
            "Epoch [194/200], Batch [25/50], Loss: 0.0057\n",
            "Epoch [194/200], Batch [30/50], Loss: 0.0034\n",
            "Epoch [194/200], Batch [35/50], Loss: 0.0034\n",
            "Epoch [194/200], Batch [40/50], Loss: 0.0027\n",
            "Epoch [194/200], Batch [45/50], Loss: 0.0047\n",
            "Epoch [194/200], Batch [50/50], Loss: 0.0066\n",
            "Epoch 194: Train Loss = 0.2167, Accuracy = 100.00%\n",
            "Epoch [195/200], Batch [5/50], Loss: 0.0052\n",
            "Epoch [195/200], Batch [10/50], Loss: 0.0041\n",
            "Epoch [195/200], Batch [15/50], Loss: 0.0039\n",
            "Epoch [195/200], Batch [20/50], Loss: 0.0060\n",
            "Epoch [195/200], Batch [25/50], Loss: 0.0056\n",
            "Epoch [195/200], Batch [30/50], Loss: 0.0045\n",
            "Epoch [195/200], Batch [35/50], Loss: 0.0071\n",
            "Epoch [195/200], Batch [40/50], Loss: 0.0053\n",
            "Epoch [195/200], Batch [45/50], Loss: 0.0080\n",
            "Epoch [195/200], Batch [50/50], Loss: 0.0050\n",
            "Epoch 195: Train Loss = 0.2169, Accuracy = 100.00%\n",
            "Epoch [196/200], Batch [5/50], Loss: 0.0024\n",
            "Epoch [196/200], Batch [10/50], Loss: 0.0048\n",
            "Epoch [196/200], Batch [15/50], Loss: 0.0025\n",
            "Epoch [196/200], Batch [20/50], Loss: 0.0051\n",
            "Epoch [196/200], Batch [25/50], Loss: 0.0059\n",
            "Epoch [196/200], Batch [30/50], Loss: 0.0041\n",
            "Epoch [196/200], Batch [35/50], Loss: 0.0035\n",
            "Epoch [196/200], Batch [40/50], Loss: 0.0030\n",
            "Epoch [196/200], Batch [45/50], Loss: 0.0031\n",
            "Epoch [196/200], Batch [50/50], Loss: 0.0066\n",
            "Epoch 196: Train Loss = 0.2167, Accuracy = 100.00%\n",
            "Epoch [197/200], Batch [5/50], Loss: 0.0059\n",
            "Epoch [197/200], Batch [10/50], Loss: 0.0040\n",
            "Epoch [197/200], Batch [15/50], Loss: 0.0054\n",
            "Epoch [197/200], Batch [20/50], Loss: 0.0024\n",
            "Epoch [197/200], Batch [25/50], Loss: 0.0046\n",
            "Epoch [197/200], Batch [30/50], Loss: 0.0047\n",
            "Epoch [197/200], Batch [35/50], Loss: 0.0042\n",
            "Epoch [197/200], Batch [40/50], Loss: 0.0017\n",
            "Epoch [197/200], Batch [45/50], Loss: 0.0035\n",
            "Epoch [197/200], Batch [50/50], Loss: 0.0041\n",
            "Epoch 197: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [198/200], Batch [5/50], Loss: 0.0043\n",
            "Epoch [198/200], Batch [10/50], Loss: 0.0038\n",
            "Epoch [198/200], Batch [15/50], Loss: 0.0031\n",
            "Epoch [198/200], Batch [20/50], Loss: 0.0032\n",
            "Epoch [198/200], Batch [25/50], Loss: 0.0047\n",
            "Epoch [198/200], Batch [30/50], Loss: 0.0027\n",
            "Epoch [198/200], Batch [35/50], Loss: 0.0077\n",
            "Epoch [198/200], Batch [40/50], Loss: 0.0049\n",
            "Epoch [198/200], Batch [45/50], Loss: 0.0043\n",
            "Epoch [198/200], Batch [50/50], Loss: 0.0035\n",
            "Epoch 198: Train Loss = 0.2170, Accuracy = 100.00%\n",
            "Epoch [199/200], Batch [5/50], Loss: 0.0058\n",
            "Epoch [199/200], Batch [10/50], Loss: 0.0042\n",
            "Epoch [199/200], Batch [15/50], Loss: 0.0042\n",
            "Epoch [199/200], Batch [20/50], Loss: 0.0036\n",
            "Epoch [199/200], Batch [25/50], Loss: 0.0047\n",
            "Epoch [199/200], Batch [30/50], Loss: 0.0065\n",
            "Epoch [199/200], Batch [35/50], Loss: 0.0027\n",
            "Epoch [199/200], Batch [40/50], Loss: 0.0043\n",
            "Epoch [199/200], Batch [45/50], Loss: 0.0036\n",
            "Epoch [199/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 199: Train Loss = 0.2171, Accuracy = 100.00%\n",
            "Epoch [200/200], Batch [5/50], Loss: 0.0042\n",
            "Epoch [200/200], Batch [10/50], Loss: 0.0066\n",
            "Epoch [200/200], Batch [15/50], Loss: 0.0078\n",
            "Epoch [200/200], Batch [20/50], Loss: 0.0048\n",
            "Epoch [200/200], Batch [25/50], Loss: 0.0047\n",
            "Epoch [200/200], Batch [30/50], Loss: 0.0015\n",
            "Epoch [200/200], Batch [35/50], Loss: 0.0046\n",
            "Epoch [200/200], Batch [40/50], Loss: 0.0019\n",
            "Epoch [200/200], Batch [45/50], Loss: 0.0027\n",
            "Epoch [200/200], Batch [50/50], Loss: 0.0048\n",
            "Epoch 200: Train Loss = 0.2170, Accuracy = 100.00%\n",
            "Validation Accuracy: 95.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model .pth file\n",
        "save_prompt_learner(model=model,model_name=model_name,dataset_name=test_dataset_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPXJ0aDZ6XrL",
        "outputId": "30bd293b-740d-48c4-e610-07d8c8db75c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt learner saved to output/ViT-B/16/caltech/coop_prompt.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Interpretation\n",
        "\n",
        "Interprets the learned context vectors from the saved CoOp prompt learner. It maps them to the closest real words in CLIP vocabulary. It helps explaining what the learned context tokens represent in natural language"
      ],
      "metadata": {
        "id": "ijIwHBJOVQVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_prompt(fpath, topk=5, model_name=model_name): #f_Path is .pth file, topk: number of top closest tokens to show per context vector\n",
        "    assert os.path.exists(fpath), f\"Prompt file not found: {fpath}\"\n",
        "    print(f\"Return the top-{topk} matched words\")\n",
        "\n",
        "    # loads the clip model and a token embedding\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    clip_model = clip.load(model_name, device=\"cpu\")[0]  # Use same model backbone\n",
        "    token_embedding = clip_model.token_embedding.weight\n",
        "    print(f\"Size of token embedding: {token_embedding.shape}\")\n",
        "\n",
        "    #load the trained prompt learner\n",
        "    prompt_learner = torch.load(fpath, map_location=\"cpu\")[\"state_dict\"]\n",
        "    # trained context embedding\n",
        "    ctx = prompt_learner[\"ctx\"].float()\n",
        "    print(f\"Size of context: {ctx.shape}\")\n",
        "\n",
        "    # This is for the shared context\n",
        "    if ctx.dim() == 2:\n",
        "        #computes the euclidean distance between each context token and each real token\n",
        "        distance = torch.cdist(ctx, token_embedding)\n",
        "        print(f\"Size of distance matrix: {distance.shape}\")\n",
        "        sorted_idxs = torch.argsort(distance, dim=1)[:, :topk]\n",
        "\n",
        "        for m, idxs in enumerate(sorted_idxs):\n",
        "            words = [tokenizer.decoder[idx.item()] for idx in idxs] # here the tokenizer.decoder helps to decodes the closest token into actual works\n",
        "            dist = [f\"{distance[m, idx].item():.4f}\" for idx in idxs]\n",
        "            print(f\"{m+1}: {words} {dist}\")\n",
        "\n",
        "    elif ctx.dim() == 3:\n",
        "        raise NotImplementedError(\"Class-specific context interpretation not supported.\")\n"
      ],
      "metadata": {
        "id": "HDzYh7JKXTAm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpret Prompts Results\n",
        "\n",
        "NUmbers on left (1:16) context vectors learned, the list shows top-5 closest vocabulary tokens (Decoded) for thier respective context vector"
      ],
      "metadata": {
        "id": "T2OMS6xV_oF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpret_prompt(path, topk=5, model_name=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZL4BA0fX7dz",
        "outputId": "68441617-f5a0-46d1-966e-5f50eed060b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return the top-5 matched words\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['ę</w>', 'E</w>', 'D</w>', 'C</w>', 'B</w>'] ['0.5396', '0.5396', '0.5396', '0.5396', '0.5396']\n",
            "2: ['mikequind', 'flyeagles', 'ìĨĮëħĦëĭ', 'è</w>', 'æ</w>'] ['0.6157', '0.6163', '0.6191', '0.6194', '0.6194']\n",
            "3: ['instaweather', 'a</w>', 'Ë</w>', 'Ì</w>', 'Ê</w>'] ['0.6475', '0.6479', '0.6483', '0.6483', '0.6483']\n",
            "4: ['X</w>', 'ė</w>', 'Ę</w>', 'ę</w>', 'Ě</w>'] ['0.6313', '0.6313', '0.6313', '0.6313', '0.6313']\n",
            "5: ['gathers</w>', 'infec', 'minis', 'northe', 'tsal</w>'] ['0.8346', '0.8375', '0.8384', '0.8388', '0.8399']\n",
            "6: ['sundaywithmarsha</w>', 'Ý', 'kirstel</w>', 'ñ', 'pational</w>'] ['0.6305', '0.6318', '0.6319', '0.6331', '0.6336']\n",
            "7: ['ìĨĮëħĦëĭ', 'sundaywithmarsha</w>', 'Ä</w>', 'Ò</w>', 'Ñ</w>'] ['0.6238', '0.6248', '0.6253', '0.6253', '0.6253']\n",
            "8: ['instaweather', 'ñ', 'ě', 'ą</w>', 'Ć</w>'] ['0.6313', '0.6342', '0.6359', '0.6359', '0.6359']\n",
            "9: ['flyeagles', 'ü', 'ă', 'Ă', 'ā'] ['0.5920', '0.5922', '0.5922', '0.5922', '0.5922']\n",
            "10: ['balot', 'whiche', 'prati', 'sz', 'chall'] ['0.8830', '0.8832', '0.8833', '0.8850', '0.8864']\n",
            "11: ['Ð</w>', 'N</w>', 'Ý</w>', 'Ü</w>', 'Û</w>'] ['0.5915', '0.5915', '0.5915', '0.5915', '0.5915']\n",
            "12: ['we</w>', 'bha</w>', 'âĺĺï¸ı</w>', 'is</w>', 'ñ'] ['0.8059', '0.8119', '0.8119', '0.8121', '0.8121']\n",
            "13: ['ô', 'Ý', 'bbhutto', 'sailed</w>', 'atility</w>'] ['0.7463', '0.7514', '0.7524', '0.7525', '0.7533']\n",
            "14: ['bbhutto', 'Ó</w>', 'Ô</w>', 'Ò</w>', 'Ñ</w>'] ['0.6948', '0.6952', '0.6952', '0.6952', '0.6952']\n",
            "15: ['pational</w>', 'Ê</w>', '×</w>', 'Ö</w>', 'Õ</w>'] ['0.5574', '0.5592', '0.5592', '0.5592', '0.5592']\n",
            "16: ['healthtech</w>', 'shie</w>', 'atene', 'ðŁĩ¨ðŁĩ¦</w>', 'gurgaon</w>'] ['1.1259', '1.1316', '1.1317', '1.1317', '1.1335']\n"
          ]
        }
      ]
    }
  ]
}