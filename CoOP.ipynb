{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRgT8UMJv8bR",
        "outputId": "7e190be4-33f0-4564-f446-8424745689e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GitHub token: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "token = getpass('Enter your GitHub token: ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi-qSVH1PEnf"
      },
      "source": [
        "#CoOP\n",
        "This notebook performs CoOP using CLIP, This notebook performs fewshot leanrning (1,2...16) on various context length, on different class token position (front, mid, end) on various models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzY99FHBQAAD"
      },
      "source": [
        "#Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ElykoMZAPC0J"
      },
      "outputs": [],
      "source": [
        "colab_clone_repo = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfOPtjAOQFjQ"
      },
      "outputs": [],
      "source": [
        "test_dataset_name = 'caltech'   #['airplane', 'caltech', 'dtd', 'flower', 'food', 'pets', 'ucf']\n",
        "model_name = \"ViT-B/16\" #['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']\n",
        "path=f\"output/{model_name}/{test_dataset_name}/coop_prompt.pth\" # Model save path\n",
        "# mention all the parameters\n",
        "n_ctx = 16  # few shot learning (1,2,....16)\n",
        "ctx_init = \"\"  # context vector, rn its not initialized\n",
        "class_token_position = \"end\"  #[\"front\", \"middle\", \"end\"]\n",
        "csc = False  # For using the class specific context\n",
        "input_size = 224  # Input Image\n",
        "# if csc is True then initialize it with \"photo of a\" or else False then use the generic\n",
        "if not ctx_init:\n",
        "    if csc:\n",
        "        ctx_init = \"a photo of a\"\n",
        "    else:\n",
        "        ctx_init = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMTquhW_QMhi"
      },
      "source": [
        "#Prepare the Execution Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6jkbSoe4QRcy",
        "outputId": "98ad207f-402d-489a-9b97-5eeaffcd3950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks that a private key is already created. If you have already push it to github, no action required.\n",
            " Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIACg2Ls9J77YFA3SsrCO1KByupjJeYLcMYUTBjtbG4Bu root@e1f18cb72fab\n",
            "\n",
            "Please use SSH method to clone repo.\n",
            "/content\n",
            "Cloning into 'applied-ml'...\n",
            "remote: Enumerating objects: 403, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 403 (delta 17), reused 22 (delta 10), pack-reused 362 (from 1)\u001b[K\n",
            "Receiving objects: 100% (403/403), 161.49 MiB | 50.34 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/applied-ml\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=fc2952921f27830f1b94b89875b695c78ce5e206b581b6966822925581510b05\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Change the path if necessary\n",
        "dataset_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    if colab_clone_repo and not os.path.exists('/content/applied-ml/'):\n",
        "        !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "        import colab_github\n",
        "        colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "        %cd /content/\n",
        "        !git clone https://{token}@github.com/oz-e/applied-ml.git\n",
        "        if not os.path.exists('/content/applied-ml/'):\n",
        "            raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "\n",
        "    if colab_clone_repo:\n",
        "        %cd /content/applied-ml/\n",
        "    else:\n",
        "        %cd /content/\n",
        "\n",
        "    # Install any other requirements (to be converted to requirements.txt)\n",
        "    !pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-bC2SNQnMx",
        "outputId": "90e38cd1-0ef6-4e24-c19c-a62888f1e494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-p1t9556o\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-p1t9556o\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=460590558930f9c0254fdac197b97a013671f051ae5740bc00a9e33ed137316a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qrukpcbq/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GzqiD99QVd2",
        "outputId": "a75f6ebf-102e-4675-ea35-f6b1fdab0fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 198 (delta 198), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 9.02 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('CoOp'):\n",
        "  !git clone https://github.com/KaiyangZhou/CoOp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SXYiEGI5QhPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import random_split\n",
        "import clip\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import aml.datasets\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from clip.simple_tokenizer import SimpleTokenizer\n",
        "from clip import clip\n",
        "_tokenizer = _Tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2rhovC0Q7fZ"
      },
      "source": [
        "#Load the CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJO1fCftQ9Zt",
        "outputId": "1fafd5f4-ca55-4fa7-8683-c8845445b9b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:06<00:00, 56.2MiB/s]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(model_name, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrj3ZErySndR"
      },
      "source": [
        "#Data Loading and Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YR1vcDJjSqFk",
        "outputId": "73ab30ee-8411-4ad5-a526-bf1944d8e94b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD&confirm=t&uuid=432622c6-6538-4ab7-abd1-6d1140ae18c6\n",
            "To: /content/applied-ml/datasets/caltech101/101_ObjectCategories.tar.gz\n",
            "100%|██████████| 132M/132M [00:01<00:00, 86.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d&confirm=t&uuid=19c6402e-fdba-4ec9-8562-a7fc405d188b\n",
            "To: /content/applied-ml/datasets/caltech101/Annotations.tar\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 74.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/applied-ml/datasets/caltech101/split.json\n",
            "100%|██████████| 809k/809k [00:00<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "# set the transform variable according to the models architecture\n",
        "transform = preprocess\n",
        "match test_dataset_name:\n",
        "    case 'airplane':\n",
        "        dataset = aml.datasets.FGVCAircraft(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'caltech':\n",
        "        dataset = aml.datasets.Caltech101(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'dtd':\n",
        "        dataset = aml.datasets.DTD(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'flower':\n",
        "        dataset = aml.datasets.Flowers102(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'food':\n",
        "        dataset = aml.datasets.Food101(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'pets':\n",
        "        dataset = aml.datasets.OxfordIIITPet(root=dataset_path,split='train', transform=preprocess)\n",
        "    case 'ucf':\n",
        "        dataset = aml.datasets.UCF101(root=dataset_path,split='train', transform=preprocess)\n",
        "\n",
        "classnames = dataset.classnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VahVei9mTri-"
      },
      "outputs": [],
      "source": [
        "# Split the data into 80/20 split\n",
        "num_train = int(0.8 * len(dataset))\n",
        "num_val = len(dataset) - num_train\n",
        "train_dataset, val_dataset = random_split(dataset, [num_train, num_val])\n",
        "\n",
        "#Set up the train and val dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GslttqYMUMCe"
      },
      "source": [
        "#TEXT ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AtdER2IyUSpN"
      },
      "outputs": [],
      "source": [
        "# Retruns the text feature vector of shape (batchsize, embedding)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        # Model to process the token Embeddings\n",
        "        self.transformer = clip_model.transformer\n",
        "        # Positional Encoder\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        # Clip final layer\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        # A learned projection matrix that maps the final hidden state to the CLIP embedding space.\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        # Model data type\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        #adds the positional embedding to the token embedding, which includes the information about the token position\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        # Changes shape from (batch_size, seq_len, dim) → (seq_len, batch_size, dim)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        #Passes the sequence through the Transformer (learns contextual relationships between tokens).\n",
        "        x = self.transformer(x)\n",
        "        # Reverts the shape back to (batch_size, seq_len, dim).\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # Applies layer normalization and converts to the appropriate data type.\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        # Selects the embedding at the position of the end-of-text token, assuming it's the most meaningful. and projects the token into clip embedding space\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG-cOzjiUUpE"
      },
      "source": [
        "#Prompt Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9ToXrbU-UWcL"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, n_ctx=16, ctx_init=\"\", class_token_position=\"end\", csc=False, input_size=224):\n",
        "        super().__init__()\n",
        "        self.n_cls = len(classnames) # total classnames\n",
        "        self.n_ctx = n_ctx # Number of learnable context tokens\n",
        "        self.ctx_init = ctx_init # String to initialize the context from words\n",
        "        self.class_token_position = class_token_position # class token position (front, middle,  end)\n",
        "        self.csc = csc # True / False value to use one prompt per class\n",
        "        self.input_size = input_size # Image resolution\n",
        "\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        device = clip_model.token_embedding.weight.device\n",
        "\n",
        "        assert self.input_size == clip_imsize, f\"cfg_imsize ({self.input_size}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        # if the string is provided to initialize the context this perform the below block (\"a photo of a\")\n",
        "        if self.ctx_init:\n",
        "            # replaces the underscore of the string to spaces\n",
        "            ctx_init = self.ctx_init.replace(\"_\", \" \")\n",
        "            # updates the n_ctx ( number of learnable tokens) based on how many words are in the prompt. here its 4 as (\"a photo of a\")\n",
        "            self.n_ctx = len(ctx_init.split(\" \"))\n",
        "            # Tokenize the prompt, output shape- [1,77]\n",
        "            prompt = clip.tokenize(ctx_init).to(device)\n",
        "            # converts the tokenized prompts imto embeddings, output shape - [1, 77, ctx_dim]\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            #extract the embedding for the context words not for the special tokens, [sos], [eos]\n",
        "            ctx_vectors = embedding[0, 1 : 1 + self.n_ctx, :].to(device)\n",
        "            # stores the original ctx_init prompt\n",
        "            prompt_prefix = ctx_init\n",
        "        # if the string is not provided the perform the below code\n",
        "        else:\n",
        "            # if csc is true which works for the class specific prompts the perform the below code\n",
        "            if self.csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                # creates separate set of learnable context tokens for each number of class (n_cls)\n",
        "                ctx_vectors = torch.empty(self.n_cls, self.n_ctx, ctx_dim, dtype=dtype, device=device) # size of [100, 16, 512] for 100 classes, 16 context length and 512 embedding\n",
        "            # if False then perform the below code\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                # single set of context tokens for all classes\n",
        "                ctx_vectors = torch.empty(self.n_ctx, ctx_dim, dtype=dtype, device=device) #output shape of [16, 512] for ctx = 16, ctx_dim=512\n",
        "            # randomly initializing the context vector (ctx_vector) from the normal distribution\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * self.n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"') # prints number of X for n_ctx\n",
        "        print(f\"Number of context words (tokens): {self.n_ctx}\")\n",
        "\n",
        "        #wraps into nn.Parameter making them trainable weights\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        # data cleaning, replaces the underscore in the class names with the space\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        # stores the length of the tokens of each class name\n",
        "        self.name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        # creates the string stored in prompts variable with class names\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        # clip.tokenize tokenize all class prompts into sequence of token ID and send to the device\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device) #output size of [n_cls, 77] number of class and 77 clip input size\n",
        "        # convert the token ids in the tokenized_prompt variable into embeddings layer\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype) # output shape [n_cls, 77, ctx_dim]\n",
        "        # start of the token (special token) not trainable, so set it as fixed buffer\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "        # end of the token as well, not trainable\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + self.n_ctx :, :])\n",
        "\n",
        "        # stores the original token IDs\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "\n",
        "    # Assembles the final prompt embeddings for each class, the output of this are input to the text encoder\n",
        "    def forward(self):\n",
        "        ctx = self.ctx\n",
        "        # if the context is shared that is [n_ctx, ctx_dim] no n_cls then expands this same context for all the classes [n_cls, n_ctx, ctx_dim]\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        prefix = self.token_prefix # get the starting token\n",
        "        suffix = self.token_suffix # get the end token\n",
        "\n",
        "        # This works on the position of the class token provided (end, middle, front)\n",
        "        if self.class_token_position == \"end\":\n",
        "            #prompt is in form- [start token, context, classname, end token] simple contactenation\n",
        "            prompts = torch.cat([prefix, ctx, suffix], dim=1)\n",
        "\n",
        "        # For Middle position\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2 # get the middle\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat([prefix_i, ctx_i_half1, class_i, ctx_i_half2, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0) #prompt is in form- [start token, context, classname, context, end token] simple contactenation\n",
        "\n",
        "        # For front position\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat([prefix_i, class_i, ctx_i, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0) #prompt is in form- [start token, classname, context, end token] simple contactenation\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid class_token_position: {self.class_token_position}\")\n",
        "\n",
        "        return prompts, self.tokenized_prompts # returns prompt tensor of shape [n_cls, sequence lenght (context), ctx_dim] and tokenized_prompt with token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAXqyuKUYl0"
      },
      "source": [
        "#Clip Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a-VCqt-PUfvh"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        # creates a learnable prompt for each class\n",
        "        self.prompt_learner = PromptLearner(\n",
        "            classnames=classnames,\n",
        "            clip_model=clip_model,\n",
        "            n_ctx=16,\n",
        "            ctx_init=\"\",\n",
        "            csc=False,\n",
        "            class_token_position=\"end\",\n",
        "            input_size=224\n",
        "        )\n",
        "        #loads clip vision encoder\n",
        "        self.image_encoder = clip_model.visual\n",
        "        #text encoder\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        #use to scale similarity logits ( this helps in improving the convergance)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        # clip model tensor data type\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image):\n",
        "      # encodes the image, outputs the image features vectors\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        # generate prompts and tokenized prompts with token IDs from the prompt learner class we declared above\n",
        "        prompts, tokenized_prompts = self.prompt_learner()\n",
        "        # encodes the prompts and tokenized output from the prompt learner into the text encoder of the model\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "        # Normalizing the image and text features\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        # Computing the similaritys\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits # returns [batch_size, num_classes] this later gets passed into the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43fMssvLUlh4"
      },
      "source": [
        "#Initialize the CLIP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3X_wXD4Un1-",
        "outputId": "c4aa5634-23ca-4393-8365-bf8862ed62c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n"
          ]
        }
      ],
      "source": [
        "model = CustomCLIP(classnames=classnames, clip_model=clip_model).to(device) # Initialize the object from the CustomClip class above which takes the classnames and model as an input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909jEGo-U2tv"
      },
      "source": [
        "#Set the hyperparameters for the model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u1PuAm_fU8Vc"
      },
      "outputs": [],
      "source": [
        "# setting the hyperparameters as per the paper\n",
        "MAX_EPOCH = 200\n",
        "LR = 0.002\n",
        "optimizer = optim.SGD(model.prompt_learner.parameters(), lr=LR, momentum=0.9)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=MAX_EPOCH)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5tUymvpVEeR"
      },
      "source": [
        "#Function to save the model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7fdC98i2VHHR"
      },
      "outputs": [],
      "source": [
        "#function to save the trained model\n",
        "def save_prompt_learner(model, model_name, dataset_name):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save({\"state_dict\": model.prompt_learner.state_dict()}, path)\n",
        "    print(f\"Prompt learner saved to {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCdgHNs8VJTO"
      },
      "source": [
        "#Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aVgWujNwVLEv",
        "outputId": "4fb0cffb-54cb-4a5f-949d-02d041384c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Batch [5/104], Loss: 0.3879\n",
            "Epoch [1/200], Batch [10/104], Loss: 0.1249\n",
            "Epoch [1/200], Batch [15/104], Loss: 0.0741\n",
            "Epoch [1/200], Batch [20/104], Loss: 0.2224\n",
            "Epoch [1/200], Batch [25/104], Loss: 0.0834\n",
            "Epoch [1/200], Batch [30/104], Loss: 0.0414\n",
            "Epoch [1/200], Batch [35/104], Loss: 0.1237\n",
            "Epoch [1/200], Batch [40/104], Loss: 0.0342\n",
            "Epoch [1/200], Batch [45/104], Loss: 0.1918\n",
            "Epoch [1/200], Batch [50/104], Loss: 0.1531\n",
            "Epoch [1/200], Batch [55/104], Loss: 0.0736\n",
            "Epoch [1/200], Batch [60/104], Loss: 0.0807\n",
            "Epoch [1/200], Batch [65/104], Loss: 0.2379\n",
            "Epoch [1/200], Batch [70/104], Loss: 0.0643\n",
            "Epoch [1/200], Batch [75/104], Loss: 0.1512\n",
            "Epoch [1/200], Batch [80/104], Loss: 0.1106\n",
            "Epoch [1/200], Batch [85/104], Loss: 0.1661\n",
            "Epoch [1/200], Batch [90/104], Loss: 0.1387\n",
            "Epoch [1/200], Batch [95/104], Loss: 0.1880\n",
            "Epoch [1/200], Batch [100/104], Loss: 0.0390\n",
            "Epoch 1: Train Loss = 18.8350, Accuracy = 94.43%\n",
            "Epoch [2/200], Batch [5/104], Loss: 0.1008\n",
            "Epoch [2/200], Batch [10/104], Loss: 0.0994\n",
            "Epoch [2/200], Batch [15/104], Loss: 0.1910\n",
            "Epoch [2/200], Batch [20/104], Loss: 0.1522\n",
            "Epoch [2/200], Batch [25/104], Loss: 0.0630\n",
            "Epoch [2/200], Batch [30/104], Loss: 0.1337\n",
            "Epoch [2/200], Batch [35/104], Loss: 0.1764\n",
            "Epoch [2/200], Batch [40/104], Loss: 0.1094\n",
            "Epoch [2/200], Batch [45/104], Loss: 0.0978\n",
            "Epoch [2/200], Batch [50/104], Loss: 0.1450\n",
            "Epoch [2/200], Batch [55/104], Loss: 0.0505\n",
            "Epoch [2/200], Batch [60/104], Loss: 0.0278\n",
            "Epoch [2/200], Batch [65/104], Loss: 0.0997\n",
            "Epoch [2/200], Batch [70/104], Loss: 0.0955\n",
            "Epoch [2/200], Batch [75/104], Loss: 0.0739\n",
            "Epoch [2/200], Batch [80/104], Loss: 0.0638\n",
            "Epoch [2/200], Batch [85/104], Loss: 0.2231\n",
            "Epoch [2/200], Batch [90/104], Loss: 0.1774\n",
            "Epoch [2/200], Batch [95/104], Loss: 0.2517\n",
            "Epoch [2/200], Batch [100/104], Loss: 0.1948\n",
            "Epoch 2: Train Loss = 12.2751, Accuracy = 95.94%\n",
            "Epoch [3/200], Batch [5/104], Loss: 0.3618\n",
            "Epoch [3/200], Batch [10/104], Loss: 0.0566\n",
            "Epoch [3/200], Batch [15/104], Loss: 0.1604\n",
            "Epoch [3/200], Batch [20/104], Loss: 0.0944\n",
            "Epoch [3/200], Batch [25/104], Loss: 0.0421\n",
            "Epoch [3/200], Batch [30/104], Loss: 0.1670\n",
            "Epoch [3/200], Batch [35/104], Loss: 0.1127\n",
            "Epoch [3/200], Batch [40/104], Loss: 0.1175\n",
            "Epoch [3/200], Batch [45/104], Loss: 0.0188\n",
            "Epoch [3/200], Batch [50/104], Loss: 0.0166\n",
            "Epoch [3/200], Batch [55/104], Loss: 0.0759\n",
            "Epoch [3/200], Batch [60/104], Loss: 0.0465\n",
            "Epoch [3/200], Batch [65/104], Loss: 0.1284\n",
            "Epoch [3/200], Batch [70/104], Loss: 0.1232\n",
            "Epoch [3/200], Batch [75/104], Loss: 0.0867\n",
            "Epoch [3/200], Batch [80/104], Loss: 0.0800\n",
            "Epoch [3/200], Batch [85/104], Loss: 0.1198\n",
            "Epoch [3/200], Batch [90/104], Loss: 0.0533\n",
            "Epoch [3/200], Batch [95/104], Loss: 0.3423\n",
            "Epoch [3/200], Batch [100/104], Loss: 0.0519\n",
            "Epoch 3: Train Loss = 10.7516, Accuracy = 96.46%\n",
            "Epoch [4/200], Batch [5/104], Loss: 0.0762\n",
            "Epoch [4/200], Batch [10/104], Loss: 0.1250\n",
            "Epoch [4/200], Batch [15/104], Loss: 0.0909\n",
            "Epoch [4/200], Batch [20/104], Loss: 0.1672\n",
            "Epoch [4/200], Batch [25/104], Loss: 0.0773\n",
            "Epoch [4/200], Batch [30/104], Loss: 0.0241\n",
            "Epoch [4/200], Batch [35/104], Loss: 0.0700\n",
            "Epoch [4/200], Batch [40/104], Loss: 0.1672\n",
            "Epoch [4/200], Batch [45/104], Loss: 0.0840\n",
            "Epoch [4/200], Batch [50/104], Loss: 0.0437\n",
            "Epoch [4/200], Batch [55/104], Loss: 0.0773\n",
            "Epoch [4/200], Batch [60/104], Loss: 0.0645\n",
            "Epoch [4/200], Batch [65/104], Loss: 0.1506\n",
            "Epoch [4/200], Batch [70/104], Loss: 0.0399\n",
            "Epoch [4/200], Batch [75/104], Loss: 0.1741\n",
            "Epoch [4/200], Batch [80/104], Loss: 0.1161\n",
            "Epoch [4/200], Batch [85/104], Loss: 0.0588\n",
            "Epoch [4/200], Batch [90/104], Loss: 0.0210\n",
            "Epoch [4/200], Batch [95/104], Loss: 0.1929\n",
            "Epoch [4/200], Batch [100/104], Loss: 0.0174\n",
            "Epoch 4: Train Loss = 9.7000, Accuracy = 96.79%\n",
            "Epoch [5/200], Batch [5/104], Loss: 0.1013\n",
            "Epoch [5/200], Batch [10/104], Loss: 0.1477\n",
            "Epoch [5/200], Batch [15/104], Loss: 0.0088\n",
            "Epoch [5/200], Batch [20/104], Loss: 0.1226\n",
            "Epoch [5/200], Batch [25/104], Loss: 0.0385\n",
            "Epoch [5/200], Batch [30/104], Loss: 0.0485\n",
            "Epoch [5/200], Batch [35/104], Loss: 0.0421\n",
            "Epoch [5/200], Batch [40/104], Loss: 0.1063\n",
            "Epoch [5/200], Batch [45/104], Loss: 0.0525\n",
            "Epoch [5/200], Batch [50/104], Loss: 0.0657\n",
            "Epoch [5/200], Batch [55/104], Loss: 0.0769\n",
            "Epoch [5/200], Batch [60/104], Loss: 0.0173\n",
            "Epoch [5/200], Batch [65/104], Loss: 0.1476\n",
            "Epoch [5/200], Batch [70/104], Loss: 0.0792\n",
            "Epoch [5/200], Batch [75/104], Loss: 0.0225\n",
            "Epoch [5/200], Batch [80/104], Loss: 0.0339\n",
            "Epoch [5/200], Batch [85/104], Loss: 0.0500\n",
            "Epoch [5/200], Batch [90/104], Loss: 0.0445\n",
            "Epoch [5/200], Batch [95/104], Loss: 0.0640\n",
            "Epoch [5/200], Batch [100/104], Loss: 0.1592\n",
            "Epoch 5: Train Loss = 8.8055, Accuracy = 96.91%\n",
            "Epoch [6/200], Batch [5/104], Loss: 0.0783\n",
            "Epoch [6/200], Batch [10/104], Loss: 0.1667\n",
            "Epoch [6/200], Batch [15/104], Loss: 0.0468\n",
            "Epoch [6/200], Batch [20/104], Loss: 0.0829\n",
            "Epoch [6/200], Batch [25/104], Loss: 0.0245\n",
            "Epoch [6/200], Batch [30/104], Loss: 0.0705\n",
            "Epoch [6/200], Batch [35/104], Loss: 0.1621\n",
            "Epoch [6/200], Batch [40/104], Loss: 0.1333\n",
            "Epoch [6/200], Batch [45/104], Loss: 0.0091\n",
            "Epoch [6/200], Batch [50/104], Loss: 0.0955\n",
            "Epoch [6/200], Batch [55/104], Loss: 0.0786\n",
            "Epoch [6/200], Batch [60/104], Loss: 0.0242\n",
            "Epoch [6/200], Batch [65/104], Loss: 0.1046\n",
            "Epoch [6/200], Batch [70/104], Loss: 0.0627\n",
            "Epoch [6/200], Batch [75/104], Loss: 0.1129\n",
            "Epoch [6/200], Batch [80/104], Loss: 0.0193\n",
            "Epoch [6/200], Batch [85/104], Loss: 0.0273\n",
            "Epoch [6/200], Batch [90/104], Loss: 0.0147\n",
            "Epoch [6/200], Batch [95/104], Loss: 0.0095\n",
            "Epoch [6/200], Batch [100/104], Loss: 0.0757\n",
            "Epoch 6: Train Loss = 8.1802, Accuracy = 97.46%\n",
            "Epoch [7/200], Batch [5/104], Loss: 0.0127\n",
            "Epoch [7/200], Batch [10/104], Loss: 0.0572\n",
            "Epoch [7/200], Batch [15/104], Loss: 0.0130\n",
            "Epoch [7/200], Batch [20/104], Loss: 0.0543\n",
            "Epoch [7/200], Batch [25/104], Loss: 0.0869\n",
            "Epoch [7/200], Batch [30/104], Loss: 0.0518\n",
            "Epoch [7/200], Batch [35/104], Loss: 0.1074\n",
            "Epoch [7/200], Batch [40/104], Loss: 0.2361\n",
            "Epoch [7/200], Batch [45/104], Loss: 0.0620\n",
            "Epoch [7/200], Batch [50/104], Loss: 0.1185\n",
            "Epoch [7/200], Batch [55/104], Loss: 0.0454\n",
            "Epoch [7/200], Batch [60/104], Loss: 0.0623\n",
            "Epoch [7/200], Batch [65/104], Loss: 0.0675\n",
            "Epoch [7/200], Batch [70/104], Loss: 0.0164\n",
            "Epoch [7/200], Batch [75/104], Loss: 0.0494\n",
            "Epoch [7/200], Batch [80/104], Loss: 0.0160\n",
            "Epoch [7/200], Batch [85/104], Loss: 0.0433\n",
            "Epoch [7/200], Batch [90/104], Loss: 0.0446\n",
            "Epoch [7/200], Batch [95/104], Loss: 0.0591\n",
            "Epoch [7/200], Batch [100/104], Loss: 0.0744\n",
            "Epoch 7: Train Loss = 7.7088, Accuracy = 97.18%\n",
            "Epoch [8/200], Batch [5/104], Loss: 0.0459\n",
            "Epoch [8/200], Batch [10/104], Loss: 0.0587\n",
            "Epoch [8/200], Batch [15/104], Loss: 0.1018\n",
            "Epoch [8/200], Batch [20/104], Loss: 0.0253\n",
            "Epoch [8/200], Batch [25/104], Loss: 0.0718\n",
            "Epoch [8/200], Batch [30/104], Loss: 0.0738\n",
            "Epoch [8/200], Batch [35/104], Loss: 0.0829\n",
            "Epoch [8/200], Batch [40/104], Loss: 0.0577\n",
            "Epoch [8/200], Batch [45/104], Loss: 0.0373\n",
            "Epoch [8/200], Batch [50/104], Loss: 0.0957\n",
            "Epoch [8/200], Batch [55/104], Loss: 0.0268\n",
            "Epoch [8/200], Batch [60/104], Loss: 0.0178\n",
            "Epoch [8/200], Batch [65/104], Loss: 0.0875\n",
            "Epoch [8/200], Batch [70/104], Loss: 0.0399\n",
            "Epoch [8/200], Batch [75/104], Loss: 0.0970\n",
            "Epoch [8/200], Batch [80/104], Loss: 0.0057\n",
            "Epoch [8/200], Batch [85/104], Loss: 0.0332\n",
            "Epoch [8/200], Batch [90/104], Loss: 0.0448\n",
            "Epoch [8/200], Batch [95/104], Loss: 0.0481\n",
            "Epoch [8/200], Batch [100/104], Loss: 0.0627\n",
            "Epoch 8: Train Loss = 7.6440, Accuracy = 97.37%\n",
            "Epoch [9/200], Batch [5/104], Loss: 0.0963\n",
            "Epoch [9/200], Batch [10/104], Loss: 0.0589\n",
            "Epoch [9/200], Batch [15/104], Loss: 0.0379\n",
            "Epoch [9/200], Batch [20/104], Loss: 0.1573\n",
            "Epoch [9/200], Batch [25/104], Loss: 0.0516\n",
            "Epoch [9/200], Batch [30/104], Loss: 0.0207\n",
            "Epoch [9/200], Batch [35/104], Loss: 0.0150\n",
            "Epoch [9/200], Batch [40/104], Loss: 0.1113\n",
            "Epoch [9/200], Batch [45/104], Loss: 0.0271\n",
            "Epoch [9/200], Batch [50/104], Loss: 0.0115\n",
            "Epoch [9/200], Batch [55/104], Loss: 0.0638\n",
            "Epoch [9/200], Batch [60/104], Loss: 0.0128\n",
            "Epoch [9/200], Batch [65/104], Loss: 0.0529\n",
            "Epoch [9/200], Batch [70/104], Loss: 0.1516\n",
            "Epoch [9/200], Batch [75/104], Loss: 0.0542\n",
            "Epoch [9/200], Batch [80/104], Loss: 0.0594\n",
            "Epoch [9/200], Batch [85/104], Loss: 0.1058\n",
            "Epoch [9/200], Batch [90/104], Loss: 0.1831\n",
            "Epoch [9/200], Batch [95/104], Loss: 0.0576\n",
            "Epoch [9/200], Batch [100/104], Loss: 0.0901\n",
            "Epoch 9: Train Loss = 6.8935, Accuracy = 97.85%\n",
            "Epoch [10/200], Batch [5/104], Loss: 0.0550\n",
            "Epoch [10/200], Batch [10/104], Loss: 0.0278\n",
            "Epoch [10/200], Batch [15/104], Loss: 0.1030\n",
            "Epoch [10/200], Batch [20/104], Loss: 0.1048\n",
            "Epoch [10/200], Batch [25/104], Loss: 0.0331\n",
            "Epoch [10/200], Batch [30/104], Loss: 0.0060\n",
            "Epoch [10/200], Batch [35/104], Loss: 0.1212\n",
            "Epoch [10/200], Batch [40/104], Loss: 0.0322\n",
            "Epoch [10/200], Batch [45/104], Loss: 0.0972\n",
            "Epoch [10/200], Batch [50/104], Loss: 0.0412\n",
            "Epoch [10/200], Batch [55/104], Loss: 0.1058\n",
            "Epoch [10/200], Batch [60/104], Loss: 0.0453\n",
            "Epoch [10/200], Batch [65/104], Loss: 0.0152\n",
            "Epoch [10/200], Batch [70/104], Loss: 0.0707\n",
            "Epoch [10/200], Batch [75/104], Loss: 0.0925\n",
            "Epoch [10/200], Batch [80/104], Loss: 0.1212\n",
            "Epoch [10/200], Batch [85/104], Loss: 0.0563\n",
            "Epoch [10/200], Batch [90/104], Loss: 0.1272\n",
            "Epoch [10/200], Batch [95/104], Loss: 0.0192\n",
            "Epoch [10/200], Batch [100/104], Loss: 0.0528\n",
            "Epoch 10: Train Loss = 6.3402, Accuracy = 97.88%\n",
            "Epoch [11/200], Batch [5/104], Loss: 0.0696\n",
            "Epoch [11/200], Batch [10/104], Loss: 0.0342\n",
            "Epoch [11/200], Batch [15/104], Loss: 0.0034\n",
            "Epoch [11/200], Batch [20/104], Loss: 0.0964\n",
            "Epoch [11/200], Batch [25/104], Loss: 0.0111\n",
            "Epoch [11/200], Batch [30/104], Loss: 0.0553\n",
            "Epoch [11/200], Batch [35/104], Loss: 0.0285\n",
            "Epoch [11/200], Batch [40/104], Loss: 0.0304\n",
            "Epoch [11/200], Batch [45/104], Loss: 0.0404\n",
            "Epoch [11/200], Batch [50/104], Loss: 0.0475\n",
            "Epoch [11/200], Batch [55/104], Loss: 0.0791\n",
            "Epoch [11/200], Batch [60/104], Loss: 0.0400\n",
            "Epoch [11/200], Batch [65/104], Loss: 0.0444\n",
            "Epoch [11/200], Batch [70/104], Loss: 0.0742\n",
            "Epoch [11/200], Batch [75/104], Loss: 0.0221\n",
            "Epoch [11/200], Batch [80/104], Loss: 0.0945\n",
            "Epoch [11/200], Batch [85/104], Loss: 0.0064\n",
            "Epoch [11/200], Batch [90/104], Loss: 0.1532\n",
            "Epoch [11/200], Batch [95/104], Loss: 0.0555\n",
            "Epoch [11/200], Batch [100/104], Loss: 0.1185\n",
            "Epoch 11: Train Loss = 6.0512, Accuracy = 97.97%\n",
            "Epoch [12/200], Batch [5/104], Loss: 0.0457\n",
            "Epoch [12/200], Batch [10/104], Loss: 0.0590\n",
            "Epoch [12/200], Batch [15/104], Loss: 0.0562\n",
            "Epoch [12/200], Batch [20/104], Loss: 0.0224\n",
            "Epoch [12/200], Batch [25/104], Loss: 0.0665\n",
            "Epoch [12/200], Batch [30/104], Loss: 0.0144\n",
            "Epoch [12/200], Batch [35/104], Loss: 0.2013\n",
            "Epoch [12/200], Batch [40/104], Loss: 0.0042\n",
            "Epoch [12/200], Batch [45/104], Loss: 0.0715\n",
            "Epoch [12/200], Batch [50/104], Loss: 0.0953\n",
            "Epoch [12/200], Batch [55/104], Loss: 0.0072\n",
            "Epoch [12/200], Batch [60/104], Loss: 0.1098\n",
            "Epoch [12/200], Batch [65/104], Loss: 0.0190\n",
            "Epoch [12/200], Batch [70/104], Loss: 0.0428\n",
            "Epoch [12/200], Batch [75/104], Loss: 0.0490\n",
            "Epoch [12/200], Batch [80/104], Loss: 0.0843\n",
            "Epoch [12/200], Batch [85/104], Loss: 0.0089\n",
            "Epoch [12/200], Batch [90/104], Loss: 0.0249\n",
            "Epoch [12/200], Batch [95/104], Loss: 0.0453\n",
            "Epoch [12/200], Batch [100/104], Loss: 0.0315\n",
            "Epoch 12: Train Loss = 5.7068, Accuracy = 98.06%\n",
            "Epoch [13/200], Batch [5/104], Loss: 0.0729\n",
            "Epoch [13/200], Batch [10/104], Loss: 0.1081\n",
            "Epoch [13/200], Batch [15/104], Loss: 0.0316\n",
            "Epoch [13/200], Batch [20/104], Loss: 0.0576\n",
            "Epoch [13/200], Batch [25/104], Loss: 0.0278\n",
            "Epoch [13/200], Batch [30/104], Loss: 0.0358\n",
            "Epoch [13/200], Batch [35/104], Loss: 0.0774\n",
            "Epoch [13/200], Batch [40/104], Loss: 0.0654\n",
            "Epoch [13/200], Batch [45/104], Loss: 0.1478\n",
            "Epoch [13/200], Batch [50/104], Loss: 0.0290\n",
            "Epoch [13/200], Batch [55/104], Loss: 0.0879\n",
            "Epoch [13/200], Batch [60/104], Loss: 0.2350\n",
            "Epoch [13/200], Batch [65/104], Loss: 0.0213\n",
            "Epoch [13/200], Batch [70/104], Loss: 0.0497\n",
            "Epoch [13/200], Batch [75/104], Loss: 0.0383\n",
            "Epoch [13/200], Batch [80/104], Loss: 0.0241\n",
            "Epoch [13/200], Batch [85/104], Loss: 0.0303\n",
            "Epoch [13/200], Batch [90/104], Loss: 0.1261\n",
            "Epoch [13/200], Batch [95/104], Loss: 0.0500\n",
            "Epoch [13/200], Batch [100/104], Loss: 0.0119\n",
            "Epoch 13: Train Loss = 5.6198, Accuracy = 98.39%\n",
            "Epoch [14/200], Batch [5/104], Loss: 0.0831\n",
            "Epoch [14/200], Batch [10/104], Loss: 0.0061\n",
            "Epoch [14/200], Batch [15/104], Loss: 0.0410\n",
            "Epoch [14/200], Batch [20/104], Loss: 0.1624\n",
            "Epoch [14/200], Batch [25/104], Loss: 0.0018\n",
            "Epoch [14/200], Batch [30/104], Loss: 0.0316\n",
            "Epoch [14/200], Batch [35/104], Loss: 0.0467\n",
            "Epoch [14/200], Batch [40/104], Loss: 0.0210\n",
            "Epoch [14/200], Batch [45/104], Loss: 0.1133\n",
            "Epoch [14/200], Batch [50/104], Loss: 0.0094\n",
            "Epoch [14/200], Batch [55/104], Loss: 0.0779\n",
            "Epoch [14/200], Batch [60/104], Loss: 0.0305\n",
            "Epoch [14/200], Batch [65/104], Loss: 0.0085\n",
            "Epoch [14/200], Batch [70/104], Loss: 0.1055\n",
            "Epoch [14/200], Batch [75/104], Loss: 0.0125\n",
            "Epoch [14/200], Batch [80/104], Loss: 0.0100\n",
            "Epoch [14/200], Batch [85/104], Loss: 0.1083\n",
            "Epoch [14/200], Batch [90/104], Loss: 0.0930\n",
            "Epoch [14/200], Batch [95/104], Loss: 0.0326\n",
            "Epoch [14/200], Batch [100/104], Loss: 0.0620\n",
            "Epoch 14: Train Loss = 5.1434, Accuracy = 98.39%\n",
            "Epoch [15/200], Batch [5/104], Loss: 0.0109\n",
            "Epoch [15/200], Batch [10/104], Loss: 0.0170\n",
            "Epoch [15/200], Batch [15/104], Loss: 0.0614\n",
            "Epoch [15/200], Batch [20/104], Loss: 0.0374\n",
            "Epoch [15/200], Batch [25/104], Loss: 0.0482\n",
            "Epoch [15/200], Batch [30/104], Loss: 0.0452\n",
            "Epoch [15/200], Batch [35/104], Loss: 0.0228\n",
            "Epoch [15/200], Batch [40/104], Loss: 0.0751\n",
            "Epoch [15/200], Batch [45/104], Loss: 0.0492\n",
            "Epoch [15/200], Batch [50/104], Loss: 0.0066\n",
            "Epoch [15/200], Batch [55/104], Loss: 0.0309\n",
            "Epoch [15/200], Batch [60/104], Loss: 0.0357\n",
            "Epoch [15/200], Batch [65/104], Loss: 0.0488\n",
            "Epoch [15/200], Batch [70/104], Loss: 0.0453\n",
            "Epoch [15/200], Batch [75/104], Loss: 0.0496\n",
            "Epoch [15/200], Batch [80/104], Loss: 0.0208\n",
            "Epoch [15/200], Batch [85/104], Loss: 0.0399\n",
            "Epoch [15/200], Batch [90/104], Loss: 0.0316\n",
            "Epoch [15/200], Batch [95/104], Loss: 0.0079\n",
            "Epoch [15/200], Batch [100/104], Loss: 0.0394\n",
            "Epoch 15: Train Loss = 4.9023, Accuracy = 98.76%\n",
            "Epoch [16/200], Batch [5/104], Loss: 0.0266\n",
            "Epoch [16/200], Batch [10/104], Loss: 0.0623\n",
            "Epoch [16/200], Batch [15/104], Loss: 0.0479\n",
            "Epoch [16/200], Batch [20/104], Loss: 0.0060\n",
            "Epoch [16/200], Batch [25/104], Loss: 0.0696\n",
            "Epoch [16/200], Batch [30/104], Loss: 0.0571\n",
            "Epoch [16/200], Batch [35/104], Loss: 0.0307\n",
            "Epoch [16/200], Batch [40/104], Loss: 0.0680\n",
            "Epoch [16/200], Batch [45/104], Loss: 0.0311\n",
            "Epoch [16/200], Batch [50/104], Loss: 0.0674\n",
            "Epoch [16/200], Batch [55/104], Loss: 0.0669\n",
            "Epoch [16/200], Batch [60/104], Loss: 0.0240\n",
            "Epoch [16/200], Batch [65/104], Loss: 0.0938\n",
            "Epoch [16/200], Batch [70/104], Loss: 0.0317\n",
            "Epoch [16/200], Batch [75/104], Loss: 0.0301\n",
            "Epoch [16/200], Batch [80/104], Loss: 0.0451\n",
            "Epoch [16/200], Batch [85/104], Loss: 0.0695\n",
            "Epoch [16/200], Batch [90/104], Loss: 0.0252\n",
            "Epoch [16/200], Batch [95/104], Loss: 0.0621\n",
            "Epoch [16/200], Batch [100/104], Loss: 0.0082\n",
            "Epoch 16: Train Loss = 4.5457, Accuracy = 98.82%\n",
            "Epoch [17/200], Batch [5/104], Loss: 0.0154\n",
            "Epoch [17/200], Batch [10/104], Loss: 0.1242\n",
            "Epoch [17/200], Batch [15/104], Loss: 0.0242\n",
            "Epoch [17/200], Batch [20/104], Loss: 0.0651\n",
            "Epoch [17/200], Batch [25/104], Loss: 0.0392\n",
            "Epoch [17/200], Batch [30/104], Loss: 0.0102\n",
            "Epoch [17/200], Batch [35/104], Loss: 0.0372\n",
            "Epoch [17/200], Batch [40/104], Loss: 0.0369\n",
            "Epoch [17/200], Batch [45/104], Loss: 0.0217\n",
            "Epoch [17/200], Batch [50/104], Loss: 0.0488\n",
            "Epoch [17/200], Batch [55/104], Loss: 0.0362\n",
            "Epoch [17/200], Batch [60/104], Loss: 0.0225\n",
            "Epoch [17/200], Batch [65/104], Loss: 0.0124\n",
            "Epoch [17/200], Batch [70/104], Loss: 0.0103\n",
            "Epoch [17/200], Batch [75/104], Loss: 0.0714\n",
            "Epoch [17/200], Batch [80/104], Loss: 0.0504\n",
            "Epoch [17/200], Batch [85/104], Loss: 0.0179\n",
            "Epoch [17/200], Batch [90/104], Loss: 0.0137\n",
            "Epoch [17/200], Batch [95/104], Loss: 0.0827\n",
            "Epoch [17/200], Batch [100/104], Loss: 0.0255\n",
            "Epoch 17: Train Loss = 4.4216, Accuracy = 98.82%\n",
            "Epoch [18/200], Batch [5/104], Loss: 0.0323\n",
            "Epoch [18/200], Batch [10/104], Loss: 0.0482\n",
            "Epoch [18/200], Batch [15/104], Loss: 0.0403\n",
            "Epoch [18/200], Batch [20/104], Loss: 0.0228\n",
            "Epoch [18/200], Batch [25/104], Loss: 0.1533\n",
            "Epoch [18/200], Batch [30/104], Loss: 0.0088\n",
            "Epoch [18/200], Batch [35/104], Loss: 0.0516\n",
            "Epoch [18/200], Batch [40/104], Loss: 0.0300\n",
            "Epoch [18/200], Batch [45/104], Loss: 0.0861\n",
            "Epoch [18/200], Batch [50/104], Loss: 0.0282\n",
            "Epoch [18/200], Batch [55/104], Loss: 0.0287\n",
            "Epoch [18/200], Batch [60/104], Loss: 0.0269\n",
            "Epoch [18/200], Batch [65/104], Loss: 0.0537\n",
            "Epoch [18/200], Batch [70/104], Loss: 0.0072\n",
            "Epoch [18/200], Batch [75/104], Loss: 0.0215\n",
            "Epoch [18/200], Batch [80/104], Loss: 0.0274\n",
            "Epoch [18/200], Batch [85/104], Loss: 0.0236\n",
            "Epoch [18/200], Batch [90/104], Loss: 0.0812\n",
            "Epoch [18/200], Batch [95/104], Loss: 0.0716\n",
            "Epoch [18/200], Batch [100/104], Loss: 0.0434\n",
            "Epoch 18: Train Loss = 4.3353, Accuracy = 98.94%\n",
            "Epoch [19/200], Batch [5/104], Loss: 0.0242\n",
            "Epoch [19/200], Batch [10/104], Loss: 0.0466\n",
            "Epoch [19/200], Batch [15/104], Loss: 0.0711\n",
            "Epoch [19/200], Batch [20/104], Loss: 0.0108\n",
            "Epoch [19/200], Batch [25/104], Loss: 0.0690\n",
            "Epoch [19/200], Batch [30/104], Loss: 0.0154\n",
            "Epoch [19/200], Batch [35/104], Loss: 0.0048\n",
            "Epoch [19/200], Batch [40/104], Loss: 0.0131\n",
            "Epoch [19/200], Batch [45/104], Loss: 0.0111\n",
            "Epoch [19/200], Batch [50/104], Loss: 0.0696\n",
            "Epoch [19/200], Batch [55/104], Loss: 0.0259\n",
            "Epoch [19/200], Batch [60/104], Loss: 0.0104\n",
            "Epoch [19/200], Batch [65/104], Loss: 0.0265\n",
            "Epoch [19/200], Batch [70/104], Loss: 0.0582\n",
            "Epoch [19/200], Batch [75/104], Loss: 0.0241\n",
            "Epoch [19/200], Batch [80/104], Loss: 0.0468\n",
            "Epoch [19/200], Batch [85/104], Loss: 0.0451\n",
            "Epoch [19/200], Batch [90/104], Loss: 0.0212\n",
            "Epoch [19/200], Batch [95/104], Loss: 0.0096\n",
            "Epoch [19/200], Batch [100/104], Loss: 0.0386\n",
            "Epoch 19: Train Loss = 4.1541, Accuracy = 98.94%\n",
            "Epoch [20/200], Batch [5/104], Loss: 0.0453\n",
            "Epoch [20/200], Batch [10/104], Loss: 0.0132\n",
            "Epoch [20/200], Batch [15/104], Loss: 0.0187\n",
            "Epoch [20/200], Batch [20/104], Loss: 0.0074\n",
            "Epoch [20/200], Batch [25/104], Loss: 0.0097\n",
            "Epoch [20/200], Batch [30/104], Loss: 0.0290\n",
            "Epoch [20/200], Batch [35/104], Loss: 0.0455\n",
            "Epoch [20/200], Batch [40/104], Loss: 0.0100\n",
            "Epoch [20/200], Batch [45/104], Loss: 0.0616\n",
            "Epoch [20/200], Batch [50/104], Loss: 0.0504\n",
            "Epoch [20/200], Batch [55/104], Loss: 0.0199\n",
            "Epoch [20/200], Batch [60/104], Loss: 0.0828\n",
            "Epoch [20/200], Batch [65/104], Loss: 0.0818\n",
            "Epoch [20/200], Batch [70/104], Loss: 0.1041\n",
            "Epoch [20/200], Batch [75/104], Loss: 0.0500\n",
            "Epoch [20/200], Batch [80/104], Loss: 0.0170\n",
            "Epoch [20/200], Batch [85/104], Loss: 0.0308\n",
            "Epoch [20/200], Batch [90/104], Loss: 0.0455\n",
            "Epoch [20/200], Batch [95/104], Loss: 0.0257\n",
            "Epoch [20/200], Batch [100/104], Loss: 0.0056\n",
            "Epoch 20: Train Loss = 3.7168, Accuracy = 99.03%\n",
            "Epoch [21/200], Batch [5/104], Loss: 0.0088\n",
            "Epoch [21/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [21/200], Batch [15/104], Loss: 0.0254\n",
            "Epoch [21/200], Batch [20/104], Loss: 0.0585\n",
            "Epoch [21/200], Batch [25/104], Loss: 0.0175\n",
            "Epoch [21/200], Batch [30/104], Loss: 0.0144\n",
            "Epoch [21/200], Batch [35/104], Loss: 0.0862\n",
            "Epoch [21/200], Batch [40/104], Loss: 0.0111\n",
            "Epoch [21/200], Batch [45/104], Loss: 0.0512\n",
            "Epoch [21/200], Batch [50/104], Loss: 0.0096\n",
            "Epoch [21/200], Batch [55/104], Loss: 0.0292\n",
            "Epoch [21/200], Batch [60/104], Loss: 0.0161\n",
            "Epoch [21/200], Batch [65/104], Loss: 0.0840\n",
            "Epoch [21/200], Batch [70/104], Loss: 0.0086\n",
            "Epoch [21/200], Batch [75/104], Loss: 0.0427\n",
            "Epoch [21/200], Batch [80/104], Loss: 0.0078\n",
            "Epoch [21/200], Batch [85/104], Loss: 0.0397\n",
            "Epoch [21/200], Batch [90/104], Loss: 0.0280\n",
            "Epoch [21/200], Batch [95/104], Loss: 0.0346\n",
            "Epoch [21/200], Batch [100/104], Loss: 0.0371\n",
            "Epoch 21: Train Loss = 3.9054, Accuracy = 99.03%\n",
            "Epoch [22/200], Batch [5/104], Loss: 0.0259\n",
            "Epoch [22/200], Batch [10/104], Loss: 0.0193\n",
            "Epoch [22/200], Batch [15/104], Loss: 0.0495\n",
            "Epoch [22/200], Batch [20/104], Loss: 0.0716\n",
            "Epoch [22/200], Batch [25/104], Loss: 0.1047\n",
            "Epoch [22/200], Batch [30/104], Loss: 0.0616\n",
            "Epoch [22/200], Batch [35/104], Loss: 0.1261\n",
            "Epoch [22/200], Batch [40/104], Loss: 0.0119\n",
            "Epoch [22/200], Batch [45/104], Loss: 0.0133\n",
            "Epoch [22/200], Batch [50/104], Loss: 0.0540\n",
            "Epoch [22/200], Batch [55/104], Loss: 0.0171\n",
            "Epoch [22/200], Batch [60/104], Loss: 0.0322\n",
            "Epoch [22/200], Batch [65/104], Loss: 0.0148\n",
            "Epoch [22/200], Batch [70/104], Loss: 0.0303\n",
            "Epoch [22/200], Batch [75/104], Loss: 0.0014\n",
            "Epoch [22/200], Batch [80/104], Loss: 0.0681\n",
            "Epoch [22/200], Batch [85/104], Loss: 0.0199\n",
            "Epoch [22/200], Batch [90/104], Loss: 0.0245\n",
            "Epoch [22/200], Batch [95/104], Loss: 0.0297\n",
            "Epoch [22/200], Batch [100/104], Loss: 0.0068\n",
            "Epoch 22: Train Loss = 3.6256, Accuracy = 99.09%\n",
            "Epoch [23/200], Batch [5/104], Loss: 0.0145\n",
            "Epoch [23/200], Batch [10/104], Loss: 0.0398\n",
            "Epoch [23/200], Batch [15/104], Loss: 0.0242\n",
            "Epoch [23/200], Batch [20/104], Loss: 0.0186\n",
            "Epoch [23/200], Batch [25/104], Loss: 0.0209\n",
            "Epoch [23/200], Batch [30/104], Loss: 0.0223\n",
            "Epoch [23/200], Batch [35/104], Loss: 0.0439\n",
            "Epoch [23/200], Batch [40/104], Loss: 0.0179\n",
            "Epoch [23/200], Batch [45/104], Loss: 0.0735\n",
            "Epoch [23/200], Batch [50/104], Loss: 0.0494\n",
            "Epoch [23/200], Batch [55/104], Loss: 0.0424\n",
            "Epoch [23/200], Batch [60/104], Loss: 0.0256\n",
            "Epoch [23/200], Batch [65/104], Loss: 0.0550\n",
            "Epoch [23/200], Batch [70/104], Loss: 0.0223\n",
            "Epoch [23/200], Batch [75/104], Loss: 0.0563\n",
            "Epoch [23/200], Batch [80/104], Loss: 0.0531\n",
            "Epoch [23/200], Batch [85/104], Loss: 0.0400\n",
            "Epoch [23/200], Batch [90/104], Loss: 0.0307\n",
            "Epoch [23/200], Batch [95/104], Loss: 0.0232\n",
            "Epoch [23/200], Batch [100/104], Loss: 0.0157\n",
            "Epoch 23: Train Loss = 3.4308, Accuracy = 99.15%\n",
            "Epoch [24/200], Batch [5/104], Loss: 0.0141\n",
            "Epoch [24/200], Batch [10/104], Loss: 0.0368\n",
            "Epoch [24/200], Batch [15/104], Loss: 0.0366\n",
            "Epoch [24/200], Batch [20/104], Loss: 0.0321\n",
            "Epoch [24/200], Batch [25/104], Loss: 0.0027\n",
            "Epoch [24/200], Batch [30/104], Loss: 0.0350\n",
            "Epoch [24/200], Batch [35/104], Loss: 0.0733\n",
            "Epoch [24/200], Batch [40/104], Loss: 0.0160\n",
            "Epoch [24/200], Batch [45/104], Loss: 0.0473\n",
            "Epoch [24/200], Batch [50/104], Loss: 0.0109\n",
            "Epoch [24/200], Batch [55/104], Loss: 0.0450\n",
            "Epoch [24/200], Batch [60/104], Loss: 0.0140\n",
            "Epoch [24/200], Batch [65/104], Loss: 0.0369\n",
            "Epoch [24/200], Batch [70/104], Loss: 0.0185\n",
            "Epoch [24/200], Batch [75/104], Loss: 0.0553\n",
            "Epoch [24/200], Batch [80/104], Loss: 0.0458\n",
            "Epoch [24/200], Batch [85/104], Loss: 0.0304\n",
            "Epoch [24/200], Batch [90/104], Loss: 0.0379\n",
            "Epoch [24/200], Batch [95/104], Loss: 0.0289\n",
            "Epoch [24/200], Batch [100/104], Loss: 0.0139\n",
            "Epoch 24: Train Loss = 3.1803, Accuracy = 99.33%\n",
            "Epoch [25/200], Batch [5/104], Loss: 0.1106\n",
            "Epoch [25/200], Batch [10/104], Loss: 0.0288\n",
            "Epoch [25/200], Batch [15/104], Loss: 0.0204\n",
            "Epoch [25/200], Batch [20/104], Loss: 0.0521\n",
            "Epoch [25/200], Batch [25/104], Loss: 0.0140\n",
            "Epoch [25/200], Batch [30/104], Loss: 0.0157\n",
            "Epoch [25/200], Batch [35/104], Loss: 0.0352\n",
            "Epoch [25/200], Batch [40/104], Loss: 0.0273\n",
            "Epoch [25/200], Batch [45/104], Loss: 0.0218\n",
            "Epoch [25/200], Batch [50/104], Loss: 0.0461\n",
            "Epoch [25/200], Batch [55/104], Loss: 0.1035\n",
            "Epoch [25/200], Batch [60/104], Loss: 0.0569\n",
            "Epoch [25/200], Batch [65/104], Loss: 0.0126\n",
            "Epoch [25/200], Batch [70/104], Loss: 0.0116\n",
            "Epoch [25/200], Batch [75/104], Loss: 0.0429\n",
            "Epoch [25/200], Batch [80/104], Loss: 0.0074\n",
            "Epoch [25/200], Batch [85/104], Loss: 0.0374\n",
            "Epoch [25/200], Batch [90/104], Loss: 0.0424\n",
            "Epoch [25/200], Batch [95/104], Loss: 0.0343\n",
            "Epoch [25/200], Batch [100/104], Loss: 0.0636\n",
            "Epoch 25: Train Loss = 3.4138, Accuracy = 99.15%\n",
            "Epoch [26/200], Batch [5/104], Loss: 0.0184\n",
            "Epoch [26/200], Batch [10/104], Loss: 0.0062\n",
            "Epoch [26/200], Batch [15/104], Loss: 0.0667\n",
            "Epoch [26/200], Batch [20/104], Loss: 0.0089\n",
            "Epoch [26/200], Batch [25/104], Loss: 0.0307\n",
            "Epoch [26/200], Batch [30/104], Loss: 0.0609\n",
            "Epoch [26/200], Batch [35/104], Loss: 0.0101\n",
            "Epoch [26/200], Batch [40/104], Loss: 0.0174\n",
            "Epoch [26/200], Batch [45/104], Loss: 0.0070\n",
            "Epoch [26/200], Batch [50/104], Loss: 0.0198\n",
            "Epoch [26/200], Batch [55/104], Loss: 0.0293\n",
            "Epoch [26/200], Batch [60/104], Loss: 0.0786\n",
            "Epoch [26/200], Batch [65/104], Loss: 0.0511\n",
            "Epoch [26/200], Batch [70/104], Loss: 0.0770\n",
            "Epoch [26/200], Batch [75/104], Loss: 0.0124\n",
            "Epoch [26/200], Batch [80/104], Loss: 0.0453\n",
            "Epoch [26/200], Batch [85/104], Loss: 0.0627\n",
            "Epoch [26/200], Batch [90/104], Loss: 0.0120\n",
            "Epoch [26/200], Batch [95/104], Loss: 0.0200\n",
            "Epoch [26/200], Batch [100/104], Loss: 0.0560\n",
            "Epoch 26: Train Loss = 3.0379, Accuracy = 99.24%\n",
            "Epoch [27/200], Batch [5/104], Loss: 0.0138\n",
            "Epoch [27/200], Batch [10/104], Loss: 0.0271\n",
            "Epoch [27/200], Batch [15/104], Loss: 0.0280\n",
            "Epoch [27/200], Batch [20/104], Loss: 0.0762\n",
            "Epoch [27/200], Batch [25/104], Loss: 0.0215\n",
            "Epoch [27/200], Batch [30/104], Loss: 0.0275\n",
            "Epoch [27/200], Batch [35/104], Loss: 0.0840\n",
            "Epoch [27/200], Batch [40/104], Loss: 0.0486\n",
            "Epoch [27/200], Batch [45/104], Loss: 0.0116\n",
            "Epoch [27/200], Batch [50/104], Loss: 0.0781\n",
            "Epoch [27/200], Batch [55/104], Loss: 0.0272\n",
            "Epoch [27/200], Batch [60/104], Loss: 0.0155\n",
            "Epoch [27/200], Batch [65/104], Loss: 0.0152\n",
            "Epoch [27/200], Batch [70/104], Loss: 0.0087\n",
            "Epoch [27/200], Batch [75/104], Loss: 0.0102\n",
            "Epoch [27/200], Batch [80/104], Loss: 0.0360\n",
            "Epoch [27/200], Batch [85/104], Loss: 0.0823\n",
            "Epoch [27/200], Batch [90/104], Loss: 0.0573\n",
            "Epoch [27/200], Batch [95/104], Loss: 0.0081\n",
            "Epoch [27/200], Batch [100/104], Loss: 0.0249\n",
            "Epoch 27: Train Loss = 3.2532, Accuracy = 99.09%\n",
            "Epoch [28/200], Batch [5/104], Loss: 0.0063\n",
            "Epoch [28/200], Batch [10/104], Loss: 0.0201\n",
            "Epoch [28/200], Batch [15/104], Loss: 0.0238\n",
            "Epoch [28/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [28/200], Batch [25/104], Loss: 0.0354\n",
            "Epoch [28/200], Batch [30/104], Loss: 0.0074\n",
            "Epoch [28/200], Batch [35/104], Loss: 0.0056\n",
            "Epoch [28/200], Batch [40/104], Loss: 0.0173\n",
            "Epoch [28/200], Batch [45/104], Loss: 0.0206\n",
            "Epoch [28/200], Batch [50/104], Loss: 0.0268\n",
            "Epoch [28/200], Batch [55/104], Loss: 0.0183\n",
            "Epoch [28/200], Batch [60/104], Loss: 0.0522\n",
            "Epoch [28/200], Batch [65/104], Loss: 0.0121\n",
            "Epoch [28/200], Batch [70/104], Loss: 0.1292\n",
            "Epoch [28/200], Batch [75/104], Loss: 0.0042\n",
            "Epoch [28/200], Batch [80/104], Loss: 0.0609\n",
            "Epoch [28/200], Batch [85/104], Loss: 0.0396\n",
            "Epoch [28/200], Batch [90/104], Loss: 0.0621\n",
            "Epoch [28/200], Batch [95/104], Loss: 0.0104\n",
            "Epoch [28/200], Batch [100/104], Loss: 0.0070\n",
            "Epoch 28: Train Loss = 3.1892, Accuracy = 99.24%\n",
            "Epoch [29/200], Batch [5/104], Loss: 0.0711\n",
            "Epoch [29/200], Batch [10/104], Loss: 0.0062\n",
            "Epoch [29/200], Batch [15/104], Loss: 0.0068\n",
            "Epoch [29/200], Batch [20/104], Loss: 0.0195\n",
            "Epoch [29/200], Batch [25/104], Loss: 0.0092\n",
            "Epoch [29/200], Batch [30/104], Loss: 0.0303\n",
            "Epoch [29/200], Batch [35/104], Loss: 0.0427\n",
            "Epoch [29/200], Batch [40/104], Loss: 0.0672\n",
            "Epoch [29/200], Batch [45/104], Loss: 0.0267\n",
            "Epoch [29/200], Batch [50/104], Loss: 0.0028\n",
            "Epoch [29/200], Batch [55/104], Loss: 0.0563\n",
            "Epoch [29/200], Batch [60/104], Loss: 0.0112\n",
            "Epoch [29/200], Batch [65/104], Loss: 0.0262\n",
            "Epoch [29/200], Batch [70/104], Loss: 0.0226\n",
            "Epoch [29/200], Batch [75/104], Loss: 0.0045\n",
            "Epoch [29/200], Batch [80/104], Loss: 0.0086\n",
            "Epoch [29/200], Batch [85/104], Loss: 0.0171\n",
            "Epoch [29/200], Batch [90/104], Loss: 0.0555\n",
            "Epoch [29/200], Batch [95/104], Loss: 0.0020\n",
            "Epoch [29/200], Batch [100/104], Loss: 0.0234\n",
            "Epoch 29: Train Loss = 2.9208, Accuracy = 99.30%\n",
            "Epoch [30/200], Batch [5/104], Loss: 0.0227\n",
            "Epoch [30/200], Batch [10/104], Loss: 0.0260\n",
            "Epoch [30/200], Batch [15/104], Loss: 0.0442\n",
            "Epoch [30/200], Batch [20/104], Loss: 0.0133\n",
            "Epoch [30/200], Batch [25/104], Loss: 0.0575\n",
            "Epoch [30/200], Batch [30/104], Loss: 0.0439\n",
            "Epoch [30/200], Batch [35/104], Loss: 0.0439\n",
            "Epoch [30/200], Batch [40/104], Loss: 0.0172\n",
            "Epoch [30/200], Batch [45/104], Loss: 0.0207\n",
            "Epoch [30/200], Batch [50/104], Loss: 0.0277\n",
            "Epoch [30/200], Batch [55/104], Loss: 0.0100\n",
            "Epoch [30/200], Batch [60/104], Loss: 0.0267\n",
            "Epoch [30/200], Batch [65/104], Loss: 0.0232\n",
            "Epoch [30/200], Batch [70/104], Loss: 0.0267\n",
            "Epoch [30/200], Batch [75/104], Loss: 0.0150\n",
            "Epoch [30/200], Batch [80/104], Loss: 0.0132\n",
            "Epoch [30/200], Batch [85/104], Loss: 0.0433\n",
            "Epoch [30/200], Batch [90/104], Loss: 0.0125\n",
            "Epoch [30/200], Batch [95/104], Loss: 0.0147\n",
            "Epoch [30/200], Batch [100/104], Loss: 0.0280\n",
            "Epoch 30: Train Loss = 2.5920, Accuracy = 99.52%\n",
            "Epoch [31/200], Batch [5/104], Loss: 0.0213\n",
            "Epoch [31/200], Batch [10/104], Loss: 0.0296\n",
            "Epoch [31/200], Batch [15/104], Loss: 0.0223\n",
            "Epoch [31/200], Batch [20/104], Loss: 0.0702\n",
            "Epoch [31/200], Batch [25/104], Loss: 0.0400\n",
            "Epoch [31/200], Batch [30/104], Loss: 0.0127\n",
            "Epoch [31/200], Batch [35/104], Loss: 0.0061\n",
            "Epoch [31/200], Batch [40/104], Loss: 0.0556\n",
            "Epoch [31/200], Batch [45/104], Loss: 0.0186\n",
            "Epoch [31/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [31/200], Batch [55/104], Loss: 0.0134\n",
            "Epoch [31/200], Batch [60/104], Loss: 0.0159\n",
            "Epoch [31/200], Batch [65/104], Loss: 0.0049\n",
            "Epoch [31/200], Batch [70/104], Loss: 0.0462\n",
            "Epoch [31/200], Batch [75/104], Loss: 0.0058\n",
            "Epoch [31/200], Batch [80/104], Loss: 0.0140\n",
            "Epoch [31/200], Batch [85/104], Loss: 0.0483\n",
            "Epoch [31/200], Batch [90/104], Loss: 0.0132\n",
            "Epoch [31/200], Batch [95/104], Loss: 0.0264\n",
            "Epoch [31/200], Batch [100/104], Loss: 0.0173\n",
            "Epoch 31: Train Loss = 2.8999, Accuracy = 99.24%\n",
            "Epoch [32/200], Batch [5/104], Loss: 0.0240\n",
            "Epoch [32/200], Batch [10/104], Loss: 0.0110\n",
            "Epoch [32/200], Batch [15/104], Loss: 0.0261\n",
            "Epoch [32/200], Batch [20/104], Loss: 0.0283\n",
            "Epoch [32/200], Batch [25/104], Loss: 0.0179\n",
            "Epoch [32/200], Batch [30/104], Loss: 0.0505\n",
            "Epoch [32/200], Batch [35/104], Loss: 0.0228\n",
            "Epoch [32/200], Batch [40/104], Loss: 0.0130\n",
            "Epoch [32/200], Batch [45/104], Loss: 0.0551\n",
            "Epoch [32/200], Batch [50/104], Loss: 0.0220\n",
            "Epoch [32/200], Batch [55/104], Loss: 0.0206\n",
            "Epoch [32/200], Batch [60/104], Loss: 0.0087\n",
            "Epoch [32/200], Batch [65/104], Loss: 0.0043\n",
            "Epoch [32/200], Batch [70/104], Loss: 0.0125\n",
            "Epoch [32/200], Batch [75/104], Loss: 0.0318\n",
            "Epoch [32/200], Batch [80/104], Loss: 0.0440\n",
            "Epoch [32/200], Batch [85/104], Loss: 0.0465\n",
            "Epoch [32/200], Batch [90/104], Loss: 0.0302\n",
            "Epoch [32/200], Batch [95/104], Loss: 0.0041\n",
            "Epoch [32/200], Batch [100/104], Loss: 0.0128\n",
            "Epoch 32: Train Loss = 2.4090, Accuracy = 99.52%\n",
            "Epoch [33/200], Batch [5/104], Loss: 0.0201\n",
            "Epoch [33/200], Batch [10/104], Loss: 0.0195\n",
            "Epoch [33/200], Batch [15/104], Loss: 0.0465\n",
            "Epoch [33/200], Batch [20/104], Loss: 0.0100\n",
            "Epoch [33/200], Batch [25/104], Loss: 0.0366\n",
            "Epoch [33/200], Batch [30/104], Loss: 0.0279\n",
            "Epoch [33/200], Batch [35/104], Loss: 0.0414\n",
            "Epoch [33/200], Batch [40/104], Loss: 0.0319\n",
            "Epoch [33/200], Batch [45/104], Loss: 0.0120\n",
            "Epoch [33/200], Batch [50/104], Loss: 0.0124\n",
            "Epoch [33/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [33/200], Batch [60/104], Loss: 0.0237\n",
            "Epoch [33/200], Batch [65/104], Loss: 0.0329\n",
            "Epoch [33/200], Batch [70/104], Loss: 0.0585\n",
            "Epoch [33/200], Batch [75/104], Loss: 0.0687\n",
            "Epoch [33/200], Batch [80/104], Loss: 0.0453\n",
            "Epoch [33/200], Batch [85/104], Loss: 0.0264\n",
            "Epoch [33/200], Batch [90/104], Loss: 0.0228\n",
            "Epoch [33/200], Batch [95/104], Loss: 0.0500\n",
            "Epoch [33/200], Batch [100/104], Loss: 0.0045\n",
            "Epoch 33: Train Loss = 2.4207, Accuracy = 99.45%\n",
            "Epoch [34/200], Batch [5/104], Loss: 0.0116\n",
            "Epoch [34/200], Batch [10/104], Loss: 0.0143\n",
            "Epoch [34/200], Batch [15/104], Loss: 0.0159\n",
            "Epoch [34/200], Batch [20/104], Loss: 0.0465\n",
            "Epoch [34/200], Batch [25/104], Loss: 0.0213\n",
            "Epoch [34/200], Batch [30/104], Loss: 0.0181\n",
            "Epoch [34/200], Batch [35/104], Loss: 0.0373\n",
            "Epoch [34/200], Batch [40/104], Loss: 0.0475\n",
            "Epoch [34/200], Batch [45/104], Loss: 0.0092\n",
            "Epoch [34/200], Batch [50/104], Loss: 0.0165\n",
            "Epoch [34/200], Batch [55/104], Loss: 0.0139\n",
            "Epoch [34/200], Batch [60/104], Loss: 0.0159\n",
            "Epoch [34/200], Batch [65/104], Loss: 0.0103\n",
            "Epoch [34/200], Batch [70/104], Loss: 0.0153\n",
            "Epoch [34/200], Batch [75/104], Loss: 0.0222\n",
            "Epoch [34/200], Batch [80/104], Loss: 0.0174\n",
            "Epoch [34/200], Batch [85/104], Loss: 0.0306\n",
            "Epoch [34/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [34/200], Batch [95/104], Loss: 0.0246\n",
            "Epoch [34/200], Batch [100/104], Loss: 0.0124\n",
            "Epoch 34: Train Loss = 2.4058, Accuracy = 99.49%\n",
            "Epoch [35/200], Batch [5/104], Loss: 0.0275\n",
            "Epoch [35/200], Batch [10/104], Loss: 0.0194\n",
            "Epoch [35/200], Batch [15/104], Loss: 0.0270\n",
            "Epoch [35/200], Batch [20/104], Loss: 0.0407\n",
            "Epoch [35/200], Batch [25/104], Loss: 0.0195\n",
            "Epoch [35/200], Batch [30/104], Loss: 0.0077\n",
            "Epoch [35/200], Batch [35/104], Loss: 0.0154\n",
            "Epoch [35/200], Batch [40/104], Loss: 0.0070\n",
            "Epoch [35/200], Batch [45/104], Loss: 0.0346\n",
            "Epoch [35/200], Batch [50/104], Loss: 0.0089\n",
            "Epoch [35/200], Batch [55/104], Loss: 0.0043\n",
            "Epoch [35/200], Batch [60/104], Loss: 0.0365\n",
            "Epoch [35/200], Batch [65/104], Loss: 0.0690\n",
            "Epoch [35/200], Batch [70/104], Loss: 0.0096\n",
            "Epoch [35/200], Batch [75/104], Loss: 0.0142\n",
            "Epoch [35/200], Batch [80/104], Loss: 0.0057\n",
            "Epoch [35/200], Batch [85/104], Loss: 0.0112\n",
            "Epoch [35/200], Batch [90/104], Loss: 0.0147\n",
            "Epoch [35/200], Batch [95/104], Loss: 0.0319\n",
            "Epoch [35/200], Batch [100/104], Loss: 0.0087\n",
            "Epoch 35: Train Loss = 2.4809, Accuracy = 99.45%\n",
            "Epoch [36/200], Batch [5/104], Loss: 0.0113\n",
            "Epoch [36/200], Batch [10/104], Loss: 0.0066\n",
            "Epoch [36/200], Batch [15/104], Loss: 0.0193\n",
            "Epoch [36/200], Batch [20/104], Loss: 0.0163\n",
            "Epoch [36/200], Batch [25/104], Loss: 0.0177\n",
            "Epoch [36/200], Batch [30/104], Loss: 0.0108\n",
            "Epoch [36/200], Batch [35/104], Loss: 0.0490\n",
            "Epoch [36/200], Batch [40/104], Loss: 0.0071\n",
            "Epoch [36/200], Batch [45/104], Loss: 0.0237\n",
            "Epoch [36/200], Batch [50/104], Loss: 0.0131\n",
            "Epoch [36/200], Batch [55/104], Loss: 0.0365\n",
            "Epoch [36/200], Batch [60/104], Loss: 0.0065\n",
            "Epoch [36/200], Batch [65/104], Loss: 0.0193\n",
            "Epoch [36/200], Batch [70/104], Loss: 0.0287\n",
            "Epoch [36/200], Batch [75/104], Loss: 0.0185\n",
            "Epoch [36/200], Batch [80/104], Loss: 0.0095\n",
            "Epoch [36/200], Batch [85/104], Loss: 0.0190\n",
            "Epoch [36/200], Batch [90/104], Loss: 0.0078\n",
            "Epoch [36/200], Batch [95/104], Loss: 0.0384\n",
            "Epoch [36/200], Batch [100/104], Loss: 0.0174\n",
            "Epoch 36: Train Loss = 2.3641, Accuracy = 99.55%\n",
            "Epoch [37/200], Batch [5/104], Loss: 0.0054\n",
            "Epoch [37/200], Batch [10/104], Loss: 0.0587\n",
            "Epoch [37/200], Batch [15/104], Loss: 0.0065\n",
            "Epoch [37/200], Batch [20/104], Loss: 0.0111\n",
            "Epoch [37/200], Batch [25/104], Loss: 0.0103\n",
            "Epoch [37/200], Batch [30/104], Loss: 0.0054\n",
            "Epoch [37/200], Batch [35/104], Loss: 0.0321\n",
            "Epoch [37/200], Batch [40/104], Loss: 0.0634\n",
            "Epoch [37/200], Batch [45/104], Loss: 0.0204\n",
            "Epoch [37/200], Batch [50/104], Loss: 0.0057\n",
            "Epoch [37/200], Batch [55/104], Loss: 0.0217\n",
            "Epoch [37/200], Batch [60/104], Loss: 0.0329\n",
            "Epoch [37/200], Batch [65/104], Loss: 0.0083\n",
            "Epoch [37/200], Batch [70/104], Loss: 0.0183\n",
            "Epoch [37/200], Batch [75/104], Loss: 0.0123\n",
            "Epoch [37/200], Batch [80/104], Loss: 0.0029\n",
            "Epoch [37/200], Batch [85/104], Loss: 0.0065\n",
            "Epoch [37/200], Batch [90/104], Loss: 0.0235\n",
            "Epoch [37/200], Batch [95/104], Loss: 0.0127\n",
            "Epoch [37/200], Batch [100/104], Loss: 0.0187\n",
            "Epoch 37: Train Loss = 2.2022, Accuracy = 99.49%\n",
            "Epoch [38/200], Batch [5/104], Loss: 0.0157\n",
            "Epoch [38/200], Batch [10/104], Loss: 0.0121\n",
            "Epoch [38/200], Batch [15/104], Loss: 0.0207\n",
            "Epoch [38/200], Batch [20/104], Loss: 0.0319\n",
            "Epoch [38/200], Batch [25/104], Loss: 0.0080\n",
            "Epoch [38/200], Batch [30/104], Loss: 0.0105\n",
            "Epoch [38/200], Batch [35/104], Loss: 0.0196\n",
            "Epoch [38/200], Batch [40/104], Loss: 0.0214\n",
            "Epoch [38/200], Batch [45/104], Loss: 0.0262\n",
            "Epoch [38/200], Batch [50/104], Loss: 0.0234\n",
            "Epoch [38/200], Batch [55/104], Loss: 0.0246\n",
            "Epoch [38/200], Batch [60/104], Loss: 0.0243\n",
            "Epoch [38/200], Batch [65/104], Loss: 0.0210\n",
            "Epoch [38/200], Batch [70/104], Loss: 0.0070\n",
            "Epoch [38/200], Batch [75/104], Loss: 0.0078\n",
            "Epoch [38/200], Batch [80/104], Loss: 0.0113\n",
            "Epoch [38/200], Batch [85/104], Loss: 0.0248\n",
            "Epoch [38/200], Batch [90/104], Loss: 0.0102\n",
            "Epoch [38/200], Batch [95/104], Loss: 0.0220\n",
            "Epoch [38/200], Batch [100/104], Loss: 0.0217\n",
            "Epoch 38: Train Loss = 2.4311, Accuracy = 99.42%\n",
            "Epoch [39/200], Batch [5/104], Loss: 0.0034\n",
            "Epoch [39/200], Batch [10/104], Loss: 0.0307\n",
            "Epoch [39/200], Batch [15/104], Loss: 0.0553\n",
            "Epoch [39/200], Batch [20/104], Loss: 0.0083\n",
            "Epoch [39/200], Batch [25/104], Loss: 0.0184\n",
            "Epoch [39/200], Batch [30/104], Loss: 0.0109\n",
            "Epoch [39/200], Batch [35/104], Loss: 0.0157\n",
            "Epoch [39/200], Batch [40/104], Loss: 0.0136\n",
            "Epoch [39/200], Batch [45/104], Loss: 0.0368\n",
            "Epoch [39/200], Batch [50/104], Loss: 0.0280\n",
            "Epoch [39/200], Batch [55/104], Loss: 0.0151\n",
            "Epoch [39/200], Batch [60/104], Loss: 0.0087\n",
            "Epoch [39/200], Batch [65/104], Loss: 0.0532\n",
            "Epoch [39/200], Batch [70/104], Loss: 0.0294\n",
            "Epoch [39/200], Batch [75/104], Loss: 0.0353\n",
            "Epoch [39/200], Batch [80/104], Loss: 0.0067\n",
            "Epoch [39/200], Batch [85/104], Loss: 0.0174\n",
            "Epoch [39/200], Batch [90/104], Loss: 0.0093\n",
            "Epoch [39/200], Batch [95/104], Loss: 0.0225\n",
            "Epoch [39/200], Batch [100/104], Loss: 0.0102\n",
            "Epoch 39: Train Loss = 2.4528, Accuracy = 99.55%\n",
            "Epoch [40/200], Batch [5/104], Loss: 0.0119\n",
            "Epoch [40/200], Batch [10/104], Loss: 0.0104\n",
            "Epoch [40/200], Batch [15/104], Loss: 0.0174\n",
            "Epoch [40/200], Batch [20/104], Loss: 0.0104\n",
            "Epoch [40/200], Batch [25/104], Loss: 0.0215\n",
            "Epoch [40/200], Batch [30/104], Loss: 0.0033\n",
            "Epoch [40/200], Batch [35/104], Loss: 0.0147\n",
            "Epoch [40/200], Batch [40/104], Loss: 0.0297\n",
            "Epoch [40/200], Batch [45/104], Loss: 0.0217\n",
            "Epoch [40/200], Batch [50/104], Loss: 0.0261\n",
            "Epoch [40/200], Batch [55/104], Loss: 0.0177\n",
            "Epoch [40/200], Batch [60/104], Loss: 0.0074\n",
            "Epoch [40/200], Batch [65/104], Loss: 0.0155\n",
            "Epoch [40/200], Batch [70/104], Loss: 0.0114\n",
            "Epoch [40/200], Batch [75/104], Loss: 0.0511\n",
            "Epoch [40/200], Batch [80/104], Loss: 0.0023\n",
            "Epoch [40/200], Batch [85/104], Loss: 0.0266\n",
            "Epoch [40/200], Batch [90/104], Loss: 0.0076\n",
            "Epoch [40/200], Batch [95/104], Loss: 0.0150\n",
            "Epoch [40/200], Batch [100/104], Loss: 0.0225\n",
            "Epoch 40: Train Loss = 2.0708, Accuracy = 99.61%\n",
            "Epoch [41/200], Batch [5/104], Loss: 0.0186\n",
            "Epoch [41/200], Batch [10/104], Loss: 0.0213\n",
            "Epoch [41/200], Batch [15/104], Loss: 0.0839\n",
            "Epoch [41/200], Batch [20/104], Loss: 0.0260\n",
            "Epoch [41/200], Batch [25/104], Loss: 0.0087\n",
            "Epoch [41/200], Batch [30/104], Loss: 0.0159\n",
            "Epoch [41/200], Batch [35/104], Loss: 0.0163\n",
            "Epoch [41/200], Batch [40/104], Loss: 0.0264\n",
            "Epoch [41/200], Batch [45/104], Loss: 0.0129\n",
            "Epoch [41/200], Batch [50/104], Loss: 0.0111\n",
            "Epoch [41/200], Batch [55/104], Loss: 0.0064\n",
            "Epoch [41/200], Batch [60/104], Loss: 0.0238\n",
            "Epoch [41/200], Batch [65/104], Loss: 0.0318\n",
            "Epoch [41/200], Batch [70/104], Loss: 0.0102\n",
            "Epoch [41/200], Batch [75/104], Loss: 0.0117\n",
            "Epoch [41/200], Batch [80/104], Loss: 0.0124\n",
            "Epoch [41/200], Batch [85/104], Loss: 0.0110\n",
            "Epoch [41/200], Batch [90/104], Loss: 0.0315\n",
            "Epoch [41/200], Batch [95/104], Loss: 0.0255\n",
            "Epoch [41/200], Batch [100/104], Loss: 0.0367\n",
            "Epoch 41: Train Loss = 1.9099, Accuracy = 99.70%\n",
            "Epoch [42/200], Batch [5/104], Loss: 0.0043\n",
            "Epoch [42/200], Batch [10/104], Loss: 0.0156\n",
            "Epoch [42/200], Batch [15/104], Loss: 0.0061\n",
            "Epoch [42/200], Batch [20/104], Loss: 0.0364\n",
            "Epoch [42/200], Batch [25/104], Loss: 0.0365\n",
            "Epoch [42/200], Batch [30/104], Loss: 0.0089\n",
            "Epoch [42/200], Batch [35/104], Loss: 0.0157\n",
            "Epoch [42/200], Batch [40/104], Loss: 0.0651\n",
            "Epoch [42/200], Batch [45/104], Loss: 0.0220\n",
            "Epoch [42/200], Batch [50/104], Loss: 0.0102\n",
            "Epoch [42/200], Batch [55/104], Loss: 0.0054\n",
            "Epoch [42/200], Batch [60/104], Loss: 0.0080\n",
            "Epoch [42/200], Batch [65/104], Loss: 0.0080\n",
            "Epoch [42/200], Batch [70/104], Loss: 0.0149\n",
            "Epoch [42/200], Batch [75/104], Loss: 0.0042\n",
            "Epoch [42/200], Batch [80/104], Loss: 0.0298\n",
            "Epoch [42/200], Batch [85/104], Loss: 0.0145\n",
            "Epoch [42/200], Batch [90/104], Loss: 0.0201\n",
            "Epoch [42/200], Batch [95/104], Loss: 0.0122\n",
            "Epoch [42/200], Batch [100/104], Loss: 0.0047\n",
            "Epoch 42: Train Loss = 2.1131, Accuracy = 99.64%\n",
            "Epoch [43/200], Batch [5/104], Loss: 0.0138\n",
            "Epoch [43/200], Batch [10/104], Loss: 0.0053\n",
            "Epoch [43/200], Batch [15/104], Loss: 0.0189\n",
            "Epoch [43/200], Batch [20/104], Loss: 0.0170\n",
            "Epoch [43/200], Batch [25/104], Loss: 0.0090\n",
            "Epoch [43/200], Batch [30/104], Loss: 0.0058\n",
            "Epoch [43/200], Batch [35/104], Loss: 0.0293\n",
            "Epoch [43/200], Batch [40/104], Loss: 0.0101\n",
            "Epoch [43/200], Batch [45/104], Loss: 0.0083\n",
            "Epoch [43/200], Batch [50/104], Loss: 0.0331\n",
            "Epoch [43/200], Batch [55/104], Loss: 0.0074\n",
            "Epoch [43/200], Batch [60/104], Loss: 0.0195\n",
            "Epoch [43/200], Batch [65/104], Loss: 0.0136\n",
            "Epoch [43/200], Batch [70/104], Loss: 0.0065\n",
            "Epoch [43/200], Batch [75/104], Loss: 0.0331\n",
            "Epoch [43/200], Batch [80/104], Loss: 0.0142\n",
            "Epoch [43/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [43/200], Batch [90/104], Loss: 0.0350\n",
            "Epoch [43/200], Batch [95/104], Loss: 0.0463\n",
            "Epoch [43/200], Batch [100/104], Loss: 0.0336\n",
            "Epoch 43: Train Loss = 1.8750, Accuracy = 99.76%\n",
            "Epoch [44/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [44/200], Batch [10/104], Loss: 0.0558\n",
            "Epoch [44/200], Batch [15/104], Loss: 0.0174\n",
            "Epoch [44/200], Batch [20/104], Loss: 0.0222\n",
            "Epoch [44/200], Batch [25/104], Loss: 0.0097\n",
            "Epoch [44/200], Batch [30/104], Loss: 0.0273\n",
            "Epoch [44/200], Batch [35/104], Loss: 0.0719\n",
            "Epoch [44/200], Batch [40/104], Loss: 0.0126\n",
            "Epoch [44/200], Batch [45/104], Loss: 0.0225\n",
            "Epoch [44/200], Batch [50/104], Loss: 0.0142\n",
            "Epoch [44/200], Batch [55/104], Loss: 0.0104\n",
            "Epoch [44/200], Batch [60/104], Loss: 0.0148\n",
            "Epoch [44/200], Batch [65/104], Loss: 0.0074\n",
            "Epoch [44/200], Batch [70/104], Loss: 0.0311\n",
            "Epoch [44/200], Batch [75/104], Loss: 0.0347\n",
            "Epoch [44/200], Batch [80/104], Loss: 0.0383\n",
            "Epoch [44/200], Batch [85/104], Loss: 0.0089\n",
            "Epoch [44/200], Batch [90/104], Loss: 0.0051\n",
            "Epoch [44/200], Batch [95/104], Loss: 0.0359\n",
            "Epoch [44/200], Batch [100/104], Loss: 0.0740\n",
            "Epoch 44: Train Loss = 1.8567, Accuracy = 99.67%\n",
            "Epoch [45/200], Batch [5/104], Loss: 0.0132\n",
            "Epoch [45/200], Batch [10/104], Loss: 0.0454\n",
            "Epoch [45/200], Batch [15/104], Loss: 0.0278\n",
            "Epoch [45/200], Batch [20/104], Loss: 0.0009\n",
            "Epoch [45/200], Batch [25/104], Loss: 0.0059\n",
            "Epoch [45/200], Batch [30/104], Loss: 0.0157\n",
            "Epoch [45/200], Batch [35/104], Loss: 0.0126\n",
            "Epoch [45/200], Batch [40/104], Loss: 0.0067\n",
            "Epoch [45/200], Batch [45/104], Loss: 0.0212\n",
            "Epoch [45/200], Batch [50/104], Loss: 0.0147\n",
            "Epoch [45/200], Batch [55/104], Loss: 0.0133\n",
            "Epoch [45/200], Batch [60/104], Loss: 0.0160\n",
            "Epoch [45/200], Batch [65/104], Loss: 0.0081\n",
            "Epoch [45/200], Batch [70/104], Loss: 0.0221\n",
            "Epoch [45/200], Batch [75/104], Loss: 0.0060\n",
            "Epoch [45/200], Batch [80/104], Loss: 0.0143\n",
            "Epoch [45/200], Batch [85/104], Loss: 0.0073\n",
            "Epoch [45/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [45/200], Batch [95/104], Loss: 0.0225\n",
            "Epoch [45/200], Batch [100/104], Loss: 0.0064\n",
            "Epoch 45: Train Loss = 1.7918, Accuracy = 99.79%\n",
            "Epoch [46/200], Batch [5/104], Loss: 0.0210\n",
            "Epoch [46/200], Batch [10/104], Loss: 0.0040\n",
            "Epoch [46/200], Batch [15/104], Loss: 0.0059\n",
            "Epoch [46/200], Batch [20/104], Loss: 0.0139\n",
            "Epoch [46/200], Batch [25/104], Loss: 0.0114\n",
            "Epoch [46/200], Batch [30/104], Loss: 0.0120\n",
            "Epoch [46/200], Batch [35/104], Loss: 0.0049\n",
            "Epoch [46/200], Batch [40/104], Loss: 0.0164\n",
            "Epoch [46/200], Batch [45/104], Loss: 0.0043\n",
            "Epoch [46/200], Batch [50/104], Loss: 0.0232\n",
            "Epoch [46/200], Batch [55/104], Loss: 0.0105\n",
            "Epoch [46/200], Batch [60/104], Loss: 0.0236\n",
            "Epoch [46/200], Batch [65/104], Loss: 0.0062\n",
            "Epoch [46/200], Batch [70/104], Loss: 0.0326\n",
            "Epoch [46/200], Batch [75/104], Loss: 0.0034\n",
            "Epoch [46/200], Batch [80/104], Loss: 0.0238\n",
            "Epoch [46/200], Batch [85/104], Loss: 0.0156\n",
            "Epoch [46/200], Batch [90/104], Loss: 0.0090\n",
            "Epoch [46/200], Batch [95/104], Loss: 0.0094\n",
            "Epoch [46/200], Batch [100/104], Loss: 0.0129\n",
            "Epoch 46: Train Loss = 1.5758, Accuracy = 99.88%\n",
            "Epoch [47/200], Batch [5/104], Loss: 0.0145\n",
            "Epoch [47/200], Batch [10/104], Loss: 0.0115\n",
            "Epoch [47/200], Batch [15/104], Loss: 0.0183\n",
            "Epoch [47/200], Batch [20/104], Loss: 0.0102\n",
            "Epoch [47/200], Batch [25/104], Loss: 0.0138\n",
            "Epoch [47/200], Batch [30/104], Loss: 0.0465\n",
            "Epoch [47/200], Batch [35/104], Loss: 0.0043\n",
            "Epoch [47/200], Batch [40/104], Loss: 0.0051\n",
            "Epoch [47/200], Batch [45/104], Loss: 0.0075\n",
            "Epoch [47/200], Batch [50/104], Loss: 0.0261\n",
            "Epoch [47/200], Batch [55/104], Loss: 0.0145\n",
            "Epoch [47/200], Batch [60/104], Loss: 0.0222\n",
            "Epoch [47/200], Batch [65/104], Loss: 0.0069\n",
            "Epoch [47/200], Batch [70/104], Loss: 0.0234\n",
            "Epoch [47/200], Batch [75/104], Loss: 0.0052\n",
            "Epoch [47/200], Batch [80/104], Loss: 0.0079\n",
            "Epoch [47/200], Batch [85/104], Loss: 0.0046\n",
            "Epoch [47/200], Batch [90/104], Loss: 0.0018\n",
            "Epoch [47/200], Batch [95/104], Loss: 0.0034\n",
            "Epoch [47/200], Batch [100/104], Loss: 0.0180\n",
            "Epoch 47: Train Loss = 1.6033, Accuracy = 99.76%\n",
            "Epoch [48/200], Batch [5/104], Loss: 0.0040\n",
            "Epoch [48/200], Batch [10/104], Loss: 0.0096\n",
            "Epoch [48/200], Batch [15/104], Loss: 0.0310\n",
            "Epoch [48/200], Batch [20/104], Loss: 0.0098\n",
            "Epoch [48/200], Batch [25/104], Loss: 0.0324\n",
            "Epoch [48/200], Batch [30/104], Loss: 0.0080\n",
            "Epoch [48/200], Batch [35/104], Loss: 0.0190\n",
            "Epoch [48/200], Batch [40/104], Loss: 0.0103\n",
            "Epoch [48/200], Batch [45/104], Loss: 0.0295\n",
            "Epoch [48/200], Batch [50/104], Loss: 0.0193\n",
            "Epoch [48/200], Batch [55/104], Loss: 0.0654\n",
            "Epoch [48/200], Batch [60/104], Loss: 0.0457\n",
            "Epoch [48/200], Batch [65/104], Loss: 0.0074\n",
            "Epoch [48/200], Batch [70/104], Loss: 0.0250\n",
            "Epoch [48/200], Batch [75/104], Loss: 0.0319\n",
            "Epoch [48/200], Batch [80/104], Loss: 0.0077\n",
            "Epoch [48/200], Batch [85/104], Loss: 0.0108\n",
            "Epoch [48/200], Batch [90/104], Loss: 0.0186\n",
            "Epoch [48/200], Batch [95/104], Loss: 0.0208\n",
            "Epoch [48/200], Batch [100/104], Loss: 0.0052\n",
            "Epoch 48: Train Loss = 1.8509, Accuracy = 99.67%\n",
            "Epoch [49/200], Batch [5/104], Loss: 0.0024\n",
            "Epoch [49/200], Batch [10/104], Loss: 0.0137\n",
            "Epoch [49/200], Batch [15/104], Loss: 0.0225\n",
            "Epoch [49/200], Batch [20/104], Loss: 0.0225\n",
            "Epoch [49/200], Batch [25/104], Loss: 0.0075\n",
            "Epoch [49/200], Batch [30/104], Loss: 0.0312\n",
            "Epoch [49/200], Batch [35/104], Loss: 0.0100\n",
            "Epoch [49/200], Batch [40/104], Loss: 0.0038\n",
            "Epoch [49/200], Batch [45/104], Loss: 0.0145\n",
            "Epoch [49/200], Batch [50/104], Loss: 0.0296\n",
            "Epoch [49/200], Batch [55/104], Loss: 0.0034\n",
            "Epoch [49/200], Batch [60/104], Loss: 0.0141\n",
            "Epoch [49/200], Batch [65/104], Loss: 0.0347\n",
            "Epoch [49/200], Batch [70/104], Loss: 0.0049\n",
            "Epoch [49/200], Batch [75/104], Loss: 0.0172\n",
            "Epoch [49/200], Batch [80/104], Loss: 0.0104\n",
            "Epoch [49/200], Batch [85/104], Loss: 0.0025\n",
            "Epoch [49/200], Batch [90/104], Loss: 0.0788\n",
            "Epoch [49/200], Batch [95/104], Loss: 0.0108\n",
            "Epoch [49/200], Batch [100/104], Loss: 0.0098\n",
            "Epoch 49: Train Loss = 1.5350, Accuracy = 99.88%\n",
            "Epoch [50/200], Batch [5/104], Loss: 0.0106\n",
            "Epoch [50/200], Batch [10/104], Loss: 0.0026\n",
            "Epoch [50/200], Batch [15/104], Loss: 0.0248\n",
            "Epoch [50/200], Batch [20/104], Loss: 0.0077\n",
            "Epoch [50/200], Batch [25/104], Loss: 0.0442\n",
            "Epoch [50/200], Batch [30/104], Loss: 0.0075\n",
            "Epoch [50/200], Batch [35/104], Loss: 0.0086\n",
            "Epoch [50/200], Batch [40/104], Loss: 0.0115\n",
            "Epoch [50/200], Batch [45/104], Loss: 0.0049\n",
            "Epoch [50/200], Batch [50/104], Loss: 0.0095\n",
            "Epoch [50/200], Batch [55/104], Loss: 0.0118\n",
            "Epoch [50/200], Batch [60/104], Loss: 0.0045\n",
            "Epoch [50/200], Batch [65/104], Loss: 0.0056\n",
            "Epoch [50/200], Batch [70/104], Loss: 0.0106\n",
            "Epoch [50/200], Batch [75/104], Loss: 0.0087\n",
            "Epoch [50/200], Batch [80/104], Loss: 0.0016\n",
            "Epoch [50/200], Batch [85/104], Loss: 0.0033\n",
            "Epoch [50/200], Batch [90/104], Loss: 0.0380\n",
            "Epoch [50/200], Batch [95/104], Loss: 0.0263\n",
            "Epoch [50/200], Batch [100/104], Loss: 0.0022\n",
            "Epoch 50: Train Loss = 1.5950, Accuracy = 99.76%\n",
            "Epoch [51/200], Batch [5/104], Loss: 0.0063\n",
            "Epoch [51/200], Batch [10/104], Loss: 0.0115\n",
            "Epoch [51/200], Batch [15/104], Loss: 0.0203\n",
            "Epoch [51/200], Batch [20/104], Loss: 0.0058\n",
            "Epoch [51/200], Batch [25/104], Loss: 0.0121\n",
            "Epoch [51/200], Batch [30/104], Loss: 0.0294\n",
            "Epoch [51/200], Batch [35/104], Loss: 0.0067\n",
            "Epoch [51/200], Batch [40/104], Loss: 0.0403\n",
            "Epoch [51/200], Batch [45/104], Loss: 0.0163\n",
            "Epoch [51/200], Batch [50/104], Loss: 0.0028\n",
            "Epoch [51/200], Batch [55/104], Loss: 0.0121\n",
            "Epoch [51/200], Batch [60/104], Loss: 0.0083\n",
            "Epoch [51/200], Batch [65/104], Loss: 0.0259\n",
            "Epoch [51/200], Batch [70/104], Loss: 0.0239\n",
            "Epoch [51/200], Batch [75/104], Loss: 0.0178\n",
            "Epoch [51/200], Batch [80/104], Loss: 0.0212\n",
            "Epoch [51/200], Batch [85/104], Loss: 0.0271\n",
            "Epoch [51/200], Batch [90/104], Loss: 0.0111\n",
            "Epoch [51/200], Batch [95/104], Loss: 0.0148\n",
            "Epoch [51/200], Batch [100/104], Loss: 0.0192\n",
            "Epoch 51: Train Loss = 1.6638, Accuracy = 99.79%\n",
            "Epoch [52/200], Batch [5/104], Loss: 0.0193\n",
            "Epoch [52/200], Batch [10/104], Loss: 0.0019\n",
            "Epoch [52/200], Batch [15/104], Loss: 0.0081\n",
            "Epoch [52/200], Batch [20/104], Loss: 0.0176\n",
            "Epoch [52/200], Batch [25/104], Loss: 0.0108\n",
            "Epoch [52/200], Batch [30/104], Loss: 0.0070\n",
            "Epoch [52/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [52/200], Batch [40/104], Loss: 0.0067\n",
            "Epoch [52/200], Batch [45/104], Loss: 0.0082\n",
            "Epoch [52/200], Batch [50/104], Loss: 0.0170\n",
            "Epoch [52/200], Batch [55/104], Loss: 0.0428\n",
            "Epoch [52/200], Batch [60/104], Loss: 0.0019\n",
            "Epoch [52/200], Batch [65/104], Loss: 0.0125\n",
            "Epoch [52/200], Batch [70/104], Loss: 0.0073\n",
            "Epoch [52/200], Batch [75/104], Loss: 0.0156\n",
            "Epoch [52/200], Batch [80/104], Loss: 0.0085\n",
            "Epoch [52/200], Batch [85/104], Loss: 0.0326\n",
            "Epoch [52/200], Batch [90/104], Loss: 0.0133\n",
            "Epoch [52/200], Batch [95/104], Loss: 0.0306\n",
            "Epoch [52/200], Batch [100/104], Loss: 0.0245\n",
            "Epoch 52: Train Loss = 1.4923, Accuracy = 99.85%\n",
            "Epoch [53/200], Batch [5/104], Loss: 0.0073\n",
            "Epoch [53/200], Batch [10/104], Loss: 0.0027\n",
            "Epoch [53/200], Batch [15/104], Loss: 0.0055\n",
            "Epoch [53/200], Batch [20/104], Loss: 0.0056\n",
            "Epoch [53/200], Batch [25/104], Loss: 0.0109\n",
            "Epoch [53/200], Batch [30/104], Loss: 0.0028\n",
            "Epoch [53/200], Batch [35/104], Loss: 0.0089\n",
            "Epoch [53/200], Batch [40/104], Loss: 0.0269\n",
            "Epoch [53/200], Batch [45/104], Loss: 0.0327\n",
            "Epoch [53/200], Batch [50/104], Loss: 0.0056\n",
            "Epoch [53/200], Batch [55/104], Loss: 0.0230\n",
            "Epoch [53/200], Batch [60/104], Loss: 0.0168\n",
            "Epoch [53/200], Batch [65/104], Loss: 0.0077\n",
            "Epoch [53/200], Batch [70/104], Loss: 0.0137\n",
            "Epoch [53/200], Batch [75/104], Loss: 0.0138\n",
            "Epoch [53/200], Batch [80/104], Loss: 0.0087\n",
            "Epoch [53/200], Batch [85/104], Loss: 0.0213\n",
            "Epoch [53/200], Batch [90/104], Loss: 0.0063\n",
            "Epoch [53/200], Batch [95/104], Loss: 0.0129\n",
            "Epoch [53/200], Batch [100/104], Loss: 0.0077\n",
            "Epoch 53: Train Loss = 1.3642, Accuracy = 99.91%\n",
            "Epoch [54/200], Batch [5/104], Loss: 0.0134\n",
            "Epoch [54/200], Batch [10/104], Loss: 0.0065\n",
            "Epoch [54/200], Batch [15/104], Loss: 0.0053\n",
            "Epoch [54/200], Batch [20/104], Loss: 0.0143\n",
            "Epoch [54/200], Batch [25/104], Loss: 0.0079\n",
            "Epoch [54/200], Batch [30/104], Loss: 0.0029\n",
            "Epoch [54/200], Batch [35/104], Loss: 0.0067\n",
            "Epoch [54/200], Batch [40/104], Loss: 0.0242\n",
            "Epoch [54/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [54/200], Batch [50/104], Loss: 0.0012\n",
            "Epoch [54/200], Batch [55/104], Loss: 0.0080\n",
            "Epoch [54/200], Batch [60/104], Loss: 0.0086\n",
            "Epoch [54/200], Batch [65/104], Loss: 0.0145\n",
            "Epoch [54/200], Batch [70/104], Loss: 0.0123\n",
            "Epoch [54/200], Batch [75/104], Loss: 0.0137\n",
            "Epoch [54/200], Batch [80/104], Loss: 0.0097\n",
            "Epoch [54/200], Batch [85/104], Loss: 0.0118\n",
            "Epoch [54/200], Batch [90/104], Loss: 0.0187\n",
            "Epoch [54/200], Batch [95/104], Loss: 0.0110\n",
            "Epoch [54/200], Batch [100/104], Loss: 0.0106\n",
            "Epoch 54: Train Loss = 1.3152, Accuracy = 99.88%\n",
            "Epoch [55/200], Batch [5/104], Loss: 0.0227\n",
            "Epoch [55/200], Batch [10/104], Loss: 0.0084\n",
            "Epoch [55/200], Batch [15/104], Loss: 0.0104\n",
            "Epoch [55/200], Batch [20/104], Loss: 0.0062\n",
            "Epoch [55/200], Batch [25/104], Loss: 0.0077\n",
            "Epoch [55/200], Batch [30/104], Loss: 0.0218\n",
            "Epoch [55/200], Batch [35/104], Loss: 0.0057\n",
            "Epoch [55/200], Batch [40/104], Loss: 0.0135\n",
            "Epoch [55/200], Batch [45/104], Loss: 0.0043\n",
            "Epoch [55/200], Batch [50/104], Loss: 0.0015\n",
            "Epoch [55/200], Batch [55/104], Loss: 0.0368\n",
            "Epoch [55/200], Batch [60/104], Loss: 0.0332\n",
            "Epoch [55/200], Batch [65/104], Loss: 0.0147\n",
            "Epoch [55/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [55/200], Batch [75/104], Loss: 0.0054\n",
            "Epoch [55/200], Batch [80/104], Loss: 0.0111\n",
            "Epoch [55/200], Batch [85/104], Loss: 0.0186\n",
            "Epoch [55/200], Batch [90/104], Loss: 0.0175\n",
            "Epoch [55/200], Batch [95/104], Loss: 0.0038\n",
            "Epoch [55/200], Batch [100/104], Loss: 0.0057\n",
            "Epoch 55: Train Loss = 1.2214, Accuracy = 99.85%\n",
            "Epoch [56/200], Batch [5/104], Loss: 0.0316\n",
            "Epoch [56/200], Batch [10/104], Loss: 0.0069\n",
            "Epoch [56/200], Batch [15/104], Loss: 0.0045\n",
            "Epoch [56/200], Batch [20/104], Loss: 0.0130\n",
            "Epoch [56/200], Batch [25/104], Loss: 0.0067\n",
            "Epoch [56/200], Batch [30/104], Loss: 0.0200\n",
            "Epoch [56/200], Batch [35/104], Loss: 0.0073\n",
            "Epoch [56/200], Batch [40/104], Loss: 0.0078\n",
            "Epoch [56/200], Batch [45/104], Loss: 0.0161\n",
            "Epoch [56/200], Batch [50/104], Loss: 0.0098\n",
            "Epoch [56/200], Batch [55/104], Loss: 0.0099\n",
            "Epoch [56/200], Batch [60/104], Loss: 0.0043\n",
            "Epoch [56/200], Batch [65/104], Loss: 0.0030\n",
            "Epoch [56/200], Batch [70/104], Loss: 0.0020\n",
            "Epoch [56/200], Batch [75/104], Loss: 0.0454\n",
            "Epoch [56/200], Batch [80/104], Loss: 0.0101\n",
            "Epoch [56/200], Batch [85/104], Loss: 0.0117\n",
            "Epoch [56/200], Batch [90/104], Loss: 0.0134\n",
            "Epoch [56/200], Batch [95/104], Loss: 0.0023\n",
            "Epoch [56/200], Batch [100/104], Loss: 0.0428\n",
            "Epoch 56: Train Loss = 1.1928, Accuracy = 99.94%\n",
            "Epoch [57/200], Batch [5/104], Loss: 0.0059\n",
            "Epoch [57/200], Batch [10/104], Loss: 0.0111\n",
            "Epoch [57/200], Batch [15/104], Loss: 0.0140\n",
            "Epoch [57/200], Batch [20/104], Loss: 0.0020\n",
            "Epoch [57/200], Batch [25/104], Loss: 0.0118\n",
            "Epoch [57/200], Batch [30/104], Loss: 0.0078\n",
            "Epoch [57/200], Batch [35/104], Loss: 0.0084\n",
            "Epoch [57/200], Batch [40/104], Loss: 0.0125\n",
            "Epoch [57/200], Batch [45/104], Loss: 0.0013\n",
            "Epoch [57/200], Batch [50/104], Loss: 0.0239\n",
            "Epoch [57/200], Batch [55/104], Loss: 0.0065\n",
            "Epoch [57/200], Batch [60/104], Loss: 0.0135\n",
            "Epoch [57/200], Batch [65/104], Loss: 0.0379\n",
            "Epoch [57/200], Batch [70/104], Loss: 0.0055\n",
            "Epoch [57/200], Batch [75/104], Loss: 0.0121\n",
            "Epoch [57/200], Batch [80/104], Loss: 0.0427\n",
            "Epoch [57/200], Batch [85/104], Loss: 0.0069\n",
            "Epoch [57/200], Batch [90/104], Loss: 0.0063\n",
            "Epoch [57/200], Batch [95/104], Loss: 0.0042\n",
            "Epoch [57/200], Batch [100/104], Loss: 0.0062\n",
            "Epoch 57: Train Loss = 1.2216, Accuracy = 99.97%\n",
            "Epoch [58/200], Batch [5/104], Loss: 0.0366\n",
            "Epoch [58/200], Batch [10/104], Loss: 0.0637\n",
            "Epoch [58/200], Batch [15/104], Loss: 0.0139\n",
            "Epoch [58/200], Batch [20/104], Loss: 0.0151\n",
            "Epoch [58/200], Batch [25/104], Loss: 0.0254\n",
            "Epoch [58/200], Batch [30/104], Loss: 0.0149\n",
            "Epoch [58/200], Batch [35/104], Loss: 0.0179\n",
            "Epoch [58/200], Batch [40/104], Loss: 0.0105\n",
            "Epoch [58/200], Batch [45/104], Loss: 0.0206\n",
            "Epoch [58/200], Batch [50/104], Loss: 0.0214\n",
            "Epoch [58/200], Batch [55/104], Loss: 0.0077\n",
            "Epoch [58/200], Batch [60/104], Loss: 0.0082\n",
            "Epoch [58/200], Batch [65/104], Loss: 0.0152\n",
            "Epoch [58/200], Batch [70/104], Loss: 0.0087\n",
            "Epoch [58/200], Batch [75/104], Loss: 0.0018\n",
            "Epoch [58/200], Batch [80/104], Loss: 0.0090\n",
            "Epoch [58/200], Batch [85/104], Loss: 0.0050\n",
            "Epoch [58/200], Batch [90/104], Loss: 0.0140\n",
            "Epoch [58/200], Batch [95/104], Loss: 0.0038\n",
            "Epoch [58/200], Batch [100/104], Loss: 0.0039\n",
            "Epoch 58: Train Loss = 1.3060, Accuracy = 99.85%\n",
            "Epoch [59/200], Batch [5/104], Loss: 0.0197\n",
            "Epoch [59/200], Batch [10/104], Loss: 0.0043\n",
            "Epoch [59/200], Batch [15/104], Loss: 0.0184\n",
            "Epoch [59/200], Batch [20/104], Loss: 0.0021\n",
            "Epoch [59/200], Batch [25/104], Loss: 0.0028\n",
            "Epoch [59/200], Batch [30/104], Loss: 0.0115\n",
            "Epoch [59/200], Batch [35/104], Loss: 0.0099\n",
            "Epoch [59/200], Batch [40/104], Loss: 0.0015\n",
            "Epoch [59/200], Batch [45/104], Loss: 0.0177\n",
            "Epoch [59/200], Batch [50/104], Loss: 0.0047\n",
            "Epoch [59/200], Batch [55/104], Loss: 0.0031\n",
            "Epoch [59/200], Batch [60/104], Loss: 0.0077\n",
            "Epoch [59/200], Batch [65/104], Loss: 0.0278\n",
            "Epoch [59/200], Batch [70/104], Loss: 0.0108\n",
            "Epoch [59/200], Batch [75/104], Loss: 0.0235\n",
            "Epoch [59/200], Batch [80/104], Loss: 0.0068\n",
            "Epoch [59/200], Batch [85/104], Loss: 0.0023\n",
            "Epoch [59/200], Batch [90/104], Loss: 0.0237\n",
            "Epoch [59/200], Batch [95/104], Loss: 0.0130\n",
            "Epoch [59/200], Batch [100/104], Loss: 0.0155\n",
            "Epoch 59: Train Loss = 1.1544, Accuracy = 99.91%\n",
            "Epoch [60/200], Batch [5/104], Loss: 0.0044\n",
            "Epoch [60/200], Batch [10/104], Loss: 0.0152\n",
            "Epoch [60/200], Batch [15/104], Loss: 0.0082\n",
            "Epoch [60/200], Batch [20/104], Loss: 0.0186\n",
            "Epoch [60/200], Batch [25/104], Loss: 0.0072\n",
            "Epoch [60/200], Batch [30/104], Loss: 0.0098\n",
            "Epoch [60/200], Batch [35/104], Loss: 0.0133\n",
            "Epoch [60/200], Batch [40/104], Loss: 0.0098\n",
            "Epoch [60/200], Batch [45/104], Loss: 0.0094\n",
            "Epoch [60/200], Batch [50/104], Loss: 0.0123\n",
            "Epoch [60/200], Batch [55/104], Loss: 0.0126\n",
            "Epoch [60/200], Batch [60/104], Loss: 0.0122\n",
            "Epoch [60/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [60/200], Batch [70/104], Loss: 0.0104\n",
            "Epoch [60/200], Batch [75/104], Loss: 0.0096\n",
            "Epoch [60/200], Batch [80/104], Loss: 0.0273\n",
            "Epoch [60/200], Batch [85/104], Loss: 0.0107\n",
            "Epoch [60/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [60/200], Batch [95/104], Loss: 0.0031\n",
            "Epoch [60/200], Batch [100/104], Loss: 0.0105\n",
            "Epoch 60: Train Loss = 1.1399, Accuracy = 99.94%\n",
            "Epoch [61/200], Batch [5/104], Loss: 0.0085\n",
            "Epoch [61/200], Batch [10/104], Loss: 0.0111\n",
            "Epoch [61/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [61/200], Batch [20/104], Loss: 0.0042\n",
            "Epoch [61/200], Batch [25/104], Loss: 0.0048\n",
            "Epoch [61/200], Batch [30/104], Loss: 0.0183\n",
            "Epoch [61/200], Batch [35/104], Loss: 0.0068\n",
            "Epoch [61/200], Batch [40/104], Loss: 0.0034\n",
            "Epoch [61/200], Batch [45/104], Loss: 0.0213\n",
            "Epoch [61/200], Batch [50/104], Loss: 0.0338\n",
            "Epoch [61/200], Batch [55/104], Loss: 0.0330\n",
            "Epoch [61/200], Batch [60/104], Loss: 0.0149\n",
            "Epoch [61/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [61/200], Batch [70/104], Loss: 0.0062\n",
            "Epoch [61/200], Batch [75/104], Loss: 0.0102\n",
            "Epoch [61/200], Batch [80/104], Loss: 0.0104\n",
            "Epoch [61/200], Batch [85/104], Loss: 0.0367\n",
            "Epoch [61/200], Batch [90/104], Loss: 0.0026\n",
            "Epoch [61/200], Batch [95/104], Loss: 0.0121\n",
            "Epoch [61/200], Batch [100/104], Loss: 0.0074\n",
            "Epoch 61: Train Loss = 1.1415, Accuracy = 99.94%\n",
            "Epoch [62/200], Batch [5/104], Loss: 0.0135\n",
            "Epoch [62/200], Batch [10/104], Loss: 0.0091\n",
            "Epoch [62/200], Batch [15/104], Loss: 0.0114\n",
            "Epoch [62/200], Batch [20/104], Loss: 0.0046\n",
            "Epoch [62/200], Batch [25/104], Loss: 0.0083\n",
            "Epoch [62/200], Batch [30/104], Loss: 0.0089\n",
            "Epoch [62/200], Batch [35/104], Loss: 0.0101\n",
            "Epoch [62/200], Batch [40/104], Loss: 0.0011\n",
            "Epoch [62/200], Batch [45/104], Loss: 0.0063\n",
            "Epoch [62/200], Batch [50/104], Loss: 0.0135\n",
            "Epoch [62/200], Batch [55/104], Loss: 0.0112\n",
            "Epoch [62/200], Batch [60/104], Loss: 0.0151\n",
            "Epoch [62/200], Batch [65/104], Loss: 0.0062\n",
            "Epoch [62/200], Batch [70/104], Loss: 0.0044\n",
            "Epoch [62/200], Batch [75/104], Loss: 0.0057\n",
            "Epoch [62/200], Batch [80/104], Loss: 0.0095\n",
            "Epoch [62/200], Batch [85/104], Loss: 0.0046\n",
            "Epoch [62/200], Batch [90/104], Loss: 0.0069\n",
            "Epoch [62/200], Batch [95/104], Loss: 0.0036\n",
            "Epoch [62/200], Batch [100/104], Loss: 0.0326\n",
            "Epoch 62: Train Loss = 1.0921, Accuracy = 99.85%\n",
            "Epoch [63/200], Batch [5/104], Loss: 0.0099\n",
            "Epoch [63/200], Batch [10/104], Loss: 0.0039\n",
            "Epoch [63/200], Batch [15/104], Loss: 0.0036\n",
            "Epoch [63/200], Batch [20/104], Loss: 0.0224\n",
            "Epoch [63/200], Batch [25/104], Loss: 0.0054\n",
            "Epoch [63/200], Batch [30/104], Loss: 0.0065\n",
            "Epoch [63/200], Batch [35/104], Loss: 0.0056\n",
            "Epoch [63/200], Batch [40/104], Loss: 0.0102\n",
            "Epoch [63/200], Batch [45/104], Loss: 0.0042\n",
            "Epoch [63/200], Batch [50/104], Loss: 0.0179\n",
            "Epoch [63/200], Batch [55/104], Loss: 0.0155\n",
            "Epoch [63/200], Batch [60/104], Loss: 0.0026\n",
            "Epoch [63/200], Batch [65/104], Loss: 0.0060\n",
            "Epoch [63/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [63/200], Batch [75/104], Loss: 0.0123\n",
            "Epoch [63/200], Batch [80/104], Loss: 0.0185\n",
            "Epoch [63/200], Batch [85/104], Loss: 0.0066\n",
            "Epoch [63/200], Batch [90/104], Loss: 0.0211\n",
            "Epoch [63/200], Batch [95/104], Loss: 0.0010\n",
            "Epoch [63/200], Batch [100/104], Loss: 0.0144\n",
            "Epoch 63: Train Loss = 1.0710, Accuracy = 99.94%\n",
            "Epoch [64/200], Batch [5/104], Loss: 0.0139\n",
            "Epoch [64/200], Batch [10/104], Loss: 0.0126\n",
            "Epoch [64/200], Batch [15/104], Loss: 0.0013\n",
            "Epoch [64/200], Batch [20/104], Loss: 0.0133\n",
            "Epoch [64/200], Batch [25/104], Loss: 0.0060\n",
            "Epoch [64/200], Batch [30/104], Loss: 0.0100\n",
            "Epoch [64/200], Batch [35/104], Loss: 0.0344\n",
            "Epoch [64/200], Batch [40/104], Loss: 0.0089\n",
            "Epoch [64/200], Batch [45/104], Loss: 0.0065\n",
            "Epoch [64/200], Batch [50/104], Loss: 0.0408\n",
            "Epoch [64/200], Batch [55/104], Loss: 0.0029\n",
            "Epoch [64/200], Batch [60/104], Loss: 0.0026\n",
            "Epoch [64/200], Batch [65/104], Loss: 0.0065\n",
            "Epoch [64/200], Batch [70/104], Loss: 0.0085\n",
            "Epoch [64/200], Batch [75/104], Loss: 0.0049\n",
            "Epoch [64/200], Batch [80/104], Loss: 0.0099\n",
            "Epoch [64/200], Batch [85/104], Loss: 0.0168\n",
            "Epoch [64/200], Batch [90/104], Loss: 0.0077\n",
            "Epoch [64/200], Batch [95/104], Loss: 0.0137\n",
            "Epoch [64/200], Batch [100/104], Loss: 0.0150\n",
            "Epoch 64: Train Loss = 1.1655, Accuracy = 99.82%\n",
            "Epoch [65/200], Batch [5/104], Loss: 0.0148\n",
            "Epoch [65/200], Batch [10/104], Loss: 0.0036\n",
            "Epoch [65/200], Batch [15/104], Loss: 0.0286\n",
            "Epoch [65/200], Batch [20/104], Loss: 0.0089\n",
            "Epoch [65/200], Batch [25/104], Loss: 0.0098\n",
            "Epoch [65/200], Batch [30/104], Loss: 0.0149\n",
            "Epoch [65/200], Batch [35/104], Loss: 0.0089\n",
            "Epoch [65/200], Batch [40/104], Loss: 0.0149\n",
            "Epoch [65/200], Batch [45/104], Loss: 0.0036\n",
            "Epoch [65/200], Batch [50/104], Loss: 0.0129\n",
            "Epoch [65/200], Batch [55/104], Loss: 0.0135\n",
            "Epoch [65/200], Batch [60/104], Loss: 0.0074\n",
            "Epoch [65/200], Batch [65/104], Loss: 0.0028\n",
            "Epoch [65/200], Batch [70/104], Loss: 0.0048\n",
            "Epoch [65/200], Batch [75/104], Loss: 0.0231\n",
            "Epoch [65/200], Batch [80/104], Loss: 0.0171\n",
            "Epoch [65/200], Batch [85/104], Loss: 0.0061\n",
            "Epoch [65/200], Batch [90/104], Loss: 0.0072\n",
            "Epoch [65/200], Batch [95/104], Loss: 0.0229\n",
            "Epoch [65/200], Batch [100/104], Loss: 0.0093\n",
            "Epoch 65: Train Loss = 1.0964, Accuracy = 99.88%\n",
            "Epoch [66/200], Batch [5/104], Loss: 0.0168\n",
            "Epoch [66/200], Batch [10/104], Loss: 0.0044\n",
            "Epoch [66/200], Batch [15/104], Loss: 0.0095\n",
            "Epoch [66/200], Batch [20/104], Loss: 0.0023\n",
            "Epoch [66/200], Batch [25/104], Loss: 0.0063\n",
            "Epoch [66/200], Batch [30/104], Loss: 0.0064\n",
            "Epoch [66/200], Batch [35/104], Loss: 0.0151\n",
            "Epoch [66/200], Batch [40/104], Loss: 0.0162\n",
            "Epoch [66/200], Batch [45/104], Loss: 0.0066\n",
            "Epoch [66/200], Batch [50/104], Loss: 0.0012\n",
            "Epoch [66/200], Batch [55/104], Loss: 0.0066\n",
            "Epoch [66/200], Batch [60/104], Loss: 0.0166\n",
            "Epoch [66/200], Batch [65/104], Loss: 0.0154\n",
            "Epoch [66/200], Batch [70/104], Loss: 0.0085\n",
            "Epoch [66/200], Batch [75/104], Loss: 0.0183\n",
            "Epoch [66/200], Batch [80/104], Loss: 0.0130\n",
            "Epoch [66/200], Batch [85/104], Loss: 0.0090\n",
            "Epoch [66/200], Batch [90/104], Loss: 0.0062\n",
            "Epoch [66/200], Batch [95/104], Loss: 0.0085\n",
            "Epoch [66/200], Batch [100/104], Loss: 0.0055\n",
            "Epoch 66: Train Loss = 1.0074, Accuracy = 99.97%\n",
            "Epoch [67/200], Batch [5/104], Loss: 0.0107\n",
            "Epoch [67/200], Batch [10/104], Loss: 0.0013\n",
            "Epoch [67/200], Batch [15/104], Loss: 0.0025\n",
            "Epoch [67/200], Batch [20/104], Loss: 0.0087\n",
            "Epoch [67/200], Batch [25/104], Loss: 0.0029\n",
            "Epoch [67/200], Batch [30/104], Loss: 0.0221\n",
            "Epoch [67/200], Batch [35/104], Loss: 0.0058\n",
            "Epoch [67/200], Batch [40/104], Loss: 0.0365\n",
            "Epoch [67/200], Batch [45/104], Loss: 0.0083\n",
            "Epoch [67/200], Batch [50/104], Loss: 0.0221\n",
            "Epoch [67/200], Batch [55/104], Loss: 0.0169\n",
            "Epoch [67/200], Batch [60/104], Loss: 0.0060\n",
            "Epoch [67/200], Batch [65/104], Loss: 0.0160\n",
            "Epoch [67/200], Batch [70/104], Loss: 0.0196\n",
            "Epoch [67/200], Batch [75/104], Loss: 0.0303\n",
            "Epoch [67/200], Batch [80/104], Loss: 0.0116\n",
            "Epoch [67/200], Batch [85/104], Loss: 0.0125\n",
            "Epoch [67/200], Batch [90/104], Loss: 0.0046\n",
            "Epoch [67/200], Batch [95/104], Loss: 0.0026\n",
            "Epoch [67/200], Batch [100/104], Loss: 0.0074\n",
            "Epoch 67: Train Loss = 0.9740, Accuracy = 99.97%\n",
            "Epoch [68/200], Batch [5/104], Loss: 0.0118\n",
            "Epoch [68/200], Batch [10/104], Loss: 0.0234\n",
            "Epoch [68/200], Batch [15/104], Loss: 0.0044\n",
            "Epoch [68/200], Batch [20/104], Loss: 0.0068\n",
            "Epoch [68/200], Batch [25/104], Loss: 0.0152\n",
            "Epoch [68/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [68/200], Batch [35/104], Loss: 0.0062\n",
            "Epoch [68/200], Batch [40/104], Loss: 0.0016\n",
            "Epoch [68/200], Batch [45/104], Loss: 0.0218\n",
            "Epoch [68/200], Batch [50/104], Loss: 0.0103\n",
            "Epoch [68/200], Batch [55/104], Loss: 0.0082\n",
            "Epoch [68/200], Batch [60/104], Loss: 0.0089\n",
            "Epoch [68/200], Batch [65/104], Loss: 0.0060\n",
            "Epoch [68/200], Batch [70/104], Loss: 0.0148\n",
            "Epoch [68/200], Batch [75/104], Loss: 0.0154\n",
            "Epoch [68/200], Batch [80/104], Loss: 0.0118\n",
            "Epoch [68/200], Batch [85/104], Loss: 0.0119\n",
            "Epoch [68/200], Batch [90/104], Loss: 0.0020\n",
            "Epoch [68/200], Batch [95/104], Loss: 0.0185\n",
            "Epoch [68/200], Batch [100/104], Loss: 0.0062\n",
            "Epoch 68: Train Loss = 0.9100, Accuracy = 99.97%\n",
            "Epoch [69/200], Batch [5/104], Loss: 0.0072\n",
            "Epoch [69/200], Batch [10/104], Loss: 0.0049\n",
            "Epoch [69/200], Batch [15/104], Loss: 0.0098\n",
            "Epoch [69/200], Batch [20/104], Loss: 0.0072\n",
            "Epoch [69/200], Batch [25/104], Loss: 0.0129\n",
            "Epoch [69/200], Batch [30/104], Loss: 0.0054\n",
            "Epoch [69/200], Batch [35/104], Loss: 0.0075\n",
            "Epoch [69/200], Batch [40/104], Loss: 0.0204\n",
            "Epoch [69/200], Batch [45/104], Loss: 0.0043\n",
            "Epoch [69/200], Batch [50/104], Loss: 0.0109\n",
            "Epoch [69/200], Batch [55/104], Loss: 0.0033\n",
            "Epoch [69/200], Batch [60/104], Loss: 0.0077\n",
            "Epoch [69/200], Batch [65/104], Loss: 0.0090\n",
            "Epoch [69/200], Batch [70/104], Loss: 0.0099\n",
            "Epoch [69/200], Batch [75/104], Loss: 0.0041\n",
            "Epoch [69/200], Batch [80/104], Loss: 0.0081\n",
            "Epoch [69/200], Batch [85/104], Loss: 0.0012\n",
            "Epoch [69/200], Batch [90/104], Loss: 0.0027\n",
            "Epoch [69/200], Batch [95/104], Loss: 0.0051\n",
            "Epoch [69/200], Batch [100/104], Loss: 0.0034\n",
            "Epoch 69: Train Loss = 0.8505, Accuracy = 100.00%\n",
            "Epoch [70/200], Batch [5/104], Loss: 0.0261\n",
            "Epoch [70/200], Batch [10/104], Loss: 0.0022\n",
            "Epoch [70/200], Batch [15/104], Loss: 0.0104\n",
            "Epoch [70/200], Batch [20/104], Loss: 0.0021\n",
            "Epoch [70/200], Batch [25/104], Loss: 0.0107\n",
            "Epoch [70/200], Batch [30/104], Loss: 0.0369\n",
            "Epoch [70/200], Batch [35/104], Loss: 0.0246\n",
            "Epoch [70/200], Batch [40/104], Loss: 0.0040\n",
            "Epoch [70/200], Batch [45/104], Loss: 0.0116\n",
            "Epoch [70/200], Batch [50/104], Loss: 0.0079\n",
            "Epoch [70/200], Batch [55/104], Loss: 0.0124\n",
            "Epoch [70/200], Batch [60/104], Loss: 0.0073\n",
            "Epoch [70/200], Batch [65/104], Loss: 0.0169\n",
            "Epoch [70/200], Batch [70/104], Loss: 0.0158\n",
            "Epoch [70/200], Batch [75/104], Loss: 0.0077\n",
            "Epoch [70/200], Batch [80/104], Loss: 0.0040\n",
            "Epoch [70/200], Batch [85/104], Loss: 0.0060\n",
            "Epoch [70/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [70/200], Batch [95/104], Loss: 0.0055\n",
            "Epoch [70/200], Batch [100/104], Loss: 0.0048\n",
            "Epoch 70: Train Loss = 0.9604, Accuracy = 99.97%\n",
            "Epoch [71/200], Batch [5/104], Loss: 0.0054\n",
            "Epoch [71/200], Batch [10/104], Loss: 0.0108\n",
            "Epoch [71/200], Batch [15/104], Loss: 0.0128\n",
            "Epoch [71/200], Batch [20/104], Loss: 0.0054\n",
            "Epoch [71/200], Batch [25/104], Loss: 0.0024\n",
            "Epoch [71/200], Batch [30/104], Loss: 0.0107\n",
            "Epoch [71/200], Batch [35/104], Loss: 0.0081\n",
            "Epoch [71/200], Batch [40/104], Loss: 0.0083\n",
            "Epoch [71/200], Batch [45/104], Loss: 0.0032\n",
            "Epoch [71/200], Batch [50/104], Loss: 0.0025\n",
            "Epoch [71/200], Batch [55/104], Loss: 0.0083\n",
            "Epoch [71/200], Batch [60/104], Loss: 0.0099\n",
            "Epoch [71/200], Batch [65/104], Loss: 0.0184\n",
            "Epoch [71/200], Batch [70/104], Loss: 0.0071\n",
            "Epoch [71/200], Batch [75/104], Loss: 0.0109\n",
            "Epoch [71/200], Batch [80/104], Loss: 0.0031\n",
            "Epoch [71/200], Batch [85/104], Loss: 0.0117\n",
            "Epoch [71/200], Batch [90/104], Loss: 0.0290\n",
            "Epoch [71/200], Batch [95/104], Loss: 0.0008\n",
            "Epoch [71/200], Batch [100/104], Loss: 0.0217\n",
            "Epoch 71: Train Loss = 0.8838, Accuracy = 99.97%\n",
            "Epoch [72/200], Batch [5/104], Loss: 0.0165\n",
            "Epoch [72/200], Batch [10/104], Loss: 0.0069\n",
            "Epoch [72/200], Batch [15/104], Loss: 0.0073\n",
            "Epoch [72/200], Batch [20/104], Loss: 0.0053\n",
            "Epoch [72/200], Batch [25/104], Loss: 0.0108\n",
            "Epoch [72/200], Batch [30/104], Loss: 0.0055\n",
            "Epoch [72/200], Batch [35/104], Loss: 0.0041\n",
            "Epoch [72/200], Batch [40/104], Loss: 0.0059\n",
            "Epoch [72/200], Batch [45/104], Loss: 0.0185\n",
            "Epoch [72/200], Batch [50/104], Loss: 0.0031\n",
            "Epoch [72/200], Batch [55/104], Loss: 0.0436\n",
            "Epoch [72/200], Batch [60/104], Loss: 0.0018\n",
            "Epoch [72/200], Batch [65/104], Loss: 0.0127\n",
            "Epoch [72/200], Batch [70/104], Loss: 0.0044\n",
            "Epoch [72/200], Batch [75/104], Loss: 0.0026\n",
            "Epoch [72/200], Batch [80/104], Loss: 0.0159\n",
            "Epoch [72/200], Batch [85/104], Loss: 0.0092\n",
            "Epoch [72/200], Batch [90/104], Loss: 0.0145\n",
            "Epoch [72/200], Batch [95/104], Loss: 0.0068\n",
            "Epoch [72/200], Batch [100/104], Loss: 0.0134\n",
            "Epoch 72: Train Loss = 0.8876, Accuracy = 99.97%\n",
            "Epoch [73/200], Batch [5/104], Loss: 0.0040\n",
            "Epoch [73/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [73/200], Batch [15/104], Loss: 0.0142\n",
            "Epoch [73/200], Batch [20/104], Loss: 0.0194\n",
            "Epoch [73/200], Batch [25/104], Loss: 0.0062\n",
            "Epoch [73/200], Batch [30/104], Loss: 0.0073\n",
            "Epoch [73/200], Batch [35/104], Loss: 0.0084\n",
            "Epoch [73/200], Batch [40/104], Loss: 0.0101\n",
            "Epoch [73/200], Batch [45/104], Loss: 0.0090\n",
            "Epoch [73/200], Batch [50/104], Loss: 0.0083\n",
            "Epoch [73/200], Batch [55/104], Loss: 0.0234\n",
            "Epoch [73/200], Batch [60/104], Loss: 0.0107\n",
            "Epoch [73/200], Batch [65/104], Loss: 0.0083\n",
            "Epoch [73/200], Batch [70/104], Loss: 0.0078\n",
            "Epoch [73/200], Batch [75/104], Loss: 0.0124\n",
            "Epoch [73/200], Batch [80/104], Loss: 0.0097\n",
            "Epoch [73/200], Batch [85/104], Loss: 0.0191\n",
            "Epoch [73/200], Batch [90/104], Loss: 0.0034\n",
            "Epoch [73/200], Batch [95/104], Loss: 0.0105\n",
            "Epoch [73/200], Batch [100/104], Loss: 0.0110\n",
            "Epoch 73: Train Loss = 0.8601, Accuracy = 100.00%\n",
            "Epoch [74/200], Batch [5/104], Loss: 0.0038\n",
            "Epoch [74/200], Batch [10/104], Loss: 0.0141\n",
            "Epoch [74/200], Batch [15/104], Loss: 0.0156\n",
            "Epoch [74/200], Batch [20/104], Loss: 0.0039\n",
            "Epoch [74/200], Batch [25/104], Loss: 0.0056\n",
            "Epoch [74/200], Batch [30/104], Loss: 0.0020\n",
            "Epoch [74/200], Batch [35/104], Loss: 0.0163\n",
            "Epoch [74/200], Batch [40/104], Loss: 0.0014\n",
            "Epoch [74/200], Batch [45/104], Loss: 0.0150\n",
            "Epoch [74/200], Batch [50/104], Loss: 0.0059\n",
            "Epoch [74/200], Batch [55/104], Loss: 0.0075\n",
            "Epoch [74/200], Batch [60/104], Loss: 0.0080\n",
            "Epoch [74/200], Batch [65/104], Loss: 0.0043\n",
            "Epoch [74/200], Batch [70/104], Loss: 0.0028\n",
            "Epoch [74/200], Batch [75/104], Loss: 0.0141\n",
            "Epoch [74/200], Batch [80/104], Loss: 0.0038\n",
            "Epoch [74/200], Batch [85/104], Loss: 0.0011\n",
            "Epoch [74/200], Batch [90/104], Loss: 0.0099\n",
            "Epoch [74/200], Batch [95/104], Loss: 0.0113\n",
            "Epoch [74/200], Batch [100/104], Loss: 0.0062\n",
            "Epoch 74: Train Loss = 0.8421, Accuracy = 100.00%\n",
            "Epoch [75/200], Batch [5/104], Loss: 0.0244\n",
            "Epoch [75/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [75/200], Batch [15/104], Loss: 0.0035\n",
            "Epoch [75/200], Batch [20/104], Loss: 0.0034\n",
            "Epoch [75/200], Batch [25/104], Loss: 0.0098\n",
            "Epoch [75/200], Batch [30/104], Loss: 0.0084\n",
            "Epoch [75/200], Batch [35/104], Loss: 0.0078\n",
            "Epoch [75/200], Batch [40/104], Loss: 0.0080\n",
            "Epoch [75/200], Batch [45/104], Loss: 0.0038\n",
            "Epoch [75/200], Batch [50/104], Loss: 0.0096\n",
            "Epoch [75/200], Batch [55/104], Loss: 0.0085\n",
            "Epoch [75/200], Batch [60/104], Loss: 0.0118\n",
            "Epoch [75/200], Batch [65/104], Loss: 0.0105\n",
            "Epoch [75/200], Batch [70/104], Loss: 0.0028\n",
            "Epoch [75/200], Batch [75/104], Loss: 0.0126\n",
            "Epoch [75/200], Batch [80/104], Loss: 0.0100\n",
            "Epoch [75/200], Batch [85/104], Loss: 0.0138\n",
            "Epoch [75/200], Batch [90/104], Loss: 0.0029\n",
            "Epoch [75/200], Batch [95/104], Loss: 0.0091\n",
            "Epoch [75/200], Batch [100/104], Loss: 0.0018\n",
            "Epoch 75: Train Loss = 0.8498, Accuracy = 99.97%\n",
            "Epoch [76/200], Batch [5/104], Loss: 0.0051\n",
            "Epoch [76/200], Batch [10/104], Loss: 0.0083\n",
            "Epoch [76/200], Batch [15/104], Loss: 0.0104\n",
            "Epoch [76/200], Batch [20/104], Loss: 0.0077\n",
            "Epoch [76/200], Batch [25/104], Loss: 0.0034\n",
            "Epoch [76/200], Batch [30/104], Loss: 0.0074\n",
            "Epoch [76/200], Batch [35/104], Loss: 0.0033\n",
            "Epoch [76/200], Batch [40/104], Loss: 0.0118\n",
            "Epoch [76/200], Batch [45/104], Loss: 0.0064\n",
            "Epoch [76/200], Batch [50/104], Loss: 0.0027\n",
            "Epoch [76/200], Batch [55/104], Loss: 0.0090\n",
            "Epoch [76/200], Batch [60/104], Loss: 0.0074\n",
            "Epoch [76/200], Batch [65/104], Loss: 0.0040\n",
            "Epoch [76/200], Batch [70/104], Loss: 0.0021\n",
            "Epoch [76/200], Batch [75/104], Loss: 0.0082\n",
            "Epoch [76/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [76/200], Batch [85/104], Loss: 0.0058\n",
            "Epoch [76/200], Batch [90/104], Loss: 0.0077\n",
            "Epoch [76/200], Batch [95/104], Loss: 0.0095\n",
            "Epoch [76/200], Batch [100/104], Loss: 0.0022\n",
            "Epoch 76: Train Loss = 0.8032, Accuracy = 100.00%\n",
            "Epoch [77/200], Batch [5/104], Loss: 0.0142\n",
            "Epoch [77/200], Batch [10/104], Loss: 0.0081\n",
            "Epoch [77/200], Batch [15/104], Loss: 0.0117\n",
            "Epoch [77/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [77/200], Batch [25/104], Loss: 0.0094\n",
            "Epoch [77/200], Batch [30/104], Loss: 0.0043\n",
            "Epoch [77/200], Batch [35/104], Loss: 0.0042\n",
            "Epoch [77/200], Batch [40/104], Loss: 0.0018\n",
            "Epoch [77/200], Batch [45/104], Loss: 0.0111\n",
            "Epoch [77/200], Batch [50/104], Loss: 0.0031\n",
            "Epoch [77/200], Batch [55/104], Loss: 0.0083\n",
            "Epoch [77/200], Batch [60/104], Loss: 0.0065\n",
            "Epoch [77/200], Batch [65/104], Loss: 0.0130\n",
            "Epoch [77/200], Batch [70/104], Loss: 0.0067\n",
            "Epoch [77/200], Batch [75/104], Loss: 0.0166\n",
            "Epoch [77/200], Batch [80/104], Loss: 0.0061\n",
            "Epoch [77/200], Batch [85/104], Loss: 0.0128\n",
            "Epoch [77/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [77/200], Batch [95/104], Loss: 0.0102\n",
            "Epoch [77/200], Batch [100/104], Loss: 0.0013\n",
            "Epoch 77: Train Loss = 0.7793, Accuracy = 100.00%\n",
            "Epoch [78/200], Batch [5/104], Loss: 0.0056\n",
            "Epoch [78/200], Batch [10/104], Loss: 0.0043\n",
            "Epoch [78/200], Batch [15/104], Loss: 0.0075\n",
            "Epoch [78/200], Batch [20/104], Loss: 0.0064\n",
            "Epoch [78/200], Batch [25/104], Loss: 0.0061\n",
            "Epoch [78/200], Batch [30/104], Loss: 0.0082\n",
            "Epoch [78/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [78/200], Batch [40/104], Loss: 0.0119\n",
            "Epoch [78/200], Batch [45/104], Loss: 0.0051\n",
            "Epoch [78/200], Batch [50/104], Loss: 0.0182\n",
            "Epoch [78/200], Batch [55/104], Loss: 0.0081\n",
            "Epoch [78/200], Batch [60/104], Loss: 0.0117\n",
            "Epoch [78/200], Batch [65/104], Loss: 0.0154\n",
            "Epoch [78/200], Batch [70/104], Loss: 0.0065\n",
            "Epoch [78/200], Batch [75/104], Loss: 0.0124\n",
            "Epoch [78/200], Batch [80/104], Loss: 0.0017\n",
            "Epoch [78/200], Batch [85/104], Loss: 0.0060\n",
            "Epoch [78/200], Batch [90/104], Loss: 0.0076\n",
            "Epoch [78/200], Batch [95/104], Loss: 0.0110\n",
            "Epoch [78/200], Batch [100/104], Loss: 0.0072\n",
            "Epoch 78: Train Loss = 0.7688, Accuracy = 100.00%\n",
            "Epoch [79/200], Batch [5/104], Loss: 0.0023\n",
            "Epoch [79/200], Batch [10/104], Loss: 0.0072\n",
            "Epoch [79/200], Batch [15/104], Loss: 0.0038\n",
            "Epoch [79/200], Batch [20/104], Loss: 0.0060\n",
            "Epoch [79/200], Batch [25/104], Loss: 0.0090\n",
            "Epoch [79/200], Batch [30/104], Loss: 0.0214\n",
            "Epoch [79/200], Batch [35/104], Loss: 0.0147\n",
            "Epoch [79/200], Batch [40/104], Loss: 0.0028\n",
            "Epoch [79/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [79/200], Batch [50/104], Loss: 0.0016\n",
            "Epoch [79/200], Batch [55/104], Loss: 0.0132\n",
            "Epoch [79/200], Batch [60/104], Loss: 0.0157\n",
            "Epoch [79/200], Batch [65/104], Loss: 0.0103\n",
            "Epoch [79/200], Batch [70/104], Loss: 0.0063\n",
            "Epoch [79/200], Batch [75/104], Loss: 0.0088\n",
            "Epoch [79/200], Batch [80/104], Loss: 0.0124\n",
            "Epoch [79/200], Batch [85/104], Loss: 0.0046\n",
            "Epoch [79/200], Batch [90/104], Loss: 0.0053\n",
            "Epoch [79/200], Batch [95/104], Loss: 0.0085\n",
            "Epoch [79/200], Batch [100/104], Loss: 0.0052\n",
            "Epoch 79: Train Loss = 0.7441, Accuracy = 100.00%\n",
            "Epoch [80/200], Batch [5/104], Loss: 0.0032\n",
            "Epoch [80/200], Batch [10/104], Loss: 0.0150\n",
            "Epoch [80/200], Batch [15/104], Loss: 0.0030\n",
            "Epoch [80/200], Batch [20/104], Loss: 0.0046\n",
            "Epoch [80/200], Batch [25/104], Loss: 0.0148\n",
            "Epoch [80/200], Batch [30/104], Loss: 0.0067\n",
            "Epoch [80/200], Batch [35/104], Loss: 0.0049\n",
            "Epoch [80/200], Batch [40/104], Loss: 0.0059\n",
            "Epoch [80/200], Batch [45/104], Loss: 0.0055\n",
            "Epoch [80/200], Batch [50/104], Loss: 0.0152\n",
            "Epoch [80/200], Batch [55/104], Loss: 0.0117\n",
            "Epoch [80/200], Batch [60/104], Loss: 0.0099\n",
            "Epoch [80/200], Batch [65/104], Loss: 0.0038\n",
            "Epoch [80/200], Batch [70/104], Loss: 0.0095\n",
            "Epoch [80/200], Batch [75/104], Loss: 0.0079\n",
            "Epoch [80/200], Batch [80/104], Loss: 0.0159\n",
            "Epoch [80/200], Batch [85/104], Loss: 0.0085\n",
            "Epoch [80/200], Batch [90/104], Loss: 0.0075\n",
            "Epoch [80/200], Batch [95/104], Loss: 0.0026\n",
            "Epoch [80/200], Batch [100/104], Loss: 0.0072\n",
            "Epoch 80: Train Loss = 0.7596, Accuracy = 100.00%\n",
            "Epoch [81/200], Batch [5/104], Loss: 0.0043\n",
            "Epoch [81/200], Batch [10/104], Loss: 0.0133\n",
            "Epoch [81/200], Batch [15/104], Loss: 0.0051\n",
            "Epoch [81/200], Batch [20/104], Loss: 0.0287\n",
            "Epoch [81/200], Batch [25/104], Loss: 0.0052\n",
            "Epoch [81/200], Batch [30/104], Loss: 0.0076\n",
            "Epoch [81/200], Batch [35/104], Loss: 0.0094\n",
            "Epoch [81/200], Batch [40/104], Loss: 0.0061\n",
            "Epoch [81/200], Batch [45/104], Loss: 0.0036\n",
            "Epoch [81/200], Batch [50/104], Loss: 0.0043\n",
            "Epoch [81/200], Batch [55/104], Loss: 0.0065\n",
            "Epoch [81/200], Batch [60/104], Loss: 0.0153\n",
            "Epoch [81/200], Batch [65/104], Loss: 0.0056\n",
            "Epoch [81/200], Batch [70/104], Loss: 0.0055\n",
            "Epoch [81/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [81/200], Batch [80/104], Loss: 0.0032\n",
            "Epoch [81/200], Batch [85/104], Loss: 0.0066\n",
            "Epoch [81/200], Batch [90/104], Loss: 0.0086\n",
            "Epoch [81/200], Batch [95/104], Loss: 0.0091\n",
            "Epoch [81/200], Batch [100/104], Loss: 0.0032\n",
            "Epoch 81: Train Loss = 0.7790, Accuracy = 100.00%\n",
            "Epoch [82/200], Batch [5/104], Loss: 0.0023\n",
            "Epoch [82/200], Batch [10/104], Loss: 0.0076\n",
            "Epoch [82/200], Batch [15/104], Loss: 0.0029\n",
            "Epoch [82/200], Batch [20/104], Loss: 0.0118\n",
            "Epoch [82/200], Batch [25/104], Loss: 0.0030\n",
            "Epoch [82/200], Batch [30/104], Loss: 0.0027\n",
            "Epoch [82/200], Batch [35/104], Loss: 0.0031\n",
            "Epoch [82/200], Batch [40/104], Loss: 0.0021\n",
            "Epoch [82/200], Batch [45/104], Loss: 0.0067\n",
            "Epoch [82/200], Batch [50/104], Loss: 0.0041\n",
            "Epoch [82/200], Batch [55/104], Loss: 0.0046\n",
            "Epoch [82/200], Batch [60/104], Loss: 0.0018\n",
            "Epoch [82/200], Batch [65/104], Loss: 0.0041\n",
            "Epoch [82/200], Batch [70/104], Loss: 0.0068\n",
            "Epoch [82/200], Batch [75/104], Loss: 0.0049\n",
            "Epoch [82/200], Batch [80/104], Loss: 0.0045\n",
            "Epoch [82/200], Batch [85/104], Loss: 0.0025\n",
            "Epoch [82/200], Batch [90/104], Loss: 0.0051\n",
            "Epoch [82/200], Batch [95/104], Loss: 0.0012\n",
            "Epoch [82/200], Batch [100/104], Loss: 0.0110\n",
            "Epoch 82: Train Loss = 0.7875, Accuracy = 100.00%\n",
            "Epoch [83/200], Batch [5/104], Loss: 0.0065\n",
            "Epoch [83/200], Batch [10/104], Loss: 0.0047\n",
            "Epoch [83/200], Batch [15/104], Loss: 0.0263\n",
            "Epoch [83/200], Batch [20/104], Loss: 0.0134\n",
            "Epoch [83/200], Batch [25/104], Loss: 0.0044\n",
            "Epoch [83/200], Batch [30/104], Loss: 0.0044\n",
            "Epoch [83/200], Batch [35/104], Loss: 0.0402\n",
            "Epoch [83/200], Batch [40/104], Loss: 0.0328\n",
            "Epoch [83/200], Batch [45/104], Loss: 0.0244\n",
            "Epoch [83/200], Batch [50/104], Loss: 0.0070\n",
            "Epoch [83/200], Batch [55/104], Loss: 0.0083\n",
            "Epoch [83/200], Batch [60/104], Loss: 0.0099\n",
            "Epoch [83/200], Batch [65/104], Loss: 0.0012\n",
            "Epoch [83/200], Batch [70/104], Loss: 0.0011\n",
            "Epoch [83/200], Batch [75/104], Loss: 0.0057\n",
            "Epoch [83/200], Batch [80/104], Loss: 0.0053\n",
            "Epoch [83/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [83/200], Batch [90/104], Loss: 0.0045\n",
            "Epoch [83/200], Batch [95/104], Loss: 0.0029\n",
            "Epoch [83/200], Batch [100/104], Loss: 0.0178\n",
            "Epoch 83: Train Loss = 0.9537, Accuracy = 99.88%\n",
            "Epoch [84/200], Batch [5/104], Loss: 0.0021\n",
            "Epoch [84/200], Batch [10/104], Loss: 0.0103\n",
            "Epoch [84/200], Batch [15/104], Loss: 0.0097\n",
            "Epoch [84/200], Batch [20/104], Loss: 0.0118\n",
            "Epoch [84/200], Batch [25/104], Loss: 0.0077\n",
            "Epoch [84/200], Batch [30/104], Loss: 0.0047\n",
            "Epoch [84/200], Batch [35/104], Loss: 0.0105\n",
            "Epoch [84/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [84/200], Batch [45/104], Loss: 0.0091\n",
            "Epoch [84/200], Batch [50/104], Loss: 0.0008\n",
            "Epoch [84/200], Batch [55/104], Loss: 0.0086\n",
            "Epoch [84/200], Batch [60/104], Loss: 0.0049\n",
            "Epoch [84/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [84/200], Batch [70/104], Loss: 0.0053\n",
            "Epoch [84/200], Batch [75/104], Loss: 0.0067\n",
            "Epoch [84/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [84/200], Batch [85/104], Loss: 0.0076\n",
            "Epoch [84/200], Batch [90/104], Loss: 0.0048\n",
            "Epoch [84/200], Batch [95/104], Loss: 0.0008\n",
            "Epoch [84/200], Batch [100/104], Loss: 0.0115\n",
            "Epoch 84: Train Loss = 0.7795, Accuracy = 100.00%\n",
            "Epoch [85/200], Batch [5/104], Loss: 0.0056\n",
            "Epoch [85/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [85/200], Batch [15/104], Loss: 0.0109\n",
            "Epoch [85/200], Batch [20/104], Loss: 0.0029\n",
            "Epoch [85/200], Batch [25/104], Loss: 0.0039\n",
            "Epoch [85/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [85/200], Batch [35/104], Loss: 0.0044\n",
            "Epoch [85/200], Batch [40/104], Loss: 0.0071\n",
            "Epoch [85/200], Batch [45/104], Loss: 0.0104\n",
            "Epoch [85/200], Batch [50/104], Loss: 0.0151\n",
            "Epoch [85/200], Batch [55/104], Loss: 0.0051\n",
            "Epoch [85/200], Batch [60/104], Loss: 0.0075\n",
            "Epoch [85/200], Batch [65/104], Loss: 0.0070\n",
            "Epoch [85/200], Batch [70/104], Loss: 0.0096\n",
            "Epoch [85/200], Batch [75/104], Loss: 0.0136\n",
            "Epoch [85/200], Batch [80/104], Loss: 0.0076\n",
            "Epoch [85/200], Batch [85/104], Loss: 0.0010\n",
            "Epoch [85/200], Batch [90/104], Loss: 0.0047\n",
            "Epoch [85/200], Batch [95/104], Loss: 0.0044\n",
            "Epoch [85/200], Batch [100/104], Loss: 0.0052\n",
            "Epoch 85: Train Loss = 0.8028, Accuracy = 99.94%\n",
            "Epoch [86/200], Batch [5/104], Loss: 0.0074\n",
            "Epoch [86/200], Batch [10/104], Loss: 0.0043\n",
            "Epoch [86/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [86/200], Batch [20/104], Loss: 0.0045\n",
            "Epoch [86/200], Batch [25/104], Loss: 0.0030\n",
            "Epoch [86/200], Batch [30/104], Loss: 0.0123\n",
            "Epoch [86/200], Batch [35/104], Loss: 0.0134\n",
            "Epoch [86/200], Batch [40/104], Loss: 0.0076\n",
            "Epoch [86/200], Batch [45/104], Loss: 0.0046\n",
            "Epoch [86/200], Batch [50/104], Loss: 0.0106\n",
            "Epoch [86/200], Batch [55/104], Loss: 0.0139\n",
            "Epoch [86/200], Batch [60/104], Loss: 0.0083\n",
            "Epoch [86/200], Batch [65/104], Loss: 0.0024\n",
            "Epoch [86/200], Batch [70/104], Loss: 0.0086\n",
            "Epoch [86/200], Batch [75/104], Loss: 0.0153\n",
            "Epoch [86/200], Batch [80/104], Loss: 0.0039\n",
            "Epoch [86/200], Batch [85/104], Loss: 0.0087\n",
            "Epoch [86/200], Batch [90/104], Loss: 0.0034\n",
            "Epoch [86/200], Batch [95/104], Loss: 0.0139\n",
            "Epoch [86/200], Batch [100/104], Loss: 0.0035\n",
            "Epoch 86: Train Loss = 0.6968, Accuracy = 100.00%\n",
            "Epoch [87/200], Batch [5/104], Loss: 0.0090\n",
            "Epoch [87/200], Batch [10/104], Loss: 0.0077\n",
            "Epoch [87/200], Batch [15/104], Loss: 0.0085\n",
            "Epoch [87/200], Batch [20/104], Loss: 0.0220\n",
            "Epoch [87/200], Batch [25/104], Loss: 0.0046\n",
            "Epoch [87/200], Batch [30/104], Loss: 0.0252\n",
            "Epoch [87/200], Batch [35/104], Loss: 0.0066\n",
            "Epoch [87/200], Batch [40/104], Loss: 0.0025\n",
            "Epoch [87/200], Batch [45/104], Loss: 0.0022\n",
            "Epoch [87/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [87/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [87/200], Batch [60/104], Loss: 0.0067\n",
            "Epoch [87/200], Batch [65/104], Loss: 0.0086\n",
            "Epoch [87/200], Batch [70/104], Loss: 0.0039\n",
            "Epoch [87/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [87/200], Batch [80/104], Loss: 0.0065\n",
            "Epoch [87/200], Batch [85/104], Loss: 0.0022\n",
            "Epoch [87/200], Batch [90/104], Loss: 0.0082\n",
            "Epoch [87/200], Batch [95/104], Loss: 0.0080\n",
            "Epoch [87/200], Batch [100/104], Loss: 0.0072\n",
            "Epoch 87: Train Loss = 0.7027, Accuracy = 100.00%\n",
            "Epoch [88/200], Batch [5/104], Loss: 0.0052\n",
            "Epoch [88/200], Batch [10/104], Loss: 0.0037\n",
            "Epoch [88/200], Batch [15/104], Loss: 0.0038\n",
            "Epoch [88/200], Batch [20/104], Loss: 0.0113\n",
            "Epoch [88/200], Batch [25/104], Loss: 0.0062\n",
            "Epoch [88/200], Batch [30/104], Loss: 0.0080\n",
            "Epoch [88/200], Batch [35/104], Loss: 0.0064\n",
            "Epoch [88/200], Batch [40/104], Loss: 0.0085\n",
            "Epoch [88/200], Batch [45/104], Loss: 0.0027\n",
            "Epoch [88/200], Batch [50/104], Loss: 0.0146\n",
            "Epoch [88/200], Batch [55/104], Loss: 0.0071\n",
            "Epoch [88/200], Batch [60/104], Loss: 0.0035\n",
            "Epoch [88/200], Batch [65/104], Loss: 0.0054\n",
            "Epoch [88/200], Batch [70/104], Loss: 0.0052\n",
            "Epoch [88/200], Batch [75/104], Loss: 0.0086\n",
            "Epoch [88/200], Batch [80/104], Loss: 0.0028\n",
            "Epoch [88/200], Batch [85/104], Loss: 0.0102\n",
            "Epoch [88/200], Batch [90/104], Loss: 0.0019\n",
            "Epoch [88/200], Batch [95/104], Loss: 0.0074\n",
            "Epoch [88/200], Batch [100/104], Loss: 0.0125\n",
            "Epoch 88: Train Loss = 0.6757, Accuracy = 100.00%\n",
            "Epoch [89/200], Batch [5/104], Loss: 0.0051\n",
            "Epoch [89/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [89/200], Batch [15/104], Loss: 0.0039\n",
            "Epoch [89/200], Batch [20/104], Loss: 0.0129\n",
            "Epoch [89/200], Batch [25/104], Loss: 0.0040\n",
            "Epoch [89/200], Batch [30/104], Loss: 0.0167\n",
            "Epoch [89/200], Batch [35/104], Loss: 0.0038\n",
            "Epoch [89/200], Batch [40/104], Loss: 0.0117\n",
            "Epoch [89/200], Batch [45/104], Loss: 0.0059\n",
            "Epoch [89/200], Batch [50/104], Loss: 0.0144\n",
            "Epoch [89/200], Batch [55/104], Loss: 0.0095\n",
            "Epoch [89/200], Batch [60/104], Loss: 0.0095\n",
            "Epoch [89/200], Batch [65/104], Loss: 0.0126\n",
            "Epoch [89/200], Batch [70/104], Loss: 0.0050\n",
            "Epoch [89/200], Batch [75/104], Loss: 0.0043\n",
            "Epoch [89/200], Batch [80/104], Loss: 0.0073\n",
            "Epoch [89/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [89/200], Batch [90/104], Loss: 0.0078\n",
            "Epoch [89/200], Batch [95/104], Loss: 0.0058\n",
            "Epoch [89/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 89: Train Loss = 0.6426, Accuracy = 100.00%\n",
            "Epoch [90/200], Batch [5/104], Loss: 0.0030\n",
            "Epoch [90/200], Batch [10/104], Loss: 0.0045\n",
            "Epoch [90/200], Batch [15/104], Loss: 0.0101\n",
            "Epoch [90/200], Batch [20/104], Loss: 0.0005\n",
            "Epoch [90/200], Batch [25/104], Loss: 0.0079\n",
            "Epoch [90/200], Batch [30/104], Loss: 0.0098\n",
            "Epoch [90/200], Batch [35/104], Loss: 0.0064\n",
            "Epoch [90/200], Batch [40/104], Loss: 0.0027\n",
            "Epoch [90/200], Batch [45/104], Loss: 0.0039\n",
            "Epoch [90/200], Batch [50/104], Loss: 0.0188\n",
            "Epoch [90/200], Batch [55/104], Loss: 0.0049\n",
            "Epoch [90/200], Batch [60/104], Loss: 0.0007\n",
            "Epoch [90/200], Batch [65/104], Loss: 0.0052\n",
            "Epoch [90/200], Batch [70/104], Loss: 0.0036\n",
            "Epoch [90/200], Batch [75/104], Loss: 0.0058\n",
            "Epoch [90/200], Batch [80/104], Loss: 0.0092\n",
            "Epoch [90/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [90/200], Batch [90/104], Loss: 0.0103\n",
            "Epoch [90/200], Batch [95/104], Loss: 0.0111\n",
            "Epoch [90/200], Batch [100/104], Loss: 0.0036\n",
            "Epoch 90: Train Loss = 0.6906, Accuracy = 100.00%\n",
            "Epoch [91/200], Batch [5/104], Loss: 0.0063\n",
            "Epoch [91/200], Batch [10/104], Loss: 0.0079\n",
            "Epoch [91/200], Batch [15/104], Loss: 0.0067\n",
            "Epoch [91/200], Batch [20/104], Loss: 0.0043\n",
            "Epoch [91/200], Batch [25/104], Loss: 0.0082\n",
            "Epoch [91/200], Batch [30/104], Loss: 0.0062\n",
            "Epoch [91/200], Batch [35/104], Loss: 0.0094\n",
            "Epoch [91/200], Batch [40/104], Loss: 0.0022\n",
            "Epoch [91/200], Batch [45/104], Loss: 0.0041\n",
            "Epoch [91/200], Batch [50/104], Loss: 0.0050\n",
            "Epoch [91/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [91/200], Batch [60/104], Loss: 0.0100\n",
            "Epoch [91/200], Batch [65/104], Loss: 0.0089\n",
            "Epoch [91/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [91/200], Batch [75/104], Loss: 0.0029\n",
            "Epoch [91/200], Batch [80/104], Loss: 0.0040\n",
            "Epoch [91/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [91/200], Batch [90/104], Loss: 0.0067\n",
            "Epoch [91/200], Batch [95/104], Loss: 0.0093\n",
            "Epoch [91/200], Batch [100/104], Loss: 0.0097\n",
            "Epoch 91: Train Loss = 0.7809, Accuracy = 99.97%\n",
            "Epoch [92/200], Batch [5/104], Loss: 0.0057\n",
            "Epoch [92/200], Batch [10/104], Loss: 0.0007\n",
            "Epoch [92/200], Batch [15/104], Loss: 0.0043\n",
            "Epoch [92/200], Batch [20/104], Loss: 0.0044\n",
            "Epoch [92/200], Batch [25/104], Loss: 0.0099\n",
            "Epoch [92/200], Batch [30/104], Loss: 0.0123\n",
            "Epoch [92/200], Batch [35/104], Loss: 0.0044\n",
            "Epoch [92/200], Batch [40/104], Loss: 0.0104\n",
            "Epoch [92/200], Batch [45/104], Loss: 0.0034\n",
            "Epoch [92/200], Batch [50/104], Loss: 0.0080\n",
            "Epoch [92/200], Batch [55/104], Loss: 0.0027\n",
            "Epoch [92/200], Batch [60/104], Loss: 0.0049\n",
            "Epoch [92/200], Batch [65/104], Loss: 0.0104\n",
            "Epoch [92/200], Batch [70/104], Loss: 0.0102\n",
            "Epoch [92/200], Batch [75/104], Loss: 0.0022\n",
            "Epoch [92/200], Batch [80/104], Loss: 0.0071\n",
            "Epoch [92/200], Batch [85/104], Loss: 0.0035\n",
            "Epoch [92/200], Batch [90/104], Loss: 0.0034\n",
            "Epoch [92/200], Batch [95/104], Loss: 0.0028\n",
            "Epoch [92/200], Batch [100/104], Loss: 0.0369\n",
            "Epoch 92: Train Loss = 0.6632, Accuracy = 99.97%\n",
            "Epoch [93/200], Batch [5/104], Loss: 0.0140\n",
            "Epoch [93/200], Batch [10/104], Loss: 0.0033\n",
            "Epoch [93/200], Batch [15/104], Loss: 0.0145\n",
            "Epoch [93/200], Batch [20/104], Loss: 0.0045\n",
            "Epoch [93/200], Batch [25/104], Loss: 0.0070\n",
            "Epoch [93/200], Batch [30/104], Loss: 0.0056\n",
            "Epoch [93/200], Batch [35/104], Loss: 0.0081\n",
            "Epoch [93/200], Batch [40/104], Loss: 0.0041\n",
            "Epoch [93/200], Batch [45/104], Loss: 0.0189\n",
            "Epoch [93/200], Batch [50/104], Loss: 0.0082\n",
            "Epoch [93/200], Batch [55/104], Loss: 0.0186\n",
            "Epoch [93/200], Batch [60/104], Loss: 0.0024\n",
            "Epoch [93/200], Batch [65/104], Loss: 0.0022\n",
            "Epoch [93/200], Batch [70/104], Loss: 0.0077\n",
            "Epoch [93/200], Batch [75/104], Loss: 0.0063\n",
            "Epoch [93/200], Batch [80/104], Loss: 0.0009\n",
            "Epoch [93/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [93/200], Batch [90/104], Loss: 0.0030\n",
            "Epoch [93/200], Batch [95/104], Loss: 0.0095\n",
            "Epoch [93/200], Batch [100/104], Loss: 0.0040\n",
            "Epoch 93: Train Loss = 0.6722, Accuracy = 100.00%\n",
            "Epoch [94/200], Batch [5/104], Loss: 0.0037\n",
            "Epoch [94/200], Batch [10/104], Loss: 0.0069\n",
            "Epoch [94/200], Batch [15/104], Loss: 0.0064\n",
            "Epoch [94/200], Batch [20/104], Loss: 0.0026\n",
            "Epoch [94/200], Batch [25/104], Loss: 0.0028\n",
            "Epoch [94/200], Batch [30/104], Loss: 0.0036\n",
            "Epoch [94/200], Batch [35/104], Loss: 0.0017\n",
            "Epoch [94/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [94/200], Batch [45/104], Loss: 0.0047\n",
            "Epoch [94/200], Batch [50/104], Loss: 0.0049\n",
            "Epoch [94/200], Batch [55/104], Loss: 0.0087\n",
            "Epoch [94/200], Batch [60/104], Loss: 0.0046\n",
            "Epoch [94/200], Batch [65/104], Loss: 0.0070\n",
            "Epoch [94/200], Batch [70/104], Loss: 0.0036\n",
            "Epoch [94/200], Batch [75/104], Loss: 0.0114\n",
            "Epoch [94/200], Batch [80/104], Loss: 0.0058\n",
            "Epoch [94/200], Batch [85/104], Loss: 0.0045\n",
            "Epoch [94/200], Batch [90/104], Loss: 0.0019\n",
            "Epoch [94/200], Batch [95/104], Loss: 0.0023\n",
            "Epoch [94/200], Batch [100/104], Loss: 0.0114\n",
            "Epoch 94: Train Loss = 0.6255, Accuracy = 100.00%\n",
            "Epoch [95/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [95/200], Batch [10/104], Loss: 0.0229\n",
            "Epoch [95/200], Batch [15/104], Loss: 0.0047\n",
            "Epoch [95/200], Batch [20/104], Loss: 0.0034\n",
            "Epoch [95/200], Batch [25/104], Loss: 0.0042\n",
            "Epoch [95/200], Batch [30/104], Loss: 0.0070\n",
            "Epoch [95/200], Batch [35/104], Loss: 0.0010\n",
            "Epoch [95/200], Batch [40/104], Loss: 0.0059\n",
            "Epoch [95/200], Batch [45/104], Loss: 0.0032\n",
            "Epoch [95/200], Batch [50/104], Loss: 0.0076\n",
            "Epoch [95/200], Batch [55/104], Loss: 0.0159\n",
            "Epoch [95/200], Batch [60/104], Loss: 0.0042\n",
            "Epoch [95/200], Batch [65/104], Loss: 0.0052\n",
            "Epoch [95/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [95/200], Batch [75/104], Loss: 0.0032\n",
            "Epoch [95/200], Batch [80/104], Loss: 0.0038\n",
            "Epoch [95/200], Batch [85/104], Loss: 0.0033\n",
            "Epoch [95/200], Batch [90/104], Loss: 0.0044\n",
            "Epoch [95/200], Batch [95/104], Loss: 0.0095\n",
            "Epoch [95/200], Batch [100/104], Loss: 0.0024\n",
            "Epoch 95: Train Loss = 0.6285, Accuracy = 100.00%\n",
            "Epoch [96/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [96/200], Batch [10/104], Loss: 0.0125\n",
            "Epoch [96/200], Batch [15/104], Loss: 0.0118\n",
            "Epoch [96/200], Batch [20/104], Loss: 0.0073\n",
            "Epoch [96/200], Batch [25/104], Loss: 0.0020\n",
            "Epoch [96/200], Batch [30/104], Loss: 0.0061\n",
            "Epoch [96/200], Batch [35/104], Loss: 0.0039\n",
            "Epoch [96/200], Batch [40/104], Loss: 0.0045\n",
            "Epoch [96/200], Batch [45/104], Loss: 0.0028\n",
            "Epoch [96/200], Batch [50/104], Loss: 0.0047\n",
            "Epoch [96/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [96/200], Batch [60/104], Loss: 0.0031\n",
            "Epoch [96/200], Batch [65/104], Loss: 0.0086\n",
            "Epoch [96/200], Batch [70/104], Loss: 0.0029\n",
            "Epoch [96/200], Batch [75/104], Loss: 0.0036\n",
            "Epoch [96/200], Batch [80/104], Loss: 0.0025\n",
            "Epoch [96/200], Batch [85/104], Loss: 0.0049\n",
            "Epoch [96/200], Batch [90/104], Loss: 0.0064\n",
            "Epoch [96/200], Batch [95/104], Loss: 0.0046\n",
            "Epoch [96/200], Batch [100/104], Loss: 0.0037\n",
            "Epoch 96: Train Loss = 0.5810, Accuracy = 100.00%\n",
            "Epoch [97/200], Batch [5/104], Loss: 0.0012\n",
            "Epoch [97/200], Batch [10/104], Loss: 0.0069\n",
            "Epoch [97/200], Batch [15/104], Loss: 0.0054\n",
            "Epoch [97/200], Batch [20/104], Loss: 0.0048\n",
            "Epoch [97/200], Batch [25/104], Loss: 0.0060\n",
            "Epoch [97/200], Batch [30/104], Loss: 0.0108\n",
            "Epoch [97/200], Batch [35/104], Loss: 0.0058\n",
            "Epoch [97/200], Batch [40/104], Loss: 0.0059\n",
            "Epoch [97/200], Batch [45/104], Loss: 0.0035\n",
            "Epoch [97/200], Batch [50/104], Loss: 0.0067\n",
            "Epoch [97/200], Batch [55/104], Loss: 0.0033\n",
            "Epoch [97/200], Batch [60/104], Loss: 0.0174\n",
            "Epoch [97/200], Batch [65/104], Loss: 0.0124\n",
            "Epoch [97/200], Batch [70/104], Loss: 0.0064\n",
            "Epoch [97/200], Batch [75/104], Loss: 0.0080\n",
            "Epoch [97/200], Batch [80/104], Loss: 0.0076\n",
            "Epoch [97/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [97/200], Batch [90/104], Loss: 0.0046\n",
            "Epoch [97/200], Batch [95/104], Loss: 0.0069\n",
            "Epoch [97/200], Batch [100/104], Loss: 0.0172\n",
            "Epoch 97: Train Loss = 0.6438, Accuracy = 100.00%\n",
            "Epoch [98/200], Batch [5/104], Loss: 0.0070\n",
            "Epoch [98/200], Batch [10/104], Loss: 0.0111\n",
            "Epoch [98/200], Batch [15/104], Loss: 0.0052\n",
            "Epoch [98/200], Batch [20/104], Loss: 0.0043\n",
            "Epoch [98/200], Batch [25/104], Loss: 0.0016\n",
            "Epoch [98/200], Batch [30/104], Loss: 0.0036\n",
            "Epoch [98/200], Batch [35/104], Loss: 0.0033\n",
            "Epoch [98/200], Batch [40/104], Loss: 0.0093\n",
            "Epoch [98/200], Batch [45/104], Loss: 0.0053\n",
            "Epoch [98/200], Batch [50/104], Loss: 0.0089\n",
            "Epoch [98/200], Batch [55/104], Loss: 0.0095\n",
            "Epoch [98/200], Batch [60/104], Loss: 0.0012\n",
            "Epoch [98/200], Batch [65/104], Loss: 0.0100\n",
            "Epoch [98/200], Batch [70/104], Loss: 0.0047\n",
            "Epoch [98/200], Batch [75/104], Loss: 0.0038\n",
            "Epoch [98/200], Batch [80/104], Loss: 0.0133\n",
            "Epoch [98/200], Batch [85/104], Loss: 0.0011\n",
            "Epoch [98/200], Batch [90/104], Loss: 0.0048\n",
            "Epoch [98/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [98/200], Batch [100/104], Loss: 0.0053\n",
            "Epoch 98: Train Loss = 0.5999, Accuracy = 100.00%\n",
            "Epoch [99/200], Batch [5/104], Loss: 0.0021\n",
            "Epoch [99/200], Batch [10/104], Loss: 0.0102\n",
            "Epoch [99/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [99/200], Batch [20/104], Loss: 0.0046\n",
            "Epoch [99/200], Batch [25/104], Loss: 0.0036\n",
            "Epoch [99/200], Batch [30/104], Loss: 0.0022\n",
            "Epoch [99/200], Batch [35/104], Loss: 0.0051\n",
            "Epoch [99/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [99/200], Batch [45/104], Loss: 0.0043\n",
            "Epoch [99/200], Batch [50/104], Loss: 0.0067\n",
            "Epoch [99/200], Batch [55/104], Loss: 0.0122\n",
            "Epoch [99/200], Batch [60/104], Loss: 0.0072\n",
            "Epoch [99/200], Batch [65/104], Loss: 0.0051\n",
            "Epoch [99/200], Batch [70/104], Loss: 0.0007\n",
            "Epoch [99/200], Batch [75/104], Loss: 0.0030\n",
            "Epoch [99/200], Batch [80/104], Loss: 0.0017\n",
            "Epoch [99/200], Batch [85/104], Loss: 0.0076\n",
            "Epoch [99/200], Batch [90/104], Loss: 0.0191\n",
            "Epoch [99/200], Batch [95/104], Loss: 0.0017\n",
            "Epoch [99/200], Batch [100/104], Loss: 0.0027\n",
            "Epoch 99: Train Loss = 0.5829, Accuracy = 100.00%\n",
            "Epoch [100/200], Batch [5/104], Loss: 0.0060\n",
            "Epoch [100/200], Batch [10/104], Loss: 0.0025\n",
            "Epoch [100/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [100/200], Batch [20/104], Loss: 0.0070\n",
            "Epoch [100/200], Batch [25/104], Loss: 0.0046\n",
            "Epoch [100/200], Batch [30/104], Loss: 0.0043\n",
            "Epoch [100/200], Batch [35/104], Loss: 0.0015\n",
            "Epoch [100/200], Batch [40/104], Loss: 0.0057\n",
            "Epoch [100/200], Batch [45/104], Loss: 0.0048\n",
            "Epoch [100/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [100/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [100/200], Batch [60/104], Loss: 0.0021\n",
            "Epoch [100/200], Batch [65/104], Loss: 0.0041\n",
            "Epoch [100/200], Batch [70/104], Loss: 0.0034\n",
            "Epoch [100/200], Batch [75/104], Loss: 0.0019\n",
            "Epoch [100/200], Batch [80/104], Loss: 0.0079\n",
            "Epoch [100/200], Batch [85/104], Loss: 0.0055\n",
            "Epoch [100/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [100/200], Batch [95/104], Loss: 0.0021\n",
            "Epoch [100/200], Batch [100/104], Loss: 0.0049\n",
            "Epoch 100: Train Loss = 0.5573, Accuracy = 100.00%\n",
            "Epoch [101/200], Batch [5/104], Loss: 0.0102\n",
            "Epoch [101/200], Batch [10/104], Loss: 0.0030\n",
            "Epoch [101/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [101/200], Batch [20/104], Loss: 0.0070\n",
            "Epoch [101/200], Batch [25/104], Loss: 0.0065\n",
            "Epoch [101/200], Batch [30/104], Loss: 0.0045\n",
            "Epoch [101/200], Batch [35/104], Loss: 0.0033\n",
            "Epoch [101/200], Batch [40/104], Loss: 0.0112\n",
            "Epoch [101/200], Batch [45/104], Loss: 0.0042\n",
            "Epoch [101/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [101/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [101/200], Batch [60/104], Loss: 0.0052\n",
            "Epoch [101/200], Batch [65/104], Loss: 0.0042\n",
            "Epoch [101/200], Batch [70/104], Loss: 0.0058\n",
            "Epoch [101/200], Batch [75/104], Loss: 0.0036\n",
            "Epoch [101/200], Batch [80/104], Loss: 0.0096\n",
            "Epoch [101/200], Batch [85/104], Loss: 0.0077\n",
            "Epoch [101/200], Batch [90/104], Loss: 0.0059\n",
            "Epoch [101/200], Batch [95/104], Loss: 0.0050\n",
            "Epoch [101/200], Batch [100/104], Loss: 0.0054\n",
            "Epoch 101: Train Loss = 0.5731, Accuracy = 100.00%\n",
            "Epoch [102/200], Batch [5/104], Loss: 0.0033\n",
            "Epoch [102/200], Batch [10/104], Loss: 0.0035\n",
            "Epoch [102/200], Batch [15/104], Loss: 0.0103\n",
            "Epoch [102/200], Batch [20/104], Loss: 0.0024\n",
            "Epoch [102/200], Batch [25/104], Loss: 0.0029\n",
            "Epoch [102/200], Batch [30/104], Loss: 0.0063\n",
            "Epoch [102/200], Batch [35/104], Loss: 0.0093\n",
            "Epoch [102/200], Batch [40/104], Loss: 0.0043\n",
            "Epoch [102/200], Batch [45/104], Loss: 0.0067\n",
            "Epoch [102/200], Batch [50/104], Loss: 0.0071\n",
            "Epoch [102/200], Batch [55/104], Loss: 0.0018\n",
            "Epoch [102/200], Batch [60/104], Loss: 0.0043\n",
            "Epoch [102/200], Batch [65/104], Loss: 0.0029\n",
            "Epoch [102/200], Batch [70/104], Loss: 0.0138\n",
            "Epoch [102/200], Batch [75/104], Loss: 0.0066\n",
            "Epoch [102/200], Batch [80/104], Loss: 0.0082\n",
            "Epoch [102/200], Batch [85/104], Loss: 0.0050\n",
            "Epoch [102/200], Batch [90/104], Loss: 0.0027\n",
            "Epoch [102/200], Batch [95/104], Loss: 0.0024\n",
            "Epoch [102/200], Batch [100/104], Loss: 0.0042\n",
            "Epoch 102: Train Loss = 0.5686, Accuracy = 100.00%\n",
            "Epoch [103/200], Batch [5/104], Loss: 0.0120\n",
            "Epoch [103/200], Batch [10/104], Loss: 0.0007\n",
            "Epoch [103/200], Batch [15/104], Loss: 0.0057\n",
            "Epoch [103/200], Batch [20/104], Loss: 0.0057\n",
            "Epoch [103/200], Batch [25/104], Loss: 0.0124\n",
            "Epoch [103/200], Batch [30/104], Loss: 0.0069\n",
            "Epoch [103/200], Batch [35/104], Loss: 0.0061\n",
            "Epoch [103/200], Batch [40/104], Loss: 0.0014\n",
            "Epoch [103/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [103/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [103/200], Batch [55/104], Loss: 0.0152\n",
            "Epoch [103/200], Batch [60/104], Loss: 0.0022\n",
            "Epoch [103/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [103/200], Batch [70/104], Loss: 0.0023\n",
            "Epoch [103/200], Batch [75/104], Loss: 0.0055\n",
            "Epoch [103/200], Batch [80/104], Loss: 0.0044\n",
            "Epoch [103/200], Batch [85/104], Loss: 0.0081\n",
            "Epoch [103/200], Batch [90/104], Loss: 0.0053\n",
            "Epoch [103/200], Batch [95/104], Loss: 0.0050\n",
            "Epoch [103/200], Batch [100/104], Loss: 0.0044\n",
            "Epoch 103: Train Loss = 0.5617, Accuracy = 100.00%\n",
            "Epoch [104/200], Batch [5/104], Loss: 0.0075\n",
            "Epoch [104/200], Batch [10/104], Loss: 0.0059\n",
            "Epoch [104/200], Batch [15/104], Loss: 0.0035\n",
            "Epoch [104/200], Batch [20/104], Loss: 0.0057\n",
            "Epoch [104/200], Batch [25/104], Loss: 0.0032\n",
            "Epoch [104/200], Batch [30/104], Loss: 0.0025\n",
            "Epoch [104/200], Batch [35/104], Loss: 0.0059\n",
            "Epoch [104/200], Batch [40/104], Loss: 0.0036\n",
            "Epoch [104/200], Batch [45/104], Loss: 0.0019\n",
            "Epoch [104/200], Batch [50/104], Loss: 0.0014\n",
            "Epoch [104/200], Batch [55/104], Loss: 0.0025\n",
            "Epoch [104/200], Batch [60/104], Loss: 0.0037\n",
            "Epoch [104/200], Batch [65/104], Loss: 0.0042\n",
            "Epoch [104/200], Batch [70/104], Loss: 0.0047\n",
            "Epoch [104/200], Batch [75/104], Loss: 0.0087\n",
            "Epoch [104/200], Batch [80/104], Loss: 0.0065\n",
            "Epoch [104/200], Batch [85/104], Loss: 0.0048\n",
            "Epoch [104/200], Batch [90/104], Loss: 0.0068\n",
            "Epoch [104/200], Batch [95/104], Loss: 0.0076\n",
            "Epoch [104/200], Batch [100/104], Loss: 0.0016\n",
            "Epoch 104: Train Loss = 0.5532, Accuracy = 100.00%\n",
            "Epoch [105/200], Batch [5/104], Loss: 0.0047\n",
            "Epoch [105/200], Batch [10/104], Loss: 0.0058\n",
            "Epoch [105/200], Batch [15/104], Loss: 0.0020\n",
            "Epoch [105/200], Batch [20/104], Loss: 0.0056\n",
            "Epoch [105/200], Batch [25/104], Loss: 0.0029\n",
            "Epoch [105/200], Batch [30/104], Loss: 0.0042\n",
            "Epoch [105/200], Batch [35/104], Loss: 0.0027\n",
            "Epoch [105/200], Batch [40/104], Loss: 0.0053\n",
            "Epoch [105/200], Batch [45/104], Loss: 0.0055\n",
            "Epoch [105/200], Batch [50/104], Loss: 0.0039\n",
            "Epoch [105/200], Batch [55/104], Loss: 0.0034\n",
            "Epoch [105/200], Batch [60/104], Loss: 0.0056\n",
            "Epoch [105/200], Batch [65/104], Loss: 0.0053\n",
            "Epoch [105/200], Batch [70/104], Loss: 0.0091\n",
            "Epoch [105/200], Batch [75/104], Loss: 0.0055\n",
            "Epoch [105/200], Batch [80/104], Loss: 0.0052\n",
            "Epoch [105/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [105/200], Batch [90/104], Loss: 0.0070\n",
            "Epoch [105/200], Batch [95/104], Loss: 0.0035\n",
            "Epoch [105/200], Batch [100/104], Loss: 0.0056\n",
            "Epoch 105: Train Loss = 0.5323, Accuracy = 100.00%\n",
            "Epoch [106/200], Batch [5/104], Loss: 0.0059\n",
            "Epoch [106/200], Batch [10/104], Loss: 0.0101\n",
            "Epoch [106/200], Batch [15/104], Loss: 0.0050\n",
            "Epoch [106/200], Batch [20/104], Loss: 0.0091\n",
            "Epoch [106/200], Batch [25/104], Loss: 0.0067\n",
            "Epoch [106/200], Batch [30/104], Loss: 0.0018\n",
            "Epoch [106/200], Batch [35/104], Loss: 0.0041\n",
            "Epoch [106/200], Batch [40/104], Loss: 0.0080\n",
            "Epoch [106/200], Batch [45/104], Loss: 0.0050\n",
            "Epoch [106/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [106/200], Batch [55/104], Loss: 0.0052\n",
            "Epoch [106/200], Batch [60/104], Loss: 0.0042\n",
            "Epoch [106/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [106/200], Batch [70/104], Loss: 0.0071\n",
            "Epoch [106/200], Batch [75/104], Loss: 0.0066\n",
            "Epoch [106/200], Batch [80/104], Loss: 0.0064\n",
            "Epoch [106/200], Batch [85/104], Loss: 0.0046\n",
            "Epoch [106/200], Batch [90/104], Loss: 0.0071\n",
            "Epoch [106/200], Batch [95/104], Loss: 0.0030\n",
            "Epoch [106/200], Batch [100/104], Loss: 0.0065\n",
            "Epoch 106: Train Loss = 0.5437, Accuracy = 100.00%\n",
            "Epoch [107/200], Batch [5/104], Loss: 0.0032\n",
            "Epoch [107/200], Batch [10/104], Loss: 0.0083\n",
            "Epoch [107/200], Batch [15/104], Loss: 0.0051\n",
            "Epoch [107/200], Batch [20/104], Loss: 0.0037\n",
            "Epoch [107/200], Batch [25/104], Loss: 0.0061\n",
            "Epoch [107/200], Batch [30/104], Loss: 0.0120\n",
            "Epoch [107/200], Batch [35/104], Loss: 0.0014\n",
            "Epoch [107/200], Batch [40/104], Loss: 0.0035\n",
            "Epoch [107/200], Batch [45/104], Loss: 0.0023\n",
            "Epoch [107/200], Batch [50/104], Loss: 0.0028\n",
            "Epoch [107/200], Batch [55/104], Loss: 0.0050\n",
            "Epoch [107/200], Batch [60/104], Loss: 0.0096\n",
            "Epoch [107/200], Batch [65/104], Loss: 0.0085\n",
            "Epoch [107/200], Batch [70/104], Loss: 0.0049\n",
            "Epoch [107/200], Batch [75/104], Loss: 0.0013\n",
            "Epoch [107/200], Batch [80/104], Loss: 0.0035\n",
            "Epoch [107/200], Batch [85/104], Loss: 0.0052\n",
            "Epoch [107/200], Batch [90/104], Loss: 0.0056\n",
            "Epoch [107/200], Batch [95/104], Loss: 0.0031\n",
            "Epoch [107/200], Batch [100/104], Loss: 0.0059\n",
            "Epoch 107: Train Loss = 0.5145, Accuracy = 100.00%\n",
            "Epoch [108/200], Batch [5/104], Loss: 0.0049\n",
            "Epoch [108/200], Batch [10/104], Loss: 0.0050\n",
            "Epoch [108/200], Batch [15/104], Loss: 0.0055\n",
            "Epoch [108/200], Batch [20/104], Loss: 0.0057\n",
            "Epoch [108/200], Batch [25/104], Loss: 0.0032\n",
            "Epoch [108/200], Batch [30/104], Loss: 0.0022\n",
            "Epoch [108/200], Batch [35/104], Loss: 0.0033\n",
            "Epoch [108/200], Batch [40/104], Loss: 0.0024\n",
            "Epoch [108/200], Batch [45/104], Loss: 0.0039\n",
            "Epoch [108/200], Batch [50/104], Loss: 0.0034\n",
            "Epoch [108/200], Batch [55/104], Loss: 0.0028\n",
            "Epoch [108/200], Batch [60/104], Loss: 0.0065\n",
            "Epoch [108/200], Batch [65/104], Loss: 0.0053\n",
            "Epoch [108/200], Batch [70/104], Loss: 0.0042\n",
            "Epoch [108/200], Batch [75/104], Loss: 0.0112\n",
            "Epoch [108/200], Batch [80/104], Loss: 0.0264\n",
            "Epoch [108/200], Batch [85/104], Loss: 0.0027\n",
            "Epoch [108/200], Batch [90/104], Loss: 0.0052\n",
            "Epoch [108/200], Batch [95/104], Loss: 0.0067\n",
            "Epoch [108/200], Batch [100/104], Loss: 0.0084\n",
            "Epoch 108: Train Loss = 0.5503, Accuracy = 100.00%\n",
            "Epoch [109/200], Batch [5/104], Loss: 0.0027\n",
            "Epoch [109/200], Batch [10/104], Loss: 0.0067\n",
            "Epoch [109/200], Batch [15/104], Loss: 0.0021\n",
            "Epoch [109/200], Batch [20/104], Loss: 0.0056\n",
            "Epoch [109/200], Batch [25/104], Loss: 0.0035\n",
            "Epoch [109/200], Batch [30/104], Loss: 0.0069\n",
            "Epoch [109/200], Batch [35/104], Loss: 0.0044\n",
            "Epoch [109/200], Batch [40/104], Loss: 0.0042\n",
            "Epoch [109/200], Batch [45/104], Loss: 0.0044\n",
            "Epoch [109/200], Batch [50/104], Loss: 0.0080\n",
            "Epoch [109/200], Batch [55/104], Loss: 0.0015\n",
            "Epoch [109/200], Batch [60/104], Loss: 0.0067\n",
            "Epoch [109/200], Batch [65/104], Loss: 0.0087\n",
            "Epoch [109/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [109/200], Batch [75/104], Loss: 0.0082\n",
            "Epoch [109/200], Batch [80/104], Loss: 0.0034\n",
            "Epoch [109/200], Batch [85/104], Loss: 0.0066\n",
            "Epoch [109/200], Batch [90/104], Loss: 0.0037\n",
            "Epoch [109/200], Batch [95/104], Loss: 0.0177\n",
            "Epoch [109/200], Batch [100/104], Loss: 0.0093\n",
            "Epoch 109: Train Loss = 0.5259, Accuracy = 100.00%\n",
            "Epoch [110/200], Batch [5/104], Loss: 0.0045\n",
            "Epoch [110/200], Batch [10/104], Loss: 0.0039\n",
            "Epoch [110/200], Batch [15/104], Loss: 0.0014\n",
            "Epoch [110/200], Batch [20/104], Loss: 0.0141\n",
            "Epoch [110/200], Batch [25/104], Loss: 0.0035\n",
            "Epoch [110/200], Batch [30/104], Loss: 0.0110\n",
            "Epoch [110/200], Batch [35/104], Loss: 0.0028\n",
            "Epoch [110/200], Batch [40/104], Loss: 0.0032\n",
            "Epoch [110/200], Batch [45/104], Loss: 0.0041\n",
            "Epoch [110/200], Batch [50/104], Loss: 0.0024\n",
            "Epoch [110/200], Batch [55/104], Loss: 0.0021\n",
            "Epoch [110/200], Batch [60/104], Loss: 0.0058\n",
            "Epoch [110/200], Batch [65/104], Loss: 0.0044\n",
            "Epoch [110/200], Batch [70/104], Loss: 0.0045\n",
            "Epoch [110/200], Batch [75/104], Loss: 0.0052\n",
            "Epoch [110/200], Batch [80/104], Loss: 0.0064\n",
            "Epoch [110/200], Batch [85/104], Loss: 0.0091\n",
            "Epoch [110/200], Batch [90/104], Loss: 0.0015\n",
            "Epoch [110/200], Batch [95/104], Loss: 0.0109\n",
            "Epoch [110/200], Batch [100/104], Loss: 0.0026\n",
            "Epoch 110: Train Loss = 0.5048, Accuracy = 100.00%\n",
            "Epoch [111/200], Batch [5/104], Loss: 0.0039\n",
            "Epoch [111/200], Batch [10/104], Loss: 0.0048\n",
            "Epoch [111/200], Batch [15/104], Loss: 0.0030\n",
            "Epoch [111/200], Batch [20/104], Loss: 0.0042\n",
            "Epoch [111/200], Batch [25/104], Loss: 0.0047\n",
            "Epoch [111/200], Batch [30/104], Loss: 0.0035\n",
            "Epoch [111/200], Batch [35/104], Loss: 0.0018\n",
            "Epoch [111/200], Batch [40/104], Loss: 0.0082\n",
            "Epoch [111/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [111/200], Batch [50/104], Loss: 0.0091\n",
            "Epoch [111/200], Batch [55/104], Loss: 0.0029\n",
            "Epoch [111/200], Batch [60/104], Loss: 0.0054\n",
            "Epoch [111/200], Batch [65/104], Loss: 0.0064\n",
            "Epoch [111/200], Batch [70/104], Loss: 0.0028\n",
            "Epoch [111/200], Batch [75/104], Loss: 0.0047\n",
            "Epoch [111/200], Batch [80/104], Loss: 0.0045\n",
            "Epoch [111/200], Batch [85/104], Loss: 0.0113\n",
            "Epoch [111/200], Batch [90/104], Loss: 0.0050\n",
            "Epoch [111/200], Batch [95/104], Loss: 0.0067\n",
            "Epoch [111/200], Batch [100/104], Loss: 0.0021\n",
            "Epoch 111: Train Loss = 0.5035, Accuracy = 100.00%\n",
            "Epoch [112/200], Batch [5/104], Loss: 0.0060\n",
            "Epoch [112/200], Batch [10/104], Loss: 0.0053\n",
            "Epoch [112/200], Batch [15/104], Loss: 0.0073\n",
            "Epoch [112/200], Batch [20/104], Loss: 0.0027\n",
            "Epoch [112/200], Batch [25/104], Loss: 0.0057\n",
            "Epoch [112/200], Batch [30/104], Loss: 0.0076\n",
            "Epoch [112/200], Batch [35/104], Loss: 0.0033\n",
            "Epoch [112/200], Batch [40/104], Loss: 0.0020\n",
            "Epoch [112/200], Batch [45/104], Loss: 0.0063\n",
            "Epoch [112/200], Batch [50/104], Loss: 0.0064\n",
            "Epoch [112/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [112/200], Batch [60/104], Loss: 0.0032\n",
            "Epoch [112/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [112/200], Batch [70/104], Loss: 0.0052\n",
            "Epoch [112/200], Batch [75/104], Loss: 0.0031\n",
            "Epoch [112/200], Batch [80/104], Loss: 0.0034\n",
            "Epoch [112/200], Batch [85/104], Loss: 0.0050\n",
            "Epoch [112/200], Batch [90/104], Loss: 0.0023\n",
            "Epoch [112/200], Batch [95/104], Loss: 0.0043\n",
            "Epoch [112/200], Batch [100/104], Loss: 0.0122\n",
            "Epoch 112: Train Loss = 0.5135, Accuracy = 100.00%\n",
            "Epoch [113/200], Batch [5/104], Loss: 0.0019\n",
            "Epoch [113/200], Batch [10/104], Loss: 0.0081\n",
            "Epoch [113/200], Batch [15/104], Loss: 0.0039\n",
            "Epoch [113/200], Batch [20/104], Loss: 0.0034\n",
            "Epoch [113/200], Batch [25/104], Loss: 0.0123\n",
            "Epoch [113/200], Batch [30/104], Loss: 0.0034\n",
            "Epoch [113/200], Batch [35/104], Loss: 0.0089\n",
            "Epoch [113/200], Batch [40/104], Loss: 0.0031\n",
            "Epoch [113/200], Batch [45/104], Loss: 0.0033\n",
            "Epoch [113/200], Batch [50/104], Loss: 0.0051\n",
            "Epoch [113/200], Batch [55/104], Loss: 0.0023\n",
            "Epoch [113/200], Batch [60/104], Loss: 0.0074\n",
            "Epoch [113/200], Batch [65/104], Loss: 0.0011\n",
            "Epoch [113/200], Batch [70/104], Loss: 0.0042\n",
            "Epoch [113/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [113/200], Batch [80/104], Loss: 0.0083\n",
            "Epoch [113/200], Batch [85/104], Loss: 0.0025\n",
            "Epoch [113/200], Batch [90/104], Loss: 0.0035\n",
            "Epoch [113/200], Batch [95/104], Loss: 0.0016\n",
            "Epoch [113/200], Batch [100/104], Loss: 0.0063\n",
            "Epoch 113: Train Loss = 0.4967, Accuracy = 100.00%\n",
            "Epoch [114/200], Batch [5/104], Loss: 0.0064\n",
            "Epoch [114/200], Batch [10/104], Loss: 0.0055\n",
            "Epoch [114/200], Batch [15/104], Loss: 0.0052\n",
            "Epoch [114/200], Batch [20/104], Loss: 0.0015\n",
            "Epoch [114/200], Batch [25/104], Loss: 0.0050\n",
            "Epoch [114/200], Batch [30/104], Loss: 0.0036\n",
            "Epoch [114/200], Batch [35/104], Loss: 0.0065\n",
            "Epoch [114/200], Batch [40/104], Loss: 0.0017\n",
            "Epoch [114/200], Batch [45/104], Loss: 0.0065\n",
            "Epoch [114/200], Batch [50/104], Loss: 0.0066\n",
            "Epoch [114/200], Batch [55/104], Loss: 0.0046\n",
            "Epoch [114/200], Batch [60/104], Loss: 0.0061\n",
            "Epoch [114/200], Batch [65/104], Loss: 0.0022\n",
            "Epoch [114/200], Batch [70/104], Loss: 0.0018\n",
            "Epoch [114/200], Batch [75/104], Loss: 0.0042\n",
            "Epoch [114/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [114/200], Batch [85/104], Loss: 0.0023\n",
            "Epoch [114/200], Batch [90/104], Loss: 0.0073\n",
            "Epoch [114/200], Batch [95/104], Loss: 0.0011\n",
            "Epoch [114/200], Batch [100/104], Loss: 0.0050\n",
            "Epoch 114: Train Loss = 0.4981, Accuracy = 100.00%\n",
            "Epoch [115/200], Batch [5/104], Loss: 0.0057\n",
            "Epoch [115/200], Batch [10/104], Loss: 0.0060\n",
            "Epoch [115/200], Batch [15/104], Loss: 0.0025\n",
            "Epoch [115/200], Batch [20/104], Loss: 0.0015\n",
            "Epoch [115/200], Batch [25/104], Loss: 0.0038\n",
            "Epoch [115/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [115/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [115/200], Batch [40/104], Loss: 0.0055\n",
            "Epoch [115/200], Batch [45/104], Loss: 0.0013\n",
            "Epoch [115/200], Batch [50/104], Loss: 0.0099\n",
            "Epoch [115/200], Batch [55/104], Loss: 0.0036\n",
            "Epoch [115/200], Batch [60/104], Loss: 0.0065\n",
            "Epoch [115/200], Batch [65/104], Loss: 0.0103\n",
            "Epoch [115/200], Batch [70/104], Loss: 0.0106\n",
            "Epoch [115/200], Batch [75/104], Loss: 0.0052\n",
            "Epoch [115/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [115/200], Batch [85/104], Loss: 0.0043\n",
            "Epoch [115/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [115/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [115/200], Batch [100/104], Loss: 0.0073\n",
            "Epoch 115: Train Loss = 0.4899, Accuracy = 100.00%\n",
            "Epoch [116/200], Batch [5/104], Loss: 0.0022\n",
            "Epoch [116/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [116/200], Batch [15/104], Loss: 0.0071\n",
            "Epoch [116/200], Batch [20/104], Loss: 0.0067\n",
            "Epoch [116/200], Batch [25/104], Loss: 0.0017\n",
            "Epoch [116/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [116/200], Batch [35/104], Loss: 0.0062\n",
            "Epoch [116/200], Batch [40/104], Loss: 0.0057\n",
            "Epoch [116/200], Batch [45/104], Loss: 0.0072\n",
            "Epoch [116/200], Batch [50/104], Loss: 0.0016\n",
            "Epoch [116/200], Batch [55/104], Loss: 0.0073\n",
            "Epoch [116/200], Batch [60/104], Loss: 0.0053\n",
            "Epoch [116/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [116/200], Batch [70/104], Loss: 0.0037\n",
            "Epoch [116/200], Batch [75/104], Loss: 0.0100\n",
            "Epoch [116/200], Batch [80/104], Loss: 0.0055\n",
            "Epoch [116/200], Batch [85/104], Loss: 0.0042\n",
            "Epoch [116/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [116/200], Batch [95/104], Loss: 0.0058\n",
            "Epoch [116/200], Batch [100/104], Loss: 0.0047\n",
            "Epoch 116: Train Loss = 0.4911, Accuracy = 100.00%\n",
            "Epoch [117/200], Batch [5/104], Loss: 0.0030\n",
            "Epoch [117/200], Batch [10/104], Loss: 0.0084\n",
            "Epoch [117/200], Batch [15/104], Loss: 0.0024\n",
            "Epoch [117/200], Batch [20/104], Loss: 0.0025\n",
            "Epoch [117/200], Batch [25/104], Loss: 0.0023\n",
            "Epoch [117/200], Batch [30/104], Loss: 0.0026\n",
            "Epoch [117/200], Batch [35/104], Loss: 0.0091\n",
            "Epoch [117/200], Batch [40/104], Loss: 0.0065\n",
            "Epoch [117/200], Batch [45/104], Loss: 0.0027\n",
            "Epoch [117/200], Batch [50/104], Loss: 0.0066\n",
            "Epoch [117/200], Batch [55/104], Loss: 0.0037\n",
            "Epoch [117/200], Batch [60/104], Loss: 0.0057\n",
            "Epoch [117/200], Batch [65/104], Loss: 0.0059\n",
            "Epoch [117/200], Batch [70/104], Loss: 0.0120\n",
            "Epoch [117/200], Batch [75/104], Loss: 0.0036\n",
            "Epoch [117/200], Batch [80/104], Loss: 0.0032\n",
            "Epoch [117/200], Batch [85/104], Loss: 0.0064\n",
            "Epoch [117/200], Batch [90/104], Loss: 0.0011\n",
            "Epoch [117/200], Batch [95/104], Loss: 0.0114\n",
            "Epoch [117/200], Batch [100/104], Loss: 0.0021\n",
            "Epoch 117: Train Loss = 0.4832, Accuracy = 100.00%\n",
            "Epoch [118/200], Batch [5/104], Loss: 0.0015\n",
            "Epoch [118/200], Batch [10/104], Loss: 0.0087\n",
            "Epoch [118/200], Batch [15/104], Loss: 0.0077\n",
            "Epoch [118/200], Batch [20/104], Loss: 0.0055\n",
            "Epoch [118/200], Batch [25/104], Loss: 0.0071\n",
            "Epoch [118/200], Batch [30/104], Loss: 0.0044\n",
            "Epoch [118/200], Batch [35/104], Loss: 0.0041\n",
            "Epoch [118/200], Batch [40/104], Loss: 0.0009\n",
            "Epoch [118/200], Batch [45/104], Loss: 0.0019\n",
            "Epoch [118/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [118/200], Batch [55/104], Loss: 0.0093\n",
            "Epoch [118/200], Batch [60/104], Loss: 0.0025\n",
            "Epoch [118/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [118/200], Batch [70/104], Loss: 0.0042\n",
            "Epoch [118/200], Batch [75/104], Loss: 0.0025\n",
            "Epoch [118/200], Batch [80/104], Loss: 0.0058\n",
            "Epoch [118/200], Batch [85/104], Loss: 0.0091\n",
            "Epoch [118/200], Batch [90/104], Loss: 0.0075\n",
            "Epoch [118/200], Batch [95/104], Loss: 0.0081\n",
            "Epoch [118/200], Batch [100/104], Loss: 0.0027\n",
            "Epoch 118: Train Loss = 0.4725, Accuracy = 100.00%\n",
            "Epoch [119/200], Batch [5/104], Loss: 0.0059\n",
            "Epoch [119/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [119/200], Batch [15/104], Loss: 0.0094\n",
            "Epoch [119/200], Batch [20/104], Loss: 0.0030\n",
            "Epoch [119/200], Batch [25/104], Loss: 0.0037\n",
            "Epoch [119/200], Batch [30/104], Loss: 0.0037\n",
            "Epoch [119/200], Batch [35/104], Loss: 0.0054\n",
            "Epoch [119/200], Batch [40/104], Loss: 0.0126\n",
            "Epoch [119/200], Batch [45/104], Loss: 0.0074\n",
            "Epoch [119/200], Batch [50/104], Loss: 0.0058\n",
            "Epoch [119/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [119/200], Batch [60/104], Loss: 0.0031\n",
            "Epoch [119/200], Batch [65/104], Loss: 0.0017\n",
            "Epoch [119/200], Batch [70/104], Loss: 0.0060\n",
            "Epoch [119/200], Batch [75/104], Loss: 0.0076\n",
            "Epoch [119/200], Batch [80/104], Loss: 0.0053\n",
            "Epoch [119/200], Batch [85/104], Loss: 0.0059\n",
            "Epoch [119/200], Batch [90/104], Loss: 0.0034\n",
            "Epoch [119/200], Batch [95/104], Loss: 0.0039\n",
            "Epoch [119/200], Batch [100/104], Loss: 0.0019\n",
            "Epoch 119: Train Loss = 0.4840, Accuracy = 100.00%\n",
            "Epoch [120/200], Batch [5/104], Loss: 0.0025\n",
            "Epoch [120/200], Batch [10/104], Loss: 0.0025\n",
            "Epoch [120/200], Batch [15/104], Loss: 0.0065\n",
            "Epoch [120/200], Batch [20/104], Loss: 0.0052\n",
            "Epoch [120/200], Batch [25/104], Loss: 0.0020\n",
            "Epoch [120/200], Batch [30/104], Loss: 0.0057\n",
            "Epoch [120/200], Batch [35/104], Loss: 0.0056\n",
            "Epoch [120/200], Batch [40/104], Loss: 0.0129\n",
            "Epoch [120/200], Batch [45/104], Loss: 0.0167\n",
            "Epoch [120/200], Batch [50/104], Loss: 0.0017\n",
            "Epoch [120/200], Batch [55/104], Loss: 0.0045\n",
            "Epoch [120/200], Batch [60/104], Loss: 0.0038\n",
            "Epoch [120/200], Batch [65/104], Loss: 0.0062\n",
            "Epoch [120/200], Batch [70/104], Loss: 0.0055\n",
            "Epoch [120/200], Batch [75/104], Loss: 0.0033\n",
            "Epoch [120/200], Batch [80/104], Loss: 0.0044\n",
            "Epoch [120/200], Batch [85/104], Loss: 0.0074\n",
            "Epoch [120/200], Batch [90/104], Loss: 0.0052\n",
            "Epoch [120/200], Batch [95/104], Loss: 0.0028\n",
            "Epoch [120/200], Batch [100/104], Loss: 0.0048\n",
            "Epoch 120: Train Loss = 0.4766, Accuracy = 100.00%\n",
            "Epoch [121/200], Batch [5/104], Loss: 0.0020\n",
            "Epoch [121/200], Batch [10/104], Loss: 0.0048\n",
            "Epoch [121/200], Batch [15/104], Loss: 0.0117\n",
            "Epoch [121/200], Batch [20/104], Loss: 0.0012\n",
            "Epoch [121/200], Batch [25/104], Loss: 0.0083\n",
            "Epoch [121/200], Batch [30/104], Loss: 0.0086\n",
            "Epoch [121/200], Batch [35/104], Loss: 0.0104\n",
            "Epoch [121/200], Batch [40/104], Loss: 0.0038\n",
            "Epoch [121/200], Batch [45/104], Loss: 0.0039\n",
            "Epoch [121/200], Batch [50/104], Loss: 0.0037\n",
            "Epoch [121/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [121/200], Batch [60/104], Loss: 0.0055\n",
            "Epoch [121/200], Batch [65/104], Loss: 0.0018\n",
            "Epoch [121/200], Batch [70/104], Loss: 0.0055\n",
            "Epoch [121/200], Batch [75/104], Loss: 0.0056\n",
            "Epoch [121/200], Batch [80/104], Loss: 0.0055\n",
            "Epoch [121/200], Batch [85/104], Loss: 0.0051\n",
            "Epoch [121/200], Batch [90/104], Loss: 0.0030\n",
            "Epoch [121/200], Batch [95/104], Loss: 0.0032\n",
            "Epoch [121/200], Batch [100/104], Loss: 0.0014\n",
            "Epoch 121: Train Loss = 0.4736, Accuracy = 100.00%\n",
            "Epoch [122/200], Batch [5/104], Loss: 0.0085\n",
            "Epoch [122/200], Batch [10/104], Loss: 0.0008\n",
            "Epoch [122/200], Batch [15/104], Loss: 0.0060\n",
            "Epoch [122/200], Batch [20/104], Loss: 0.0026\n",
            "Epoch [122/200], Batch [25/104], Loss: 0.0091\n",
            "Epoch [122/200], Batch [30/104], Loss: 0.0052\n",
            "Epoch [122/200], Batch [35/104], Loss: 0.0017\n",
            "Epoch [122/200], Batch [40/104], Loss: 0.0055\n",
            "Epoch [122/200], Batch [45/104], Loss: 0.0063\n",
            "Epoch [122/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [122/200], Batch [55/104], Loss: 0.0080\n",
            "Epoch [122/200], Batch [60/104], Loss: 0.0039\n",
            "Epoch [122/200], Batch [65/104], Loss: 0.0035\n",
            "Epoch [122/200], Batch [70/104], Loss: 0.0016\n",
            "Epoch [122/200], Batch [75/104], Loss: 0.0025\n",
            "Epoch [122/200], Batch [80/104], Loss: 0.0018\n",
            "Epoch [122/200], Batch [85/104], Loss: 0.0027\n",
            "Epoch [122/200], Batch [90/104], Loss: 0.0021\n",
            "Epoch [122/200], Batch [95/104], Loss: 0.0064\n",
            "Epoch [122/200], Batch [100/104], Loss: 0.0094\n",
            "Epoch 122: Train Loss = 0.4685, Accuracy = 100.00%\n",
            "Epoch [123/200], Batch [5/104], Loss: 0.0026\n",
            "Epoch [123/200], Batch [10/104], Loss: 0.0065\n",
            "Epoch [123/200], Batch [15/104], Loss: 0.0017\n",
            "Epoch [123/200], Batch [20/104], Loss: 0.0027\n",
            "Epoch [123/200], Batch [25/104], Loss: 0.0030\n",
            "Epoch [123/200], Batch [30/104], Loss: 0.0033\n",
            "Epoch [123/200], Batch [35/104], Loss: 0.0067\n",
            "Epoch [123/200], Batch [40/104], Loss: 0.0125\n",
            "Epoch [123/200], Batch [45/104], Loss: 0.0053\n",
            "Epoch [123/200], Batch [50/104], Loss: 0.0051\n",
            "Epoch [123/200], Batch [55/104], Loss: 0.0026\n",
            "Epoch [123/200], Batch [60/104], Loss: 0.0024\n",
            "Epoch [123/200], Batch [65/104], Loss: 0.0028\n",
            "Epoch [123/200], Batch [70/104], Loss: 0.0056\n",
            "Epoch [123/200], Batch [75/104], Loss: 0.0033\n",
            "Epoch [123/200], Batch [80/104], Loss: 0.0062\n",
            "Epoch [123/200], Batch [85/104], Loss: 0.0057\n",
            "Epoch [123/200], Batch [90/104], Loss: 0.0082\n",
            "Epoch [123/200], Batch [95/104], Loss: 0.0055\n",
            "Epoch [123/200], Batch [100/104], Loss: 0.0021\n",
            "Epoch 123: Train Loss = 0.4603, Accuracy = 100.00%\n",
            "Epoch [124/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [124/200], Batch [10/104], Loss: 0.0035\n",
            "Epoch [124/200], Batch [15/104], Loss: 0.0054\n",
            "Epoch [124/200], Batch [20/104], Loss: 0.0064\n",
            "Epoch [124/200], Batch [25/104], Loss: 0.0024\n",
            "Epoch [124/200], Batch [30/104], Loss: 0.0033\n",
            "Epoch [124/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [124/200], Batch [40/104], Loss: 0.0054\n",
            "Epoch [124/200], Batch [45/104], Loss: 0.0054\n",
            "Epoch [124/200], Batch [50/104], Loss: 0.0019\n",
            "Epoch [124/200], Batch [55/104], Loss: 0.0034\n",
            "Epoch [124/200], Batch [60/104], Loss: 0.0036\n",
            "Epoch [124/200], Batch [65/104], Loss: 0.0063\n",
            "Epoch [124/200], Batch [70/104], Loss: 0.0016\n",
            "Epoch [124/200], Batch [75/104], Loss: 0.0025\n",
            "Epoch [124/200], Batch [80/104], Loss: 0.0069\n",
            "Epoch [124/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [124/200], Batch [90/104], Loss: 0.0081\n",
            "Epoch [124/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [124/200], Batch [100/104], Loss: 0.0104\n",
            "Epoch 124: Train Loss = 0.4621, Accuracy = 100.00%\n",
            "Epoch [125/200], Batch [5/104], Loss: 0.0039\n",
            "Epoch [125/200], Batch [10/104], Loss: 0.0030\n",
            "Epoch [125/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [125/200], Batch [20/104], Loss: 0.0037\n",
            "Epoch [125/200], Batch [25/104], Loss: 0.0042\n",
            "Epoch [125/200], Batch [30/104], Loss: 0.0028\n",
            "Epoch [125/200], Batch [35/104], Loss: 0.0064\n",
            "Epoch [125/200], Batch [40/104], Loss: 0.0044\n",
            "Epoch [125/200], Batch [45/104], Loss: 0.0023\n",
            "Epoch [125/200], Batch [50/104], Loss: 0.0121\n",
            "Epoch [125/200], Batch [55/104], Loss: 0.0041\n",
            "Epoch [125/200], Batch [60/104], Loss: 0.0042\n",
            "Epoch [125/200], Batch [65/104], Loss: 0.0021\n",
            "Epoch [125/200], Batch [70/104], Loss: 0.0032\n",
            "Epoch [125/200], Batch [75/104], Loss: 0.0036\n",
            "Epoch [125/200], Batch [80/104], Loss: 0.0042\n",
            "Epoch [125/200], Batch [85/104], Loss: 0.0068\n",
            "Epoch [125/200], Batch [90/104], Loss: 0.0072\n",
            "Epoch [125/200], Batch [95/104], Loss: 0.0085\n",
            "Epoch [125/200], Batch [100/104], Loss: 0.0035\n",
            "Epoch 125: Train Loss = 0.4749, Accuracy = 100.00%\n",
            "Epoch [126/200], Batch [5/104], Loss: 0.0068\n",
            "Epoch [126/200], Batch [10/104], Loss: 0.0030\n",
            "Epoch [126/200], Batch [15/104], Loss: 0.0026\n",
            "Epoch [126/200], Batch [20/104], Loss: 0.0018\n",
            "Epoch [126/200], Batch [25/104], Loss: 0.0054\n",
            "Epoch [126/200], Batch [30/104], Loss: 0.0031\n",
            "Epoch [126/200], Batch [35/104], Loss: 0.0057\n",
            "Epoch [126/200], Batch [40/104], Loss: 0.0036\n",
            "Epoch [126/200], Batch [45/104], Loss: 0.0043\n",
            "Epoch [126/200], Batch [50/104], Loss: 0.0022\n",
            "Epoch [126/200], Batch [55/104], Loss: 0.0035\n",
            "Epoch [126/200], Batch [60/104], Loss: 0.0044\n",
            "Epoch [126/200], Batch [65/104], Loss: 0.0051\n",
            "Epoch [126/200], Batch [70/104], Loss: 0.0111\n",
            "Epoch [126/200], Batch [75/104], Loss: 0.0088\n",
            "Epoch [126/200], Batch [80/104], Loss: 0.0016\n",
            "Epoch [126/200], Batch [85/104], Loss: 0.0062\n",
            "Epoch [126/200], Batch [90/104], Loss: 0.0013\n",
            "Epoch [126/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [126/200], Batch [100/104], Loss: 0.0016\n",
            "Epoch 126: Train Loss = 0.4923, Accuracy = 100.00%\n",
            "Epoch [127/200], Batch [5/104], Loss: 0.0023\n",
            "Epoch [127/200], Batch [10/104], Loss: 0.0024\n",
            "Epoch [127/200], Batch [15/104], Loss: 0.0079\n",
            "Epoch [127/200], Batch [20/104], Loss: 0.0054\n",
            "Epoch [127/200], Batch [25/104], Loss: 0.0039\n",
            "Epoch [127/200], Batch [30/104], Loss: 0.0038\n",
            "Epoch [127/200], Batch [35/104], Loss: 0.0036\n",
            "Epoch [127/200], Batch [40/104], Loss: 0.0103\n",
            "Epoch [127/200], Batch [45/104], Loss: 0.0025\n",
            "Epoch [127/200], Batch [50/104], Loss: 0.0038\n",
            "Epoch [127/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [127/200], Batch [60/104], Loss: 0.0036\n",
            "Epoch [127/200], Batch [65/104], Loss: 0.0037\n",
            "Epoch [127/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [127/200], Batch [75/104], Loss: 0.0022\n",
            "Epoch [127/200], Batch [80/104], Loss: 0.0051\n",
            "Epoch [127/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [127/200], Batch [90/104], Loss: 0.0061\n",
            "Epoch [127/200], Batch [95/104], Loss: 0.0030\n",
            "Epoch [127/200], Batch [100/104], Loss: 0.0053\n",
            "Epoch 127: Train Loss = 0.5004, Accuracy = 100.00%\n",
            "Epoch [128/200], Batch [5/104], Loss: 0.0030\n",
            "Epoch [128/200], Batch [10/104], Loss: 0.0060\n",
            "Epoch [128/200], Batch [15/104], Loss: 0.0058\n",
            "Epoch [128/200], Batch [20/104], Loss: 0.0034\n",
            "Epoch [128/200], Batch [25/104], Loss: 0.0035\n",
            "Epoch [128/200], Batch [30/104], Loss: 0.0052\n",
            "Epoch [128/200], Batch [35/104], Loss: 0.0050\n",
            "Epoch [128/200], Batch [40/104], Loss: 0.0059\n",
            "Epoch [128/200], Batch [45/104], Loss: 0.0091\n",
            "Epoch [128/200], Batch [50/104], Loss: 0.0085\n",
            "Epoch [128/200], Batch [55/104], Loss: 0.0024\n",
            "Epoch [128/200], Batch [60/104], Loss: 0.0100\n",
            "Epoch [128/200], Batch [65/104], Loss: 0.0010\n",
            "Epoch [128/200], Batch [70/104], Loss: 0.0021\n",
            "Epoch [128/200], Batch [75/104], Loss: 0.0054\n",
            "Epoch [128/200], Batch [80/104], Loss: 0.0043\n",
            "Epoch [128/200], Batch [85/104], Loss: 0.0046\n",
            "Epoch [128/200], Batch [90/104], Loss: 0.0056\n",
            "Epoch [128/200], Batch [95/104], Loss: 0.0022\n",
            "Epoch [128/200], Batch [100/104], Loss: 0.0091\n",
            "Epoch 128: Train Loss = 0.4601, Accuracy = 100.00%\n",
            "Epoch [129/200], Batch [5/104], Loss: 0.0036\n",
            "Epoch [129/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [129/200], Batch [15/104], Loss: 0.0071\n",
            "Epoch [129/200], Batch [20/104], Loss: 0.0039\n",
            "Epoch [129/200], Batch [25/104], Loss: 0.0025\n",
            "Epoch [129/200], Batch [30/104], Loss: 0.0088\n",
            "Epoch [129/200], Batch [35/104], Loss: 0.0051\n",
            "Epoch [129/200], Batch [40/104], Loss: 0.0061\n",
            "Epoch [129/200], Batch [45/104], Loss: 0.0071\n",
            "Epoch [129/200], Batch [50/104], Loss: 0.0016\n",
            "Epoch [129/200], Batch [55/104], Loss: 0.0019\n",
            "Epoch [129/200], Batch [60/104], Loss: 0.0063\n",
            "Epoch [129/200], Batch [65/104], Loss: 0.0047\n",
            "Epoch [129/200], Batch [70/104], Loss: 0.0065\n",
            "Epoch [129/200], Batch [75/104], Loss: 0.0014\n",
            "Epoch [129/200], Batch [80/104], Loss: 0.0064\n",
            "Epoch [129/200], Batch [85/104], Loss: 0.0027\n",
            "Epoch [129/200], Batch [90/104], Loss: 0.0018\n",
            "Epoch [129/200], Batch [95/104], Loss: 0.0043\n",
            "Epoch [129/200], Batch [100/104], Loss: 0.0063\n",
            "Epoch 129: Train Loss = 0.4579, Accuracy = 100.00%\n",
            "Epoch [130/200], Batch [5/104], Loss: 0.0069\n",
            "Epoch [130/200], Batch [10/104], Loss: 0.0035\n",
            "Epoch [130/200], Batch [15/104], Loss: 0.0041\n",
            "Epoch [130/200], Batch [20/104], Loss: 0.0052\n",
            "Epoch [130/200], Batch [25/104], Loss: 0.0038\n",
            "Epoch [130/200], Batch [30/104], Loss: 0.0042\n",
            "Epoch [130/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [130/200], Batch [40/104], Loss: 0.0031\n",
            "Epoch [130/200], Batch [45/104], Loss: 0.0088\n",
            "Epoch [130/200], Batch [50/104], Loss: 0.0038\n",
            "Epoch [130/200], Batch [55/104], Loss: 0.0075\n",
            "Epoch [130/200], Batch [60/104], Loss: 0.0040\n",
            "Epoch [130/200], Batch [65/104], Loss: 0.0057\n",
            "Epoch [130/200], Batch [70/104], Loss: 0.0042\n",
            "Epoch [130/200], Batch [75/104], Loss: 0.0057\n",
            "Epoch [130/200], Batch [80/104], Loss: 0.0106\n",
            "Epoch [130/200], Batch [85/104], Loss: 0.0051\n",
            "Epoch [130/200], Batch [90/104], Loss: 0.0014\n",
            "Epoch [130/200], Batch [95/104], Loss: 0.0075\n",
            "Epoch [130/200], Batch [100/104], Loss: 0.0022\n",
            "Epoch 130: Train Loss = 0.4490, Accuracy = 100.00%\n",
            "Epoch [131/200], Batch [5/104], Loss: 0.0015\n",
            "Epoch [131/200], Batch [10/104], Loss: 0.0028\n",
            "Epoch [131/200], Batch [15/104], Loss: 0.0036\n",
            "Epoch [131/200], Batch [20/104], Loss: 0.0012\n",
            "Epoch [131/200], Batch [25/104], Loss: 0.0039\n",
            "Epoch [131/200], Batch [30/104], Loss: 0.0029\n",
            "Epoch [131/200], Batch [35/104], Loss: 0.0032\n",
            "Epoch [131/200], Batch [40/104], Loss: 0.0036\n",
            "Epoch [131/200], Batch [45/104], Loss: 0.0023\n",
            "Epoch [131/200], Batch [50/104], Loss: 0.0066\n",
            "Epoch [131/200], Batch [55/104], Loss: 0.0051\n",
            "Epoch [131/200], Batch [60/104], Loss: 0.0040\n",
            "Epoch [131/200], Batch [65/104], Loss: 0.0032\n",
            "Epoch [131/200], Batch [70/104], Loss: 0.0077\n",
            "Epoch [131/200], Batch [75/104], Loss: 0.0042\n",
            "Epoch [131/200], Batch [80/104], Loss: 0.0101\n",
            "Epoch [131/200], Batch [85/104], Loss: 0.0070\n",
            "Epoch [131/200], Batch [90/104], Loss: 0.0098\n",
            "Epoch [131/200], Batch [95/104], Loss: 0.0050\n",
            "Epoch [131/200], Batch [100/104], Loss: 0.0040\n",
            "Epoch 131: Train Loss = 0.4408, Accuracy = 100.00%\n",
            "Epoch [132/200], Batch [5/104], Loss: 0.0019\n",
            "Epoch [132/200], Batch [10/104], Loss: 0.0068\n",
            "Epoch [132/200], Batch [15/104], Loss: 0.0052\n",
            "Epoch [132/200], Batch [20/104], Loss: 0.0074\n",
            "Epoch [132/200], Batch [25/104], Loss: 0.0069\n",
            "Epoch [132/200], Batch [30/104], Loss: 0.0028\n",
            "Epoch [132/200], Batch [35/104], Loss: 0.0060\n",
            "Epoch [132/200], Batch [40/104], Loss: 0.0063\n",
            "Epoch [132/200], Batch [45/104], Loss: 0.0022\n",
            "Epoch [132/200], Batch [50/104], Loss: 0.0030\n",
            "Epoch [132/200], Batch [55/104], Loss: 0.0091\n",
            "Epoch [132/200], Batch [60/104], Loss: 0.0020\n",
            "Epoch [132/200], Batch [65/104], Loss: 0.0038\n",
            "Epoch [132/200], Batch [70/104], Loss: 0.0020\n",
            "Epoch [132/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [132/200], Batch [80/104], Loss: 0.0032\n",
            "Epoch [132/200], Batch [85/104], Loss: 0.0045\n",
            "Epoch [132/200], Batch [90/104], Loss: 0.0020\n",
            "Epoch [132/200], Batch [95/104], Loss: 0.0029\n",
            "Epoch [132/200], Batch [100/104], Loss: 0.0034\n",
            "Epoch 132: Train Loss = 0.4434, Accuracy = 100.00%\n",
            "Epoch [133/200], Batch [5/104], Loss: 0.0053\n",
            "Epoch [133/200], Batch [10/104], Loss: 0.0045\n",
            "Epoch [133/200], Batch [15/104], Loss: 0.0048\n",
            "Epoch [133/200], Batch [20/104], Loss: 0.0067\n",
            "Epoch [133/200], Batch [25/104], Loss: 0.0030\n",
            "Epoch [133/200], Batch [30/104], Loss: 0.0044\n",
            "Epoch [133/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [133/200], Batch [40/104], Loss: 0.0037\n",
            "Epoch [133/200], Batch [45/104], Loss: 0.0017\n",
            "Epoch [133/200], Batch [50/104], Loss: 0.0074\n",
            "Epoch [133/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [133/200], Batch [60/104], Loss: 0.0027\n",
            "Epoch [133/200], Batch [65/104], Loss: 0.0025\n",
            "Epoch [133/200], Batch [70/104], Loss: 0.0064\n",
            "Epoch [133/200], Batch [75/104], Loss: 0.0021\n",
            "Epoch [133/200], Batch [80/104], Loss: 0.0020\n",
            "Epoch [133/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [133/200], Batch [90/104], Loss: 0.0015\n",
            "Epoch [133/200], Batch [95/104], Loss: 0.0021\n",
            "Epoch [133/200], Batch [100/104], Loss: 0.0046\n",
            "Epoch 133: Train Loss = 0.4324, Accuracy = 100.00%\n",
            "Epoch [134/200], Batch [5/104], Loss: 0.0030\n",
            "Epoch [134/200], Batch [10/104], Loss: 0.0020\n",
            "Epoch [134/200], Batch [15/104], Loss: 0.0038\n",
            "Epoch [134/200], Batch [20/104], Loss: 0.0035\n",
            "Epoch [134/200], Batch [25/104], Loss: 0.0078\n",
            "Epoch [134/200], Batch [30/104], Loss: 0.0025\n",
            "Epoch [134/200], Batch [35/104], Loss: 0.0021\n",
            "Epoch [134/200], Batch [40/104], Loss: 0.0030\n",
            "Epoch [134/200], Batch [45/104], Loss: 0.0026\n",
            "Epoch [134/200], Batch [50/104], Loss: 0.0052\n",
            "Epoch [134/200], Batch [55/104], Loss: 0.0059\n",
            "Epoch [134/200], Batch [60/104], Loss: 0.0035\n",
            "Epoch [134/200], Batch [65/104], Loss: 0.0017\n",
            "Epoch [134/200], Batch [70/104], Loss: 0.0046\n",
            "Epoch [134/200], Batch [75/104], Loss: 0.0075\n",
            "Epoch [134/200], Batch [80/104], Loss: 0.0053\n",
            "Epoch [134/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [134/200], Batch [90/104], Loss: 0.0019\n",
            "Epoch [134/200], Batch [95/104], Loss: 0.0016\n",
            "Epoch [134/200], Batch [100/104], Loss: 0.0036\n",
            "Epoch 134: Train Loss = 0.4398, Accuracy = 100.00%\n",
            "Epoch [135/200], Batch [5/104], Loss: 0.0025\n",
            "Epoch [135/200], Batch [10/104], Loss: 0.0066\n",
            "Epoch [135/200], Batch [15/104], Loss: 0.0034\n",
            "Epoch [135/200], Batch [20/104], Loss: 0.0038\n",
            "Epoch [135/200], Batch [25/104], Loss: 0.0070\n",
            "Epoch [135/200], Batch [30/104], Loss: 0.0048\n",
            "Epoch [135/200], Batch [35/104], Loss: 0.0030\n",
            "Epoch [135/200], Batch [40/104], Loss: 0.0066\n",
            "Epoch [135/200], Batch [45/104], Loss: 0.0037\n",
            "Epoch [135/200], Batch [50/104], Loss: 0.0043\n",
            "Epoch [135/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [135/200], Batch [60/104], Loss: 0.0045\n",
            "Epoch [135/200], Batch [65/104], Loss: 0.0040\n",
            "Epoch [135/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [135/200], Batch [75/104], Loss: 0.0031\n",
            "Epoch [135/200], Batch [80/104], Loss: 0.0027\n",
            "Epoch [135/200], Batch [85/104], Loss: 0.0030\n",
            "Epoch [135/200], Batch [90/104], Loss: 0.0064\n",
            "Epoch [135/200], Batch [95/104], Loss: 0.0037\n",
            "Epoch [135/200], Batch [100/104], Loss: 0.0052\n",
            "Epoch 135: Train Loss = 0.4481, Accuracy = 100.00%\n",
            "Epoch [136/200], Batch [5/104], Loss: 0.0023\n",
            "Epoch [136/200], Batch [10/104], Loss: 0.0050\n",
            "Epoch [136/200], Batch [15/104], Loss: 0.0054\n",
            "Epoch [136/200], Batch [20/104], Loss: 0.0077\n",
            "Epoch [136/200], Batch [25/104], Loss: 0.0071\n",
            "Epoch [136/200], Batch [30/104], Loss: 0.0101\n",
            "Epoch [136/200], Batch [35/104], Loss: 0.0034\n",
            "Epoch [136/200], Batch [40/104], Loss: 0.0053\n",
            "Epoch [136/200], Batch [45/104], Loss: 0.0020\n",
            "Epoch [136/200], Batch [50/104], Loss: 0.0030\n",
            "Epoch [136/200], Batch [55/104], Loss: 0.0019\n",
            "Epoch [136/200], Batch [60/104], Loss: 0.0048\n",
            "Epoch [136/200], Batch [65/104], Loss: 0.0088\n",
            "Epoch [136/200], Batch [70/104], Loss: 0.0035\n",
            "Epoch [136/200], Batch [75/104], Loss: 0.0012\n",
            "Epoch [136/200], Batch [80/104], Loss: 0.0035\n",
            "Epoch [136/200], Batch [85/104], Loss: 0.0029\n",
            "Epoch [136/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [136/200], Batch [95/104], Loss: 0.0077\n",
            "Epoch [136/200], Batch [100/104], Loss: 0.0025\n",
            "Epoch 136: Train Loss = 0.4421, Accuracy = 100.00%\n",
            "Epoch [137/200], Batch [5/104], Loss: 0.0015\n",
            "Epoch [137/200], Batch [10/104], Loss: 0.0012\n",
            "Epoch [137/200], Batch [15/104], Loss: 0.0022\n",
            "Epoch [137/200], Batch [20/104], Loss: 0.0049\n",
            "Epoch [137/200], Batch [25/104], Loss: 0.0015\n",
            "Epoch [137/200], Batch [30/104], Loss: 0.0047\n",
            "Epoch [137/200], Batch [35/104], Loss: 0.0037\n",
            "Epoch [137/200], Batch [40/104], Loss: 0.0025\n",
            "Epoch [137/200], Batch [45/104], Loss: 0.0015\n",
            "Epoch [137/200], Batch [50/104], Loss: 0.0021\n",
            "Epoch [137/200], Batch [55/104], Loss: 0.0036\n",
            "Epoch [137/200], Batch [60/104], Loss: 0.0025\n",
            "Epoch [137/200], Batch [65/104], Loss: 0.0024\n",
            "Epoch [137/200], Batch [70/104], Loss: 0.0050\n",
            "Epoch [137/200], Batch [75/104], Loss: 0.0055\n",
            "Epoch [137/200], Batch [80/104], Loss: 0.0079\n",
            "Epoch [137/200], Batch [85/104], Loss: 0.0038\n",
            "Epoch [137/200], Batch [90/104], Loss: 0.0010\n",
            "Epoch [137/200], Batch [95/104], Loss: 0.0041\n",
            "Epoch [137/200], Batch [100/104], Loss: 0.0068\n",
            "Epoch 137: Train Loss = 0.4279, Accuracy = 100.00%\n",
            "Epoch [138/200], Batch [5/104], Loss: 0.0033\n",
            "Epoch [138/200], Batch [10/104], Loss: 0.0026\n",
            "Epoch [138/200], Batch [15/104], Loss: 0.0059\n",
            "Epoch [138/200], Batch [20/104], Loss: 0.0071\n",
            "Epoch [138/200], Batch [25/104], Loss: 0.0049\n",
            "Epoch [138/200], Batch [30/104], Loss: 0.0029\n",
            "Epoch [138/200], Batch [35/104], Loss: 0.0011\n",
            "Epoch [138/200], Batch [40/104], Loss: 0.0034\n",
            "Epoch [138/200], Batch [45/104], Loss: 0.0034\n",
            "Epoch [138/200], Batch [50/104], Loss: 0.0044\n",
            "Epoch [138/200], Batch [55/104], Loss: 0.0026\n",
            "Epoch [138/200], Batch [60/104], Loss: 0.0019\n",
            "Epoch [138/200], Batch [65/104], Loss: 0.0037\n",
            "Epoch [138/200], Batch [70/104], Loss: 0.0035\n",
            "Epoch [138/200], Batch [75/104], Loss: 0.0038\n",
            "Epoch [138/200], Batch [80/104], Loss: 0.0015\n",
            "Epoch [138/200], Batch [85/104], Loss: 0.0048\n",
            "Epoch [138/200], Batch [90/104], Loss: 0.0043\n",
            "Epoch [138/200], Batch [95/104], Loss: 0.0058\n",
            "Epoch [138/200], Batch [100/104], Loss: 0.0112\n",
            "Epoch 138: Train Loss = 0.4267, Accuracy = 100.00%\n",
            "Epoch [139/200], Batch [5/104], Loss: 0.0024\n",
            "Epoch [139/200], Batch [10/104], Loss: 0.0022\n",
            "Epoch [139/200], Batch [15/104], Loss: 0.0021\n",
            "Epoch [139/200], Batch [20/104], Loss: 0.0029\n",
            "Epoch [139/200], Batch [25/104], Loss: 0.0029\n",
            "Epoch [139/200], Batch [30/104], Loss: 0.0025\n",
            "Epoch [139/200], Batch [35/104], Loss: 0.0040\n",
            "Epoch [139/200], Batch [40/104], Loss: 0.0151\n",
            "Epoch [139/200], Batch [45/104], Loss: 0.0042\n",
            "Epoch [139/200], Batch [50/104], Loss: 0.0060\n",
            "Epoch [139/200], Batch [55/104], Loss: 0.0031\n",
            "Epoch [139/200], Batch [60/104], Loss: 0.0060\n",
            "Epoch [139/200], Batch [65/104], Loss: 0.0081\n",
            "Epoch [139/200], Batch [70/104], Loss: 0.0043\n",
            "Epoch [139/200], Batch [75/104], Loss: 0.0017\n",
            "Epoch [139/200], Batch [80/104], Loss: 0.0027\n",
            "Epoch [139/200], Batch [85/104], Loss: 0.0045\n",
            "Epoch [139/200], Batch [90/104], Loss: 0.0079\n",
            "Epoch [139/200], Batch [95/104], Loss: 0.0043\n",
            "Epoch [139/200], Batch [100/104], Loss: 0.0065\n",
            "Epoch 139: Train Loss = 0.4262, Accuracy = 100.00%\n",
            "Epoch [140/200], Batch [5/104], Loss: 0.0092\n",
            "Epoch [140/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [140/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [140/200], Batch [20/104], Loss: 0.0044\n",
            "Epoch [140/200], Batch [25/104], Loss: 0.0084\n",
            "Epoch [140/200], Batch [30/104], Loss: 0.0070\n",
            "Epoch [140/200], Batch [35/104], Loss: 0.0023\n",
            "Epoch [140/200], Batch [40/104], Loss: 0.0042\n",
            "Epoch [140/200], Batch [45/104], Loss: 0.0027\n",
            "Epoch [140/200], Batch [50/104], Loss: 0.0043\n",
            "Epoch [140/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [140/200], Batch [60/104], Loss: 0.0050\n",
            "Epoch [140/200], Batch [65/104], Loss: 0.0037\n",
            "Epoch [140/200], Batch [70/104], Loss: 0.0021\n",
            "Epoch [140/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [140/200], Batch [80/104], Loss: 0.0026\n",
            "Epoch [140/200], Batch [85/104], Loss: 0.0063\n",
            "Epoch [140/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [140/200], Batch [95/104], Loss: 0.0048\n",
            "Epoch [140/200], Batch [100/104], Loss: 0.0022\n",
            "Epoch 140: Train Loss = 0.4291, Accuracy = 100.00%\n",
            "Epoch [141/200], Batch [5/104], Loss: 0.0054\n",
            "Epoch [141/200], Batch [10/104], Loss: 0.0029\n",
            "Epoch [141/200], Batch [15/104], Loss: 0.0021\n",
            "Epoch [141/200], Batch [20/104], Loss: 0.0019\n",
            "Epoch [141/200], Batch [25/104], Loss: 0.0059\n",
            "Epoch [141/200], Batch [30/104], Loss: 0.0018\n",
            "Epoch [141/200], Batch [35/104], Loss: 0.0057\n",
            "Epoch [141/200], Batch [40/104], Loss: 0.0019\n",
            "Epoch [141/200], Batch [45/104], Loss: 0.0033\n",
            "Epoch [141/200], Batch [50/104], Loss: 0.0029\n",
            "Epoch [141/200], Batch [55/104], Loss: 0.0011\n",
            "Epoch [141/200], Batch [60/104], Loss: 0.0012\n",
            "Epoch [141/200], Batch [65/104], Loss: 0.0061\n",
            "Epoch [141/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [141/200], Batch [75/104], Loss: 0.0038\n",
            "Epoch [141/200], Batch [80/104], Loss: 0.0053\n",
            "Epoch [141/200], Batch [85/104], Loss: 0.0026\n",
            "Epoch [141/200], Batch [90/104], Loss: 0.0030\n",
            "Epoch [141/200], Batch [95/104], Loss: 0.0044\n",
            "Epoch [141/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 141: Train Loss = 0.4212, Accuracy = 100.00%\n",
            "Epoch [142/200], Batch [5/104], Loss: 0.0016\n",
            "Epoch [142/200], Batch [10/104], Loss: 0.0045\n",
            "Epoch [142/200], Batch [15/104], Loss: 0.0037\n",
            "Epoch [142/200], Batch [20/104], Loss: 0.0037\n",
            "Epoch [142/200], Batch [25/104], Loss: 0.0066\n",
            "Epoch [142/200], Batch [30/104], Loss: 0.0020\n",
            "Epoch [142/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [142/200], Batch [40/104], Loss: 0.0135\n",
            "Epoch [142/200], Batch [45/104], Loss: 0.0089\n",
            "Epoch [142/200], Batch [50/104], Loss: 0.0031\n",
            "Epoch [142/200], Batch [55/104], Loss: 0.0019\n",
            "Epoch [142/200], Batch [60/104], Loss: 0.0032\n",
            "Epoch [142/200], Batch [65/104], Loss: 0.0048\n",
            "Epoch [142/200], Batch [70/104], Loss: 0.0050\n",
            "Epoch [142/200], Batch [75/104], Loss: 0.0026\n",
            "Epoch [142/200], Batch [80/104], Loss: 0.0063\n",
            "Epoch [142/200], Batch [85/104], Loss: 0.0076\n",
            "Epoch [142/200], Batch [90/104], Loss: 0.0058\n",
            "Epoch [142/200], Batch [95/104], Loss: 0.0008\n",
            "Epoch [142/200], Batch [100/104], Loss: 0.0029\n",
            "Epoch 142: Train Loss = 0.4372, Accuracy = 100.00%\n",
            "Epoch [143/200], Batch [5/104], Loss: 0.0020\n",
            "Epoch [143/200], Batch [10/104], Loss: 0.0018\n",
            "Epoch [143/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [143/200], Batch [20/104], Loss: 0.0062\n",
            "Epoch [143/200], Batch [25/104], Loss: 0.0037\n",
            "Epoch [143/200], Batch [30/104], Loss: 0.0020\n",
            "Epoch [143/200], Batch [35/104], Loss: 0.0044\n",
            "Epoch [143/200], Batch [40/104], Loss: 0.0041\n",
            "Epoch [143/200], Batch [45/104], Loss: 0.0010\n",
            "Epoch [143/200], Batch [50/104], Loss: 0.0042\n",
            "Epoch [143/200], Batch [55/104], Loss: 0.0033\n",
            "Epoch [143/200], Batch [60/104], Loss: 0.0102\n",
            "Epoch [143/200], Batch [65/104], Loss: 0.0035\n",
            "Epoch [143/200], Batch [70/104], Loss: 0.0065\n",
            "Epoch [143/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [143/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [143/200], Batch [85/104], Loss: 0.0042\n",
            "Epoch [143/200], Batch [90/104], Loss: 0.0038\n",
            "Epoch [143/200], Batch [95/104], Loss: 0.0053\n",
            "Epoch [143/200], Batch [100/104], Loss: 0.0056\n",
            "Epoch 143: Train Loss = 0.4212, Accuracy = 100.00%\n",
            "Epoch [144/200], Batch [5/104], Loss: 0.0025\n",
            "Epoch [144/200], Batch [10/104], Loss: 0.0066\n",
            "Epoch [144/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [144/200], Batch [20/104], Loss: 0.0037\n",
            "Epoch [144/200], Batch [25/104], Loss: 0.0062\n",
            "Epoch [144/200], Batch [30/104], Loss: 0.0022\n",
            "Epoch [144/200], Batch [35/104], Loss: 0.0061\n",
            "Epoch [144/200], Batch [40/104], Loss: 0.0036\n",
            "Epoch [144/200], Batch [45/104], Loss: 0.0032\n",
            "Epoch [144/200], Batch [50/104], Loss: 0.0022\n",
            "Epoch [144/200], Batch [55/104], Loss: 0.0020\n",
            "Epoch [144/200], Batch [60/104], Loss: 0.0054\n",
            "Epoch [144/200], Batch [65/104], Loss: 0.0079\n",
            "Epoch [144/200], Batch [70/104], Loss: 0.0012\n",
            "Epoch [144/200], Batch [75/104], Loss: 0.0051\n",
            "Epoch [144/200], Batch [80/104], Loss: 0.0026\n",
            "Epoch [144/200], Batch [85/104], Loss: 0.0035\n",
            "Epoch [144/200], Batch [90/104], Loss: 0.0005\n",
            "Epoch [144/200], Batch [95/104], Loss: 0.0021\n",
            "Epoch [144/200], Batch [100/104], Loss: 0.0032\n",
            "Epoch 144: Train Loss = 0.4222, Accuracy = 100.00%\n",
            "Epoch [145/200], Batch [5/104], Loss: 0.0036\n",
            "Epoch [145/200], Batch [10/104], Loss: 0.0019\n",
            "Epoch [145/200], Batch [15/104], Loss: 0.0031\n",
            "Epoch [145/200], Batch [20/104], Loss: 0.0029\n",
            "Epoch [145/200], Batch [25/104], Loss: 0.0031\n",
            "Epoch [145/200], Batch [30/104], Loss: 0.0090\n",
            "Epoch [145/200], Batch [35/104], Loss: 0.0024\n",
            "Epoch [145/200], Batch [40/104], Loss: 0.0014\n",
            "Epoch [145/200], Batch [45/104], Loss: 0.0026\n",
            "Epoch [145/200], Batch [50/104], Loss: 0.0027\n",
            "Epoch [145/200], Batch [55/104], Loss: 0.0095\n",
            "Epoch [145/200], Batch [60/104], Loss: 0.0031\n",
            "Epoch [145/200], Batch [65/104], Loss: 0.0030\n",
            "Epoch [145/200], Batch [70/104], Loss: 0.0076\n",
            "Epoch [145/200], Batch [75/104], Loss: 0.0067\n",
            "Epoch [145/200], Batch [80/104], Loss: 0.0037\n",
            "Epoch [145/200], Batch [85/104], Loss: 0.0061\n",
            "Epoch [145/200], Batch [90/104], Loss: 0.0022\n",
            "Epoch [145/200], Batch [95/104], Loss: 0.0039\n",
            "Epoch [145/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 145: Train Loss = 0.4160, Accuracy = 100.00%\n",
            "Epoch [146/200], Batch [5/104], Loss: 0.0014\n",
            "Epoch [146/200], Batch [10/104], Loss: 0.0024\n",
            "Epoch [146/200], Batch [15/104], Loss: 0.0060\n",
            "Epoch [146/200], Batch [20/104], Loss: 0.0041\n",
            "Epoch [146/200], Batch [25/104], Loss: 0.0036\n",
            "Epoch [146/200], Batch [30/104], Loss: 0.0066\n",
            "Epoch [146/200], Batch [35/104], Loss: 0.0039\n",
            "Epoch [146/200], Batch [40/104], Loss: 0.0052\n",
            "Epoch [146/200], Batch [45/104], Loss: 0.0050\n",
            "Epoch [146/200], Batch [50/104], Loss: 0.0033\n",
            "Epoch [146/200], Batch [55/104], Loss: 0.0045\n",
            "Epoch [146/200], Batch [60/104], Loss: 0.0012\n",
            "Epoch [146/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [146/200], Batch [70/104], Loss: 0.0073\n",
            "Epoch [146/200], Batch [75/104], Loss: 0.0024\n",
            "Epoch [146/200], Batch [80/104], Loss: 0.0007\n",
            "Epoch [146/200], Batch [85/104], Loss: 0.0053\n",
            "Epoch [146/200], Batch [90/104], Loss: 0.0043\n",
            "Epoch [146/200], Batch [95/104], Loss: 0.0013\n",
            "Epoch [146/200], Batch [100/104], Loss: 0.0017\n",
            "Epoch 146: Train Loss = 0.4144, Accuracy = 100.00%\n",
            "Epoch [147/200], Batch [5/104], Loss: 0.0031\n",
            "Epoch [147/200], Batch [10/104], Loss: 0.0029\n",
            "Epoch [147/200], Batch [15/104], Loss: 0.0036\n",
            "Epoch [147/200], Batch [20/104], Loss: 0.0045\n",
            "Epoch [147/200], Batch [25/104], Loss: 0.0046\n",
            "Epoch [147/200], Batch [30/104], Loss: 0.0021\n",
            "Epoch [147/200], Batch [35/104], Loss: 0.0042\n",
            "Epoch [147/200], Batch [40/104], Loss: 0.0028\n",
            "Epoch [147/200], Batch [45/104], Loss: 0.0046\n",
            "Epoch [147/200], Batch [50/104], Loss: 0.0038\n",
            "Epoch [147/200], Batch [55/104], Loss: 0.0040\n",
            "Epoch [147/200], Batch [60/104], Loss: 0.0028\n",
            "Epoch [147/200], Batch [65/104], Loss: 0.0079\n",
            "Epoch [147/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [147/200], Batch [75/104], Loss: 0.0048\n",
            "Epoch [147/200], Batch [80/104], Loss: 0.0043\n",
            "Epoch [147/200], Batch [85/104], Loss: 0.0076\n",
            "Epoch [147/200], Batch [90/104], Loss: 0.0063\n",
            "Epoch [147/200], Batch [95/104], Loss: 0.0059\n",
            "Epoch [147/200], Batch [100/104], Loss: 0.0062\n",
            "Epoch 147: Train Loss = 0.4116, Accuracy = 100.00%\n",
            "Epoch [148/200], Batch [5/104], Loss: 0.0037\n",
            "Epoch [148/200], Batch [10/104], Loss: 0.0081\n",
            "Epoch [148/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [148/200], Batch [20/104], Loss: 0.0036\n",
            "Epoch [148/200], Batch [25/104], Loss: 0.0051\n",
            "Epoch [148/200], Batch [30/104], Loss: 0.0069\n",
            "Epoch [148/200], Batch [35/104], Loss: 0.0087\n",
            "Epoch [148/200], Batch [40/104], Loss: 0.0025\n",
            "Epoch [148/200], Batch [45/104], Loss: 0.0017\n",
            "Epoch [148/200], Batch [50/104], Loss: 0.0047\n",
            "Epoch [148/200], Batch [55/104], Loss: 0.0017\n",
            "Epoch [148/200], Batch [60/104], Loss: 0.0033\n",
            "Epoch [148/200], Batch [65/104], Loss: 0.0052\n",
            "Epoch [148/200], Batch [70/104], Loss: 0.0026\n",
            "Epoch [148/200], Batch [75/104], Loss: 0.0028\n",
            "Epoch [148/200], Batch [80/104], Loss: 0.0020\n",
            "Epoch [148/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [148/200], Batch [90/104], Loss: 0.0071\n",
            "Epoch [148/200], Batch [95/104], Loss: 0.0025\n",
            "Epoch [148/200], Batch [100/104], Loss: 0.0056\n",
            "Epoch 148: Train Loss = 0.4145, Accuracy = 100.00%\n",
            "Epoch [149/200], Batch [5/104], Loss: 0.0017\n",
            "Epoch [149/200], Batch [10/104], Loss: 0.0022\n",
            "Epoch [149/200], Batch [15/104], Loss: 0.0041\n",
            "Epoch [149/200], Batch [20/104], Loss: 0.0056\n",
            "Epoch [149/200], Batch [25/104], Loss: 0.0049\n",
            "Epoch [149/200], Batch [30/104], Loss: 0.0035\n",
            "Epoch [149/200], Batch [35/104], Loss: 0.0016\n",
            "Epoch [149/200], Batch [40/104], Loss: 0.0048\n",
            "Epoch [149/200], Batch [45/104], Loss: 0.0036\n",
            "Epoch [149/200], Batch [50/104], Loss: 0.0014\n",
            "Epoch [149/200], Batch [55/104], Loss: 0.0013\n",
            "Epoch [149/200], Batch [60/104], Loss: 0.0056\n",
            "Epoch [149/200], Batch [65/104], Loss: 0.0056\n",
            "Epoch [149/200], Batch [70/104], Loss: 0.0031\n",
            "Epoch [149/200], Batch [75/104], Loss: 0.0058\n",
            "Epoch [149/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [149/200], Batch [85/104], Loss: 0.0032\n",
            "Epoch [149/200], Batch [90/104], Loss: 0.0052\n",
            "Epoch [149/200], Batch [95/104], Loss: 0.0064\n",
            "Epoch [149/200], Batch [100/104], Loss: 0.0028\n",
            "Epoch 149: Train Loss = 0.4198, Accuracy = 100.00%\n",
            "Epoch [150/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [150/200], Batch [10/104], Loss: 0.0027\n",
            "Epoch [150/200], Batch [15/104], Loss: 0.0023\n",
            "Epoch [150/200], Batch [20/104], Loss: 0.0015\n",
            "Epoch [150/200], Batch [25/104], Loss: 0.0039\n",
            "Epoch [150/200], Batch [30/104], Loss: 0.0017\n",
            "Epoch [150/200], Batch [35/104], Loss: 0.0065\n",
            "Epoch [150/200], Batch [40/104], Loss: 0.0037\n",
            "Epoch [150/200], Batch [45/104], Loss: 0.0042\n",
            "Epoch [150/200], Batch [50/104], Loss: 0.0049\n",
            "Epoch [150/200], Batch [55/104], Loss: 0.0038\n",
            "Epoch [150/200], Batch [60/104], Loss: 0.0038\n",
            "Epoch [150/200], Batch [65/104], Loss: 0.0033\n",
            "Epoch [150/200], Batch [70/104], Loss: 0.0022\n",
            "Epoch [150/200], Batch [75/104], Loss: 0.0029\n",
            "Epoch [150/200], Batch [80/104], Loss: 0.0014\n",
            "Epoch [150/200], Batch [85/104], Loss: 0.0093\n",
            "Epoch [150/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [150/200], Batch [95/104], Loss: 0.0056\n",
            "Epoch [150/200], Batch [100/104], Loss: 0.0026\n",
            "Epoch 150: Train Loss = 0.4105, Accuracy = 100.00%\n",
            "Epoch [151/200], Batch [5/104], Loss: 0.0050\n",
            "Epoch [151/200], Batch [10/104], Loss: 0.0036\n",
            "Epoch [151/200], Batch [15/104], Loss: 0.0041\n",
            "Epoch [151/200], Batch [20/104], Loss: 0.0029\n",
            "Epoch [151/200], Batch [25/104], Loss: 0.0040\n",
            "Epoch [151/200], Batch [30/104], Loss: 0.0005\n",
            "Epoch [151/200], Batch [35/104], Loss: 0.0017\n",
            "Epoch [151/200], Batch [40/104], Loss: 0.0079\n",
            "Epoch [151/200], Batch [45/104], Loss: 0.0044\n",
            "Epoch [151/200], Batch [50/104], Loss: 0.0037\n",
            "Epoch [151/200], Batch [55/104], Loss: 0.0075\n",
            "Epoch [151/200], Batch [60/104], Loss: 0.0019\n",
            "Epoch [151/200], Batch [65/104], Loss: 0.0029\n",
            "Epoch [151/200], Batch [70/104], Loss: 0.0108\n",
            "Epoch [151/200], Batch [75/104], Loss: 0.0057\n",
            "Epoch [151/200], Batch [80/104], Loss: 0.0026\n",
            "Epoch [151/200], Batch [85/104], Loss: 0.0063\n",
            "Epoch [151/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [151/200], Batch [95/104], Loss: 0.0034\n",
            "Epoch [151/200], Batch [100/104], Loss: 0.0012\n",
            "Epoch 151: Train Loss = 0.4109, Accuracy = 100.00%\n",
            "Epoch [152/200], Batch [5/104], Loss: 0.0019\n",
            "Epoch [152/200], Batch [10/104], Loss: 0.0027\n",
            "Epoch [152/200], Batch [15/104], Loss: 0.0040\n",
            "Epoch [152/200], Batch [20/104], Loss: 0.0015\n",
            "Epoch [152/200], Batch [25/104], Loss: 0.0022\n",
            "Epoch [152/200], Batch [30/104], Loss: 0.0025\n",
            "Epoch [152/200], Batch [35/104], Loss: 0.0007\n",
            "Epoch [152/200], Batch [40/104], Loss: 0.0056\n",
            "Epoch [152/200], Batch [45/104], Loss: 0.0024\n",
            "Epoch [152/200], Batch [50/104], Loss: 0.0040\n",
            "Epoch [152/200], Batch [55/104], Loss: 0.0070\n",
            "Epoch [152/200], Batch [60/104], Loss: 0.0041\n",
            "Epoch [152/200], Batch [65/104], Loss: 0.0030\n",
            "Epoch [152/200], Batch [70/104], Loss: 0.0034\n",
            "Epoch [152/200], Batch [75/104], Loss: 0.0016\n",
            "Epoch [152/200], Batch [80/104], Loss: 0.0022\n",
            "Epoch [152/200], Batch [85/104], Loss: 0.0036\n",
            "Epoch [152/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [152/200], Batch [95/104], Loss: 0.0053\n",
            "Epoch [152/200], Batch [100/104], Loss: 0.0041\n",
            "Epoch 152: Train Loss = 0.4108, Accuracy = 100.00%\n",
            "Epoch [153/200], Batch [5/104], Loss: 0.0024\n",
            "Epoch [153/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [153/200], Batch [15/104], Loss: 0.0069\n",
            "Epoch [153/200], Batch [20/104], Loss: 0.0067\n",
            "Epoch [153/200], Batch [25/104], Loss: 0.0013\n",
            "Epoch [153/200], Batch [30/104], Loss: 0.0022\n",
            "Epoch [153/200], Batch [35/104], Loss: 0.0020\n",
            "Epoch [153/200], Batch [40/104], Loss: 0.0056\n",
            "Epoch [153/200], Batch [45/104], Loss: 0.0050\n",
            "Epoch [153/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [153/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [153/200], Batch [60/104], Loss: 0.0019\n",
            "Epoch [153/200], Batch [65/104], Loss: 0.0020\n",
            "Epoch [153/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [153/200], Batch [75/104], Loss: 0.0030\n",
            "Epoch [153/200], Batch [80/104], Loss: 0.0042\n",
            "Epoch [153/200], Batch [85/104], Loss: 0.0049\n",
            "Epoch [153/200], Batch [90/104], Loss: 0.0086\n",
            "Epoch [153/200], Batch [95/104], Loss: 0.0018\n",
            "Epoch [153/200], Batch [100/104], Loss: 0.0059\n",
            "Epoch 153: Train Loss = 0.4082, Accuracy = 100.00%\n",
            "Epoch [154/200], Batch [5/104], Loss: 0.0021\n",
            "Epoch [154/200], Batch [10/104], Loss: 0.0055\n",
            "Epoch [154/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [154/200], Batch [20/104], Loss: 0.0013\n",
            "Epoch [154/200], Batch [25/104], Loss: 0.0075\n",
            "Epoch [154/200], Batch [30/104], Loss: 0.0020\n",
            "Epoch [154/200], Batch [35/104], Loss: 0.0036\n",
            "Epoch [154/200], Batch [40/104], Loss: 0.0044\n",
            "Epoch [154/200], Batch [45/104], Loss: 0.0048\n",
            "Epoch [154/200], Batch [50/104], Loss: 0.0048\n",
            "Epoch [154/200], Batch [55/104], Loss: 0.0017\n",
            "Epoch [154/200], Batch [60/104], Loss: 0.0046\n",
            "Epoch [154/200], Batch [65/104], Loss: 0.0014\n",
            "Epoch [154/200], Batch [70/104], Loss: 0.0044\n",
            "Epoch [154/200], Batch [75/104], Loss: 0.0071\n",
            "Epoch [154/200], Batch [80/104], Loss: 0.0049\n",
            "Epoch [154/200], Batch [85/104], Loss: 0.0015\n",
            "Epoch [154/200], Batch [90/104], Loss: 0.0024\n",
            "Epoch [154/200], Batch [95/104], Loss: 0.0049\n",
            "Epoch [154/200], Batch [100/104], Loss: 0.0032\n",
            "Epoch 154: Train Loss = 0.4100, Accuracy = 100.00%\n",
            "Epoch [155/200], Batch [5/104], Loss: 0.0054\n",
            "Epoch [155/200], Batch [10/104], Loss: 0.0004\n",
            "Epoch [155/200], Batch [15/104], Loss: 0.0037\n",
            "Epoch [155/200], Batch [20/104], Loss: 0.0015\n",
            "Epoch [155/200], Batch [25/104], Loss: 0.0069\n",
            "Epoch [155/200], Batch [30/104], Loss: 0.0039\n",
            "Epoch [155/200], Batch [35/104], Loss: 0.0028\n",
            "Epoch [155/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [155/200], Batch [45/104], Loss: 0.0019\n",
            "Epoch [155/200], Batch [50/104], Loss: 0.0056\n",
            "Epoch [155/200], Batch [55/104], Loss: 0.0022\n",
            "Epoch [155/200], Batch [60/104], Loss: 0.0030\n",
            "Epoch [155/200], Batch [65/104], Loss: 0.0047\n",
            "Epoch [155/200], Batch [70/104], Loss: 0.0026\n",
            "Epoch [155/200], Batch [75/104], Loss: 0.0032\n",
            "Epoch [155/200], Batch [80/104], Loss: 0.0034\n",
            "Epoch [155/200], Batch [85/104], Loss: 0.0015\n",
            "Epoch [155/200], Batch [90/104], Loss: 0.0098\n",
            "Epoch [155/200], Batch [95/104], Loss: 0.0020\n",
            "Epoch [155/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 155: Train Loss = 0.4054, Accuracy = 100.00%\n",
            "Epoch [156/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [156/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [156/200], Batch [15/104], Loss: 0.0038\n",
            "Epoch [156/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [156/200], Batch [25/104], Loss: 0.0055\n",
            "Epoch [156/200], Batch [30/104], Loss: 0.0062\n",
            "Epoch [156/200], Batch [35/104], Loss: 0.0088\n",
            "Epoch [156/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [156/200], Batch [45/104], Loss: 0.0012\n",
            "Epoch [156/200], Batch [50/104], Loss: 0.0068\n",
            "Epoch [156/200], Batch [55/104], Loss: 0.0070\n",
            "Epoch [156/200], Batch [60/104], Loss: 0.0030\n",
            "Epoch [156/200], Batch [65/104], Loss: 0.0020\n",
            "Epoch [156/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [156/200], Batch [75/104], Loss: 0.0014\n",
            "Epoch [156/200], Batch [80/104], Loss: 0.0037\n",
            "Epoch [156/200], Batch [85/104], Loss: 0.0016\n",
            "Epoch [156/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [156/200], Batch [95/104], Loss: 0.0035\n",
            "Epoch [156/200], Batch [100/104], Loss: 0.0012\n",
            "Epoch 156: Train Loss = 0.4068, Accuracy = 100.00%\n",
            "Epoch [157/200], Batch [5/104], Loss: 0.0026\n",
            "Epoch [157/200], Batch [10/104], Loss: 0.0092\n",
            "Epoch [157/200], Batch [15/104], Loss: 0.0020\n",
            "Epoch [157/200], Batch [20/104], Loss: 0.0048\n",
            "Epoch [157/200], Batch [25/104], Loss: 0.0064\n",
            "Epoch [157/200], Batch [30/104], Loss: 0.0054\n",
            "Epoch [157/200], Batch [35/104], Loss: 0.0038\n",
            "Epoch [157/200], Batch [40/104], Loss: 0.0071\n",
            "Epoch [157/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [157/200], Batch [50/104], Loss: 0.0041\n",
            "Epoch [157/200], Batch [55/104], Loss: 0.0045\n",
            "Epoch [157/200], Batch [60/104], Loss: 0.0051\n",
            "Epoch [157/200], Batch [65/104], Loss: 0.0013\n",
            "Epoch [157/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [157/200], Batch [75/104], Loss: 0.0025\n",
            "Epoch [157/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [157/200], Batch [85/104], Loss: 0.0065\n",
            "Epoch [157/200], Batch [90/104], Loss: 0.0053\n",
            "Epoch [157/200], Batch [95/104], Loss: 0.0048\n",
            "Epoch [157/200], Batch [100/104], Loss: 0.0011\n",
            "Epoch 157: Train Loss = 0.4042, Accuracy = 100.00%\n",
            "Epoch [158/200], Batch [5/104], Loss: 0.0018\n",
            "Epoch [158/200], Batch [10/104], Loss: 0.0067\n",
            "Epoch [158/200], Batch [15/104], Loss: 0.0032\n",
            "Epoch [158/200], Batch [20/104], Loss: 0.0057\n",
            "Epoch [158/200], Batch [25/104], Loss: 0.0044\n",
            "Epoch [158/200], Batch [30/104], Loss: 0.0049\n",
            "Epoch [158/200], Batch [35/104], Loss: 0.0029\n",
            "Epoch [158/200], Batch [40/104], Loss: 0.0033\n",
            "Epoch [158/200], Batch [45/104], Loss: 0.0041\n",
            "Epoch [158/200], Batch [50/104], Loss: 0.0042\n",
            "Epoch [158/200], Batch [55/104], Loss: 0.0019\n",
            "Epoch [158/200], Batch [60/104], Loss: 0.0041\n",
            "Epoch [158/200], Batch [65/104], Loss: 0.0008\n",
            "Epoch [158/200], Batch [70/104], Loss: 0.0048\n",
            "Epoch [158/200], Batch [75/104], Loss: 0.0049\n",
            "Epoch [158/200], Batch [80/104], Loss: 0.0034\n",
            "Epoch [158/200], Batch [85/104], Loss: 0.0086\n",
            "Epoch [158/200], Batch [90/104], Loss: 0.0025\n",
            "Epoch [158/200], Batch [95/104], Loss: 0.0017\n",
            "Epoch [158/200], Batch [100/104], Loss: 0.0037\n",
            "Epoch 158: Train Loss = 0.4025, Accuracy = 100.00%\n",
            "Epoch [159/200], Batch [5/104], Loss: 0.0066\n",
            "Epoch [159/200], Batch [10/104], Loss: 0.0036\n",
            "Epoch [159/200], Batch [15/104], Loss: 0.0034\n",
            "Epoch [159/200], Batch [20/104], Loss: 0.0069\n",
            "Epoch [159/200], Batch [25/104], Loss: 0.0034\n",
            "Epoch [159/200], Batch [30/104], Loss: 0.0048\n",
            "Epoch [159/200], Batch [35/104], Loss: 0.0014\n",
            "Epoch [159/200], Batch [40/104], Loss: 0.0040\n",
            "Epoch [159/200], Batch [45/104], Loss: 0.0019\n",
            "Epoch [159/200], Batch [50/104], Loss: 0.0042\n",
            "Epoch [159/200], Batch [55/104], Loss: 0.0048\n",
            "Epoch [159/200], Batch [60/104], Loss: 0.0017\n",
            "Epoch [159/200], Batch [65/104], Loss: 0.0057\n",
            "Epoch [159/200], Batch [70/104], Loss: 0.0082\n",
            "Epoch [159/200], Batch [75/104], Loss: 0.0044\n",
            "Epoch [159/200], Batch [80/104], Loss: 0.0019\n",
            "Epoch [159/200], Batch [85/104], Loss: 0.0029\n",
            "Epoch [159/200], Batch [90/104], Loss: 0.0029\n",
            "Epoch [159/200], Batch [95/104], Loss: 0.0016\n",
            "Epoch [159/200], Batch [100/104], Loss: 0.0047\n",
            "Epoch 159: Train Loss = 0.4142, Accuracy = 100.00%\n",
            "Epoch [160/200], Batch [5/104], Loss: 0.0032\n",
            "Epoch [160/200], Batch [10/104], Loss: 0.0032\n",
            "Epoch [160/200], Batch [15/104], Loss: 0.0046\n",
            "Epoch [160/200], Batch [20/104], Loss: 0.0019\n",
            "Epoch [160/200], Batch [25/104], Loss: 0.0035\n",
            "Epoch [160/200], Batch [30/104], Loss: 0.0052\n",
            "Epoch [160/200], Batch [35/104], Loss: 0.0042\n",
            "Epoch [160/200], Batch [40/104], Loss: 0.0035\n",
            "Epoch [160/200], Batch [45/104], Loss: 0.0037\n",
            "Epoch [160/200], Batch [50/104], Loss: 0.0025\n",
            "Epoch [160/200], Batch [55/104], Loss: 0.0022\n",
            "Epoch [160/200], Batch [60/104], Loss: 0.0032\n",
            "Epoch [160/200], Batch [65/104], Loss: 0.0028\n",
            "Epoch [160/200], Batch [70/104], Loss: 0.0033\n",
            "Epoch [160/200], Batch [75/104], Loss: 0.0043\n",
            "Epoch [160/200], Batch [80/104], Loss: 0.0025\n",
            "Epoch [160/200], Batch [85/104], Loss: 0.0045\n",
            "Epoch [160/200], Batch [90/104], Loss: 0.0054\n",
            "Epoch [160/200], Batch [95/104], Loss: 0.0030\n",
            "Epoch [160/200], Batch [100/104], Loss: 0.0007\n",
            "Epoch 160: Train Loss = 0.4074, Accuracy = 100.00%\n",
            "Epoch [161/200], Batch [5/104], Loss: 0.0027\n",
            "Epoch [161/200], Batch [10/104], Loss: 0.0072\n",
            "Epoch [161/200], Batch [15/104], Loss: 0.0042\n",
            "Epoch [161/200], Batch [20/104], Loss: 0.0016\n",
            "Epoch [161/200], Batch [25/104], Loss: 0.0061\n",
            "Epoch [161/200], Batch [30/104], Loss: 0.0056\n",
            "Epoch [161/200], Batch [35/104], Loss: 0.0027\n",
            "Epoch [161/200], Batch [40/104], Loss: 0.0046\n",
            "Epoch [161/200], Batch [45/104], Loss: 0.0026\n",
            "Epoch [161/200], Batch [50/104], Loss: 0.0020\n",
            "Epoch [161/200], Batch [55/104], Loss: 0.0027\n",
            "Epoch [161/200], Batch [60/104], Loss: 0.0082\n",
            "Epoch [161/200], Batch [65/104], Loss: 0.0033\n",
            "Epoch [161/200], Batch [70/104], Loss: 0.0064\n",
            "Epoch [161/200], Batch [75/104], Loss: 0.0045\n",
            "Epoch [161/200], Batch [80/104], Loss: 0.0031\n",
            "Epoch [161/200], Batch [85/104], Loss: 0.0035\n",
            "Epoch [161/200], Batch [90/104], Loss: 0.0042\n",
            "Epoch [161/200], Batch [95/104], Loss: 0.0032\n",
            "Epoch [161/200], Batch [100/104], Loss: 0.0023\n",
            "Epoch 161: Train Loss = 0.4042, Accuracy = 100.00%\n",
            "Epoch [162/200], Batch [5/104], Loss: 0.0011\n",
            "Epoch [162/200], Batch [10/104], Loss: 0.0041\n",
            "Epoch [162/200], Batch [15/104], Loss: 0.0028\n",
            "Epoch [162/200], Batch [20/104], Loss: 0.0079\n",
            "Epoch [162/200], Batch [25/104], Loss: 0.0037\n",
            "Epoch [162/200], Batch [30/104], Loss: 0.0045\n",
            "Epoch [162/200], Batch [35/104], Loss: 0.0047\n",
            "Epoch [162/200], Batch [40/104], Loss: 0.0036\n",
            "Epoch [162/200], Batch [45/104], Loss: 0.0082\n",
            "Epoch [162/200], Batch [50/104], Loss: 0.0016\n",
            "Epoch [162/200], Batch [55/104], Loss: 0.0053\n",
            "Epoch [162/200], Batch [60/104], Loss: 0.0033\n",
            "Epoch [162/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [162/200], Batch [70/104], Loss: 0.0023\n",
            "Epoch [162/200], Batch [75/104], Loss: 0.0037\n",
            "Epoch [162/200], Batch [80/104], Loss: 0.0092\n",
            "Epoch [162/200], Batch [85/104], Loss: 0.0017\n",
            "Epoch [162/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [162/200], Batch [95/104], Loss: 0.0035\n",
            "Epoch [162/200], Batch [100/104], Loss: 0.0013\n",
            "Epoch 162: Train Loss = 0.3990, Accuracy = 100.00%\n",
            "Epoch [163/200], Batch [5/104], Loss: 0.0053\n",
            "Epoch [163/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [163/200], Batch [15/104], Loss: 0.0035\n",
            "Epoch [163/200], Batch [20/104], Loss: 0.0027\n",
            "Epoch [163/200], Batch [25/104], Loss: 0.0027\n",
            "Epoch [163/200], Batch [30/104], Loss: 0.0026\n",
            "Epoch [163/200], Batch [35/104], Loss: 0.0022\n",
            "Epoch [163/200], Batch [40/104], Loss: 0.0018\n",
            "Epoch [163/200], Batch [45/104], Loss: 0.0032\n",
            "Epoch [163/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [163/200], Batch [55/104], Loss: 0.0092\n",
            "Epoch [163/200], Batch [60/104], Loss: 0.0081\n",
            "Epoch [163/200], Batch [65/104], Loss: 0.0029\n",
            "Epoch [163/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [163/200], Batch [75/104], Loss: 0.0022\n",
            "Epoch [163/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [163/200], Batch [85/104], Loss: 0.0019\n",
            "Epoch [163/200], Batch [90/104], Loss: 0.0041\n",
            "Epoch [163/200], Batch [95/104], Loss: 0.0031\n",
            "Epoch [163/200], Batch [100/104], Loss: 0.0045\n",
            "Epoch 163: Train Loss = 0.3999, Accuracy = 100.00%\n",
            "Epoch [164/200], Batch [5/104], Loss: 0.0043\n",
            "Epoch [164/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [164/200], Batch [15/104], Loss: 0.0080\n",
            "Epoch [164/200], Batch [20/104], Loss: 0.0025\n",
            "Epoch [164/200], Batch [25/104], Loss: 0.0014\n",
            "Epoch [164/200], Batch [30/104], Loss: 0.0047\n",
            "Epoch [164/200], Batch [35/104], Loss: 0.0049\n",
            "Epoch [164/200], Batch [40/104], Loss: 0.0022\n",
            "Epoch [164/200], Batch [45/104], Loss: 0.0028\n",
            "Epoch [164/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [164/200], Batch [55/104], Loss: 0.0033\n",
            "Epoch [164/200], Batch [60/104], Loss: 0.0054\n",
            "Epoch [164/200], Batch [65/104], Loss: 0.0076\n",
            "Epoch [164/200], Batch [70/104], Loss: 0.0029\n",
            "Epoch [164/200], Batch [75/104], Loss: 0.0037\n",
            "Epoch [164/200], Batch [80/104], Loss: 0.0041\n",
            "Epoch [164/200], Batch [85/104], Loss: 0.0025\n",
            "Epoch [164/200], Batch [90/104], Loss: 0.0027\n",
            "Epoch [164/200], Batch [95/104], Loss: 0.0024\n",
            "Epoch [164/200], Batch [100/104], Loss: 0.0031\n",
            "Epoch 164: Train Loss = 0.4033, Accuracy = 100.00%\n",
            "Epoch [165/200], Batch [5/104], Loss: 0.0061\n",
            "Epoch [165/200], Batch [10/104], Loss: 0.0038\n",
            "Epoch [165/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [165/200], Batch [20/104], Loss: 0.0057\n",
            "Epoch [165/200], Batch [25/104], Loss: 0.0021\n",
            "Epoch [165/200], Batch [30/104], Loss: 0.0033\n",
            "Epoch [165/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [165/200], Batch [40/104], Loss: 0.0012\n",
            "Epoch [165/200], Batch [45/104], Loss: 0.0026\n",
            "Epoch [165/200], Batch [50/104], Loss: 0.0039\n",
            "Epoch [165/200], Batch [55/104], Loss: 0.0027\n",
            "Epoch [165/200], Batch [60/104], Loss: 0.0033\n",
            "Epoch [165/200], Batch [65/104], Loss: 0.0037\n",
            "Epoch [165/200], Batch [70/104], Loss: 0.0108\n",
            "Epoch [165/200], Batch [75/104], Loss: 0.0036\n",
            "Epoch [165/200], Batch [80/104], Loss: 0.0009\n",
            "Epoch [165/200], Batch [85/104], Loss: 0.0034\n",
            "Epoch [165/200], Batch [90/104], Loss: 0.0033\n",
            "Epoch [165/200], Batch [95/104], Loss: 0.0071\n",
            "Epoch [165/200], Batch [100/104], Loss: 0.0013\n",
            "Epoch 165: Train Loss = 0.3974, Accuracy = 100.00%\n",
            "Epoch [166/200], Batch [5/104], Loss: 0.0062\n",
            "Epoch [166/200], Batch [10/104], Loss: 0.0041\n",
            "Epoch [166/200], Batch [15/104], Loss: 0.0030\n",
            "Epoch [166/200], Batch [20/104], Loss: 0.0045\n",
            "Epoch [166/200], Batch [25/104], Loss: 0.0023\n",
            "Epoch [166/200], Batch [30/104], Loss: 0.0013\n",
            "Epoch [166/200], Batch [35/104], Loss: 0.0080\n",
            "Epoch [166/200], Batch [40/104], Loss: 0.0057\n",
            "Epoch [166/200], Batch [45/104], Loss: 0.0018\n",
            "Epoch [166/200], Batch [50/104], Loss: 0.0038\n",
            "Epoch [166/200], Batch [55/104], Loss: 0.0067\n",
            "Epoch [166/200], Batch [60/104], Loss: 0.0037\n",
            "Epoch [166/200], Batch [65/104], Loss: 0.0056\n",
            "Epoch [166/200], Batch [70/104], Loss: 0.0043\n",
            "Epoch [166/200], Batch [75/104], Loss: 0.0023\n",
            "Epoch [166/200], Batch [80/104], Loss: 0.0046\n",
            "Epoch [166/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [166/200], Batch [90/104], Loss: 0.0040\n",
            "Epoch [166/200], Batch [95/104], Loss: 0.0071\n",
            "Epoch [166/200], Batch [100/104], Loss: 0.0058\n",
            "Epoch 166: Train Loss = 0.3958, Accuracy = 100.00%\n",
            "Epoch [167/200], Batch [5/104], Loss: 0.0016\n",
            "Epoch [167/200], Batch [10/104], Loss: 0.0039\n",
            "Epoch [167/200], Batch [15/104], Loss: 0.0014\n",
            "Epoch [167/200], Batch [20/104], Loss: 0.0073\n",
            "Epoch [167/200], Batch [25/104], Loss: 0.0043\n",
            "Epoch [167/200], Batch [30/104], Loss: 0.0058\n",
            "Epoch [167/200], Batch [35/104], Loss: 0.0052\n",
            "Epoch [167/200], Batch [40/104], Loss: 0.0018\n",
            "Epoch [167/200], Batch [45/104], Loss: 0.0016\n",
            "Epoch [167/200], Batch [50/104], Loss: 0.0037\n",
            "Epoch [167/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [167/200], Batch [60/104], Loss: 0.0065\n",
            "Epoch [167/200], Batch [65/104], Loss: 0.0029\n",
            "Epoch [167/200], Batch [70/104], Loss: 0.0040\n",
            "Epoch [167/200], Batch [75/104], Loss: 0.0014\n",
            "Epoch [167/200], Batch [80/104], Loss: 0.0055\n",
            "Epoch [167/200], Batch [85/104], Loss: 0.0056\n",
            "Epoch [167/200], Batch [90/104], Loss: 0.0054\n",
            "Epoch [167/200], Batch [95/104], Loss: 0.0016\n",
            "Epoch [167/200], Batch [100/104], Loss: 0.0081\n",
            "Epoch 167: Train Loss = 0.4063, Accuracy = 100.00%\n",
            "Epoch [168/200], Batch [5/104], Loss: 0.0011\n",
            "Epoch [168/200], Batch [10/104], Loss: 0.0013\n",
            "Epoch [168/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [168/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [168/200], Batch [25/104], Loss: 0.0054\n",
            "Epoch [168/200], Batch [30/104], Loss: 0.0057\n",
            "Epoch [168/200], Batch [35/104], Loss: 0.0051\n",
            "Epoch [168/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [168/200], Batch [45/104], Loss: 0.0050\n",
            "Epoch [168/200], Batch [50/104], Loss: 0.0083\n",
            "Epoch [168/200], Batch [55/104], Loss: 0.0012\n",
            "Epoch [168/200], Batch [60/104], Loss: 0.0032\n",
            "Epoch [168/200], Batch [65/104], Loss: 0.0025\n",
            "Epoch [168/200], Batch [70/104], Loss: 0.0066\n",
            "Epoch [168/200], Batch [75/104], Loss: 0.0029\n",
            "Epoch [168/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [168/200], Batch [85/104], Loss: 0.0038\n",
            "Epoch [168/200], Batch [90/104], Loss: 0.0041\n",
            "Epoch [168/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [168/200], Batch [100/104], Loss: 0.0023\n",
            "Epoch 168: Train Loss = 0.3975, Accuracy = 100.00%\n",
            "Epoch [169/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [169/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [169/200], Batch [15/104], Loss: 0.0049\n",
            "Epoch [169/200], Batch [20/104], Loss: 0.0035\n",
            "Epoch [169/200], Batch [25/104], Loss: 0.0057\n",
            "Epoch [169/200], Batch [30/104], Loss: 0.0055\n",
            "Epoch [169/200], Batch [35/104], Loss: 0.0030\n",
            "Epoch [169/200], Batch [40/104], Loss: 0.0007\n",
            "Epoch [169/200], Batch [45/104], Loss: 0.0070\n",
            "Epoch [169/200], Batch [50/104], Loss: 0.0069\n",
            "Epoch [169/200], Batch [55/104], Loss: 0.0049\n",
            "Epoch [169/200], Batch [60/104], Loss: 0.0014\n",
            "Epoch [169/200], Batch [65/104], Loss: 0.0056\n",
            "Epoch [169/200], Batch [70/104], Loss: 0.0043\n",
            "Epoch [169/200], Batch [75/104], Loss: 0.0055\n",
            "Epoch [169/200], Batch [80/104], Loss: 0.0042\n",
            "Epoch [169/200], Batch [85/104], Loss: 0.0054\n",
            "Epoch [169/200], Batch [90/104], Loss: 0.0048\n",
            "Epoch [169/200], Batch [95/104], Loss: 0.0006\n",
            "Epoch [169/200], Batch [100/104], Loss: 0.0055\n",
            "Epoch 169: Train Loss = 0.3949, Accuracy = 100.00%\n",
            "Epoch [170/200], Batch [5/104], Loss: 0.0021\n",
            "Epoch [170/200], Batch [10/104], Loss: 0.0073\n",
            "Epoch [170/200], Batch [15/104], Loss: 0.0077\n",
            "Epoch [170/200], Batch [20/104], Loss: 0.0036\n",
            "Epoch [170/200], Batch [25/104], Loss: 0.0041\n",
            "Epoch [170/200], Batch [30/104], Loss: 0.0031\n",
            "Epoch [170/200], Batch [35/104], Loss: 0.0053\n",
            "Epoch [170/200], Batch [40/104], Loss: 0.0039\n",
            "Epoch [170/200], Batch [45/104], Loss: 0.0037\n",
            "Epoch [170/200], Batch [50/104], Loss: 0.0069\n",
            "Epoch [170/200], Batch [55/104], Loss: 0.0020\n",
            "Epoch [170/200], Batch [60/104], Loss: 0.0069\n",
            "Epoch [170/200], Batch [65/104], Loss: 0.0017\n",
            "Epoch [170/200], Batch [70/104], Loss: 0.0042\n",
            "Epoch [170/200], Batch [75/104], Loss: 0.0050\n",
            "Epoch [170/200], Batch [80/104], Loss: 0.0052\n",
            "Epoch [170/200], Batch [85/104], Loss: 0.0055\n",
            "Epoch [170/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [170/200], Batch [95/104], Loss: 0.0054\n",
            "Epoch [170/200], Batch [100/104], Loss: 0.0016\n",
            "Epoch 170: Train Loss = 0.3959, Accuracy = 100.00%\n",
            "Epoch [171/200], Batch [5/104], Loss: 0.0031\n",
            "Epoch [171/200], Batch [10/104], Loss: 0.0020\n",
            "Epoch [171/200], Batch [15/104], Loss: 0.0083\n",
            "Epoch [171/200], Batch [20/104], Loss: 0.0027\n",
            "Epoch [171/200], Batch [25/104], Loss: 0.0025\n",
            "Epoch [171/200], Batch [30/104], Loss: 0.0066\n",
            "Epoch [171/200], Batch [35/104], Loss: 0.0037\n",
            "Epoch [171/200], Batch [40/104], Loss: 0.0039\n",
            "Epoch [171/200], Batch [45/104], Loss: 0.0023\n",
            "Epoch [171/200], Batch [50/104], Loss: 0.0072\n",
            "Epoch [171/200], Batch [55/104], Loss: 0.0047\n",
            "Epoch [171/200], Batch [60/104], Loss: 0.0020\n",
            "Epoch [171/200], Batch [65/104], Loss: 0.0047\n",
            "Epoch [171/200], Batch [70/104], Loss: 0.0034\n",
            "Epoch [171/200], Batch [75/104], Loss: 0.0044\n",
            "Epoch [171/200], Batch [80/104], Loss: 0.0041\n",
            "Epoch [171/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [171/200], Batch [90/104], Loss: 0.0054\n",
            "Epoch [171/200], Batch [95/104], Loss: 0.0027\n",
            "Epoch [171/200], Batch [100/104], Loss: 0.0058\n",
            "Epoch 171: Train Loss = 0.4035, Accuracy = 100.00%\n",
            "Epoch [172/200], Batch [5/104], Loss: 0.0032\n",
            "Epoch [172/200], Batch [10/104], Loss: 0.0078\n",
            "Epoch [172/200], Batch [15/104], Loss: 0.0071\n",
            "Epoch [172/200], Batch [20/104], Loss: 0.0061\n",
            "Epoch [172/200], Batch [25/104], Loss: 0.0031\n",
            "Epoch [172/200], Batch [30/104], Loss: 0.0049\n",
            "Epoch [172/200], Batch [35/104], Loss: 0.0036\n",
            "Epoch [172/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [172/200], Batch [45/104], Loss: 0.0047\n",
            "Epoch [172/200], Batch [50/104], Loss: 0.0056\n",
            "Epoch [172/200], Batch [55/104], Loss: 0.0049\n",
            "Epoch [172/200], Batch [60/104], Loss: 0.0048\n",
            "Epoch [172/200], Batch [65/104], Loss: 0.0039\n",
            "Epoch [172/200], Batch [70/104], Loss: 0.0022\n",
            "Epoch [172/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [172/200], Batch [80/104], Loss: 0.0048\n",
            "Epoch [172/200], Batch [85/104], Loss: 0.0022\n",
            "Epoch [172/200], Batch [90/104], Loss: 0.0023\n",
            "Epoch [172/200], Batch [95/104], Loss: 0.0029\n",
            "Epoch [172/200], Batch [100/104], Loss: 0.0045\n",
            "Epoch 172: Train Loss = 0.3950, Accuracy = 100.00%\n",
            "Epoch [173/200], Batch [5/104], Loss: 0.0043\n",
            "Epoch [173/200], Batch [10/104], Loss: 0.0041\n",
            "Epoch [173/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [173/200], Batch [20/104], Loss: 0.0042\n",
            "Epoch [173/200], Batch [25/104], Loss: 0.0021\n",
            "Epoch [173/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [173/200], Batch [35/104], Loss: 0.0052\n",
            "Epoch [173/200], Batch [40/104], Loss: 0.0058\n",
            "Epoch [173/200], Batch [45/104], Loss: 0.0014\n",
            "Epoch [173/200], Batch [50/104], Loss: 0.0025\n",
            "Epoch [173/200], Batch [55/104], Loss: 0.0095\n",
            "Epoch [173/200], Batch [60/104], Loss: 0.0068\n",
            "Epoch [173/200], Batch [65/104], Loss: 0.0099\n",
            "Epoch [173/200], Batch [70/104], Loss: 0.0015\n",
            "Epoch [173/200], Batch [75/104], Loss: 0.0022\n",
            "Epoch [173/200], Batch [80/104], Loss: 0.0039\n",
            "Epoch [173/200], Batch [85/104], Loss: 0.0035\n",
            "Epoch [173/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [173/200], Batch [95/104], Loss: 0.0029\n",
            "Epoch [173/200], Batch [100/104], Loss: 0.0022\n",
            "Epoch 173: Train Loss = 0.4041, Accuracy = 100.00%\n",
            "Epoch [174/200], Batch [5/104], Loss: 0.0059\n",
            "Epoch [174/200], Batch [10/104], Loss: 0.0071\n",
            "Epoch [174/200], Batch [15/104], Loss: 0.0045\n",
            "Epoch [174/200], Batch [20/104], Loss: 0.0032\n",
            "Epoch [174/200], Batch [25/104], Loss: 0.0013\n",
            "Epoch [174/200], Batch [30/104], Loss: 0.0053\n",
            "Epoch [174/200], Batch [35/104], Loss: 0.0038\n",
            "Epoch [174/200], Batch [40/104], Loss: 0.0075\n",
            "Epoch [174/200], Batch [45/104], Loss: 0.0055\n",
            "Epoch [174/200], Batch [50/104], Loss: 0.0014\n",
            "Epoch [174/200], Batch [55/104], Loss: 0.0059\n",
            "Epoch [174/200], Batch [60/104], Loss: 0.0038\n",
            "Epoch [174/200], Batch [65/104], Loss: 0.0026\n",
            "Epoch [174/200], Batch [70/104], Loss: 0.0040\n",
            "Epoch [174/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [174/200], Batch [80/104], Loss: 0.0025\n",
            "Epoch [174/200], Batch [85/104], Loss: 0.0018\n",
            "Epoch [174/200], Batch [90/104], Loss: 0.0055\n",
            "Epoch [174/200], Batch [95/104], Loss: 0.0071\n",
            "Epoch [174/200], Batch [100/104], Loss: 0.0046\n",
            "Epoch 174: Train Loss = 0.3951, Accuracy = 100.00%\n",
            "Epoch [175/200], Batch [5/104], Loss: 0.0028\n",
            "Epoch [175/200], Batch [10/104], Loss: 0.0042\n",
            "Epoch [175/200], Batch [15/104], Loss: 0.0050\n",
            "Epoch [175/200], Batch [20/104], Loss: 0.0036\n",
            "Epoch [175/200], Batch [25/104], Loss: 0.0024\n",
            "Epoch [175/200], Batch [30/104], Loss: 0.0053\n",
            "Epoch [175/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [175/200], Batch [40/104], Loss: 0.0053\n",
            "Epoch [175/200], Batch [45/104], Loss: 0.0025\n",
            "Epoch [175/200], Batch [50/104], Loss: 0.0074\n",
            "Epoch [175/200], Batch [55/104], Loss: 0.0057\n",
            "Epoch [175/200], Batch [60/104], Loss: 0.0041\n",
            "Epoch [175/200], Batch [65/104], Loss: 0.0012\n",
            "Epoch [175/200], Batch [70/104], Loss: 0.0043\n",
            "Epoch [175/200], Batch [75/104], Loss: 0.0033\n",
            "Epoch [175/200], Batch [80/104], Loss: 0.0033\n",
            "Epoch [175/200], Batch [85/104], Loss: 0.0071\n",
            "Epoch [175/200], Batch [90/104], Loss: 0.0057\n",
            "Epoch [175/200], Batch [95/104], Loss: 0.0037\n",
            "Epoch [175/200], Batch [100/104], Loss: 0.0055\n",
            "Epoch 175: Train Loss = 0.3988, Accuracy = 100.00%\n",
            "Epoch [176/200], Batch [5/104], Loss: 0.0047\n",
            "Epoch [176/200], Batch [10/104], Loss: 0.0068\n",
            "Epoch [176/200], Batch [15/104], Loss: 0.0031\n",
            "Epoch [176/200], Batch [20/104], Loss: 0.0063\n",
            "Epoch [176/200], Batch [25/104], Loss: 0.0022\n",
            "Epoch [176/200], Batch [30/104], Loss: 0.0034\n",
            "Epoch [176/200], Batch [35/104], Loss: 0.0028\n",
            "Epoch [176/200], Batch [40/104], Loss: 0.0019\n",
            "Epoch [176/200], Batch [45/104], Loss: 0.0079\n",
            "Epoch [176/200], Batch [50/104], Loss: 0.0054\n",
            "Epoch [176/200], Batch [55/104], Loss: 0.0059\n",
            "Epoch [176/200], Batch [60/104], Loss: 0.0053\n",
            "Epoch [176/200], Batch [65/104], Loss: 0.0053\n",
            "Epoch [176/200], Batch [70/104], Loss: 0.0023\n",
            "Epoch [176/200], Batch [75/104], Loss: 0.0078\n",
            "Epoch [176/200], Batch [80/104], Loss: 0.0060\n",
            "Epoch [176/200], Batch [85/104], Loss: 0.0029\n",
            "Epoch [176/200], Batch [90/104], Loss: 0.0046\n",
            "Epoch [176/200], Batch [95/104], Loss: 0.0009\n",
            "Epoch [176/200], Batch [100/104], Loss: 0.0009\n",
            "Epoch 176: Train Loss = 0.3958, Accuracy = 100.00%\n",
            "Epoch [177/200], Batch [5/104], Loss: 0.0017\n",
            "Epoch [177/200], Batch [10/104], Loss: 0.0047\n",
            "Epoch [177/200], Batch [15/104], Loss: 0.0048\n",
            "Epoch [177/200], Batch [20/104], Loss: 0.0019\n",
            "Epoch [177/200], Batch [25/104], Loss: 0.0031\n",
            "Epoch [177/200], Batch [30/104], Loss: 0.0018\n",
            "Epoch [177/200], Batch [35/104], Loss: 0.0019\n",
            "Epoch [177/200], Batch [40/104], Loss: 0.0063\n",
            "Epoch [177/200], Batch [45/104], Loss: 0.0035\n",
            "Epoch [177/200], Batch [50/104], Loss: 0.0009\n",
            "Epoch [177/200], Batch [55/104], Loss: 0.0042\n",
            "Epoch [177/200], Batch [60/104], Loss: 0.0043\n",
            "Epoch [177/200], Batch [65/104], Loss: 0.0047\n",
            "Epoch [177/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [177/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [177/200], Batch [80/104], Loss: 0.0043\n",
            "Epoch [177/200], Batch [85/104], Loss: 0.0043\n",
            "Epoch [177/200], Batch [90/104], Loss: 0.0029\n",
            "Epoch [177/200], Batch [95/104], Loss: 0.0056\n",
            "Epoch [177/200], Batch [100/104], Loss: 0.0014\n",
            "Epoch 177: Train Loss = 0.3983, Accuracy = 100.00%\n",
            "Epoch [178/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [178/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [178/200], Batch [15/104], Loss: 0.0028\n",
            "Epoch [178/200], Batch [20/104], Loss: 0.0030\n",
            "Epoch [178/200], Batch [25/104], Loss: 0.0056\n",
            "Epoch [178/200], Batch [30/104], Loss: 0.0094\n",
            "Epoch [178/200], Batch [35/104], Loss: 0.0023\n",
            "Epoch [178/200], Batch [40/104], Loss: 0.0037\n",
            "Epoch [178/200], Batch [45/104], Loss: 0.0028\n",
            "Epoch [178/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [178/200], Batch [55/104], Loss: 0.0020\n",
            "Epoch [178/200], Batch [60/104], Loss: 0.0040\n",
            "Epoch [178/200], Batch [65/104], Loss: 0.0009\n",
            "Epoch [178/200], Batch [70/104], Loss: 0.0066\n",
            "Epoch [178/200], Batch [75/104], Loss: 0.0043\n",
            "Epoch [178/200], Batch [80/104], Loss: 0.0064\n",
            "Epoch [178/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [178/200], Batch [90/104], Loss: 0.0030\n",
            "Epoch [178/200], Batch [95/104], Loss: 0.0041\n",
            "Epoch [178/200], Batch [100/104], Loss: 0.0029\n",
            "Epoch 178: Train Loss = 0.3938, Accuracy = 100.00%\n",
            "Epoch [179/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [179/200], Batch [10/104], Loss: 0.0011\n",
            "Epoch [179/200], Batch [15/104], Loss: 0.0018\n",
            "Epoch [179/200], Batch [20/104], Loss: 0.0039\n",
            "Epoch [179/200], Batch [25/104], Loss: 0.0088\n",
            "Epoch [179/200], Batch [30/104], Loss: 0.0013\n",
            "Epoch [179/200], Batch [35/104], Loss: 0.0023\n",
            "Epoch [179/200], Batch [40/104], Loss: 0.0062\n",
            "Epoch [179/200], Batch [45/104], Loss: 0.0042\n",
            "Epoch [179/200], Batch [50/104], Loss: 0.0028\n",
            "Epoch [179/200], Batch [55/104], Loss: 0.0037\n",
            "Epoch [179/200], Batch [60/104], Loss: 0.0051\n",
            "Epoch [179/200], Batch [65/104], Loss: 0.0040\n",
            "Epoch [179/200], Batch [70/104], Loss: 0.0019\n",
            "Epoch [179/200], Batch [75/104], Loss: 0.0019\n",
            "Epoch [179/200], Batch [80/104], Loss: 0.0012\n",
            "Epoch [179/200], Batch [85/104], Loss: 0.0014\n",
            "Epoch [179/200], Batch [90/104], Loss: 0.0044\n",
            "Epoch [179/200], Batch [95/104], Loss: 0.0097\n",
            "Epoch [179/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 179: Train Loss = 0.3942, Accuracy = 100.00%\n",
            "Epoch [180/200], Batch [5/104], Loss: 0.0040\n",
            "Epoch [180/200], Batch [10/104], Loss: 0.0027\n",
            "Epoch [180/200], Batch [15/104], Loss: 0.0031\n",
            "Epoch [180/200], Batch [20/104], Loss: 0.0021\n",
            "Epoch [180/200], Batch [25/104], Loss: 0.0033\n",
            "Epoch [180/200], Batch [30/104], Loss: 0.0040\n",
            "Epoch [180/200], Batch [35/104], Loss: 0.0069\n",
            "Epoch [180/200], Batch [40/104], Loss: 0.0027\n",
            "Epoch [180/200], Batch [45/104], Loss: 0.0060\n",
            "Epoch [180/200], Batch [50/104], Loss: 0.0037\n",
            "Epoch [180/200], Batch [55/104], Loss: 0.0013\n",
            "Epoch [180/200], Batch [60/104], Loss: 0.0050\n",
            "Epoch [180/200], Batch [65/104], Loss: 0.0038\n",
            "Epoch [180/200], Batch [70/104], Loss: 0.0038\n",
            "Epoch [180/200], Batch [75/104], Loss: 0.0084\n",
            "Epoch [180/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [180/200], Batch [85/104], Loss: 0.0024\n",
            "Epoch [180/200], Batch [90/104], Loss: 0.0028\n",
            "Epoch [180/200], Batch [95/104], Loss: 0.0046\n",
            "Epoch [180/200], Batch [100/104], Loss: 0.0017\n",
            "Epoch 180: Train Loss = 0.3953, Accuracy = 100.00%\n",
            "Epoch [181/200], Batch [5/104], Loss: 0.0057\n",
            "Epoch [181/200], Batch [10/104], Loss: 0.0064\n",
            "Epoch [181/200], Batch [15/104], Loss: 0.0022\n",
            "Epoch [181/200], Batch [20/104], Loss: 0.0032\n",
            "Epoch [181/200], Batch [25/104], Loss: 0.0051\n",
            "Epoch [181/200], Batch [30/104], Loss: 0.0025\n",
            "Epoch [181/200], Batch [35/104], Loss: 0.0025\n",
            "Epoch [181/200], Batch [40/104], Loss: 0.0017\n",
            "Epoch [181/200], Batch [45/104], Loss: 0.0022\n",
            "Epoch [181/200], Batch [50/104], Loss: 0.0048\n",
            "Epoch [181/200], Batch [55/104], Loss: 0.0032\n",
            "Epoch [181/200], Batch [60/104], Loss: 0.0033\n",
            "Epoch [181/200], Batch [65/104], Loss: 0.0048\n",
            "Epoch [181/200], Batch [70/104], Loss: 0.0069\n",
            "Epoch [181/200], Batch [75/104], Loss: 0.0032\n",
            "Epoch [181/200], Batch [80/104], Loss: 0.0031\n",
            "Epoch [181/200], Batch [85/104], Loss: 0.0007\n",
            "Epoch [181/200], Batch [90/104], Loss: 0.0058\n",
            "Epoch [181/200], Batch [95/104], Loss: 0.0021\n",
            "Epoch [181/200], Batch [100/104], Loss: 0.0024\n",
            "Epoch 181: Train Loss = 0.3935, Accuracy = 100.00%\n",
            "Epoch [182/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [182/200], Batch [10/104], Loss: 0.0057\n",
            "Epoch [182/200], Batch [15/104], Loss: 0.0024\n",
            "Epoch [182/200], Batch [20/104], Loss: 0.0038\n",
            "Epoch [182/200], Batch [25/104], Loss: 0.0029\n",
            "Epoch [182/200], Batch [30/104], Loss: 0.0038\n",
            "Epoch [182/200], Batch [35/104], Loss: 0.0041\n",
            "Epoch [182/200], Batch [40/104], Loss: 0.0055\n",
            "Epoch [182/200], Batch [45/104], Loss: 0.0092\n",
            "Epoch [182/200], Batch [50/104], Loss: 0.0068\n",
            "Epoch [182/200], Batch [55/104], Loss: 0.0045\n",
            "Epoch [182/200], Batch [60/104], Loss: 0.0052\n",
            "Epoch [182/200], Batch [65/104], Loss: 0.0023\n",
            "Epoch [182/200], Batch [70/104], Loss: 0.0019\n",
            "Epoch [182/200], Batch [75/104], Loss: 0.0074\n",
            "Epoch [182/200], Batch [80/104], Loss: 0.0018\n",
            "Epoch [182/200], Batch [85/104], Loss: 0.0033\n",
            "Epoch [182/200], Batch [90/104], Loss: 0.0010\n",
            "Epoch [182/200], Batch [95/104], Loss: 0.0042\n",
            "Epoch [182/200], Batch [100/104], Loss: 0.0042\n",
            "Epoch 182: Train Loss = 0.3960, Accuracy = 100.00%\n",
            "Epoch [183/200], Batch [5/104], Loss: 0.0072\n",
            "Epoch [183/200], Batch [10/104], Loss: 0.0045\n",
            "Epoch [183/200], Batch [15/104], Loss: 0.0044\n",
            "Epoch [183/200], Batch [20/104], Loss: 0.0071\n",
            "Epoch [183/200], Batch [25/104], Loss: 0.0037\n",
            "Epoch [183/200], Batch [30/104], Loss: 0.0035\n",
            "Epoch [183/200], Batch [35/104], Loss: 0.0015\n",
            "Epoch [183/200], Batch [40/104], Loss: 0.0019\n",
            "Epoch [183/200], Batch [45/104], Loss: 0.0059\n",
            "Epoch [183/200], Batch [50/104], Loss: 0.0036\n",
            "Epoch [183/200], Batch [55/104], Loss: 0.0026\n",
            "Epoch [183/200], Batch [60/104], Loss: 0.0024\n",
            "Epoch [183/200], Batch [65/104], Loss: 0.0027\n",
            "Epoch [183/200], Batch [70/104], Loss: 0.0027\n",
            "Epoch [183/200], Batch [75/104], Loss: 0.0038\n",
            "Epoch [183/200], Batch [80/104], Loss: 0.0029\n",
            "Epoch [183/200], Batch [85/104], Loss: 0.0019\n",
            "Epoch [183/200], Batch [90/104], Loss: 0.0058\n",
            "Epoch [183/200], Batch [95/104], Loss: 0.0031\n",
            "Epoch [183/200], Batch [100/104], Loss: 0.0032\n",
            "Epoch 183: Train Loss = 0.3945, Accuracy = 100.00%\n",
            "Epoch [184/200], Batch [5/104], Loss: 0.0028\n",
            "Epoch [184/200], Batch [10/104], Loss: 0.0013\n",
            "Epoch [184/200], Batch [15/104], Loss: 0.0077\n",
            "Epoch [184/200], Batch [20/104], Loss: 0.0064\n",
            "Epoch [184/200], Batch [25/104], Loss: 0.0044\n",
            "Epoch [184/200], Batch [30/104], Loss: 0.0031\n",
            "Epoch [184/200], Batch [35/104], Loss: 0.0027\n",
            "Epoch [184/200], Batch [40/104], Loss: 0.0053\n",
            "Epoch [184/200], Batch [45/104], Loss: 0.0012\n",
            "Epoch [184/200], Batch [50/104], Loss: 0.0075\n",
            "Epoch [184/200], Batch [55/104], Loss: 0.0021\n",
            "Epoch [184/200], Batch [60/104], Loss: 0.0054\n",
            "Epoch [184/200], Batch [65/104], Loss: 0.0016\n",
            "Epoch [184/200], Batch [70/104], Loss: 0.0053\n",
            "Epoch [184/200], Batch [75/104], Loss: 0.0032\n",
            "Epoch [184/200], Batch [80/104], Loss: 0.0032\n",
            "Epoch [184/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [184/200], Batch [90/104], Loss: 0.0069\n",
            "Epoch [184/200], Batch [95/104], Loss: 0.0094\n",
            "Epoch [184/200], Batch [100/104], Loss: 0.0020\n",
            "Epoch 184: Train Loss = 0.3938, Accuracy = 100.00%\n",
            "Epoch [185/200], Batch [5/104], Loss: 0.0008\n",
            "Epoch [185/200], Batch [10/104], Loss: 0.0022\n",
            "Epoch [185/200], Batch [15/104], Loss: 0.0058\n",
            "Epoch [185/200], Batch [20/104], Loss: 0.0034\n",
            "Epoch [185/200], Batch [25/104], Loss: 0.0048\n",
            "Epoch [185/200], Batch [30/104], Loss: 0.0010\n",
            "Epoch [185/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [185/200], Batch [40/104], Loss: 0.0043\n",
            "Epoch [185/200], Batch [45/104], Loss: 0.0033\n",
            "Epoch [185/200], Batch [50/104], Loss: 0.0020\n",
            "Epoch [185/200], Batch [55/104], Loss: 0.0045\n",
            "Epoch [185/200], Batch [60/104], Loss: 0.0024\n",
            "Epoch [185/200], Batch [65/104], Loss: 0.0042\n",
            "Epoch [185/200], Batch [70/104], Loss: 0.0020\n",
            "Epoch [185/200], Batch [75/104], Loss: 0.0043\n",
            "Epoch [185/200], Batch [80/104], Loss: 0.0028\n",
            "Epoch [185/200], Batch [85/104], Loss: 0.0037\n",
            "Epoch [185/200], Batch [90/104], Loss: 0.0026\n",
            "Epoch [185/200], Batch [95/104], Loss: 0.0070\n",
            "Epoch [185/200], Batch [100/104], Loss: 0.0034\n",
            "Epoch 185: Train Loss = 0.3949, Accuracy = 100.00%\n",
            "Epoch [186/200], Batch [5/104], Loss: 0.0029\n",
            "Epoch [186/200], Batch [10/104], Loss: 0.0015\n",
            "Epoch [186/200], Batch [15/104], Loss: 0.0036\n",
            "Epoch [186/200], Batch [20/104], Loss: 0.0025\n",
            "Epoch [186/200], Batch [25/104], Loss: 0.0033\n",
            "Epoch [186/200], Batch [30/104], Loss: 0.0043\n",
            "Epoch [186/200], Batch [35/104], Loss: 0.0023\n",
            "Epoch [186/200], Batch [40/104], Loss: 0.0021\n",
            "Epoch [186/200], Batch [45/104], Loss: 0.0048\n",
            "Epoch [186/200], Batch [50/104], Loss: 0.0014\n",
            "Epoch [186/200], Batch [55/104], Loss: 0.0061\n",
            "Epoch [186/200], Batch [60/104], Loss: 0.0032\n",
            "Epoch [186/200], Batch [65/104], Loss: 0.0012\n",
            "Epoch [186/200], Batch [70/104], Loss: 0.0011\n",
            "Epoch [186/200], Batch [75/104], Loss: 0.0028\n",
            "Epoch [186/200], Batch [80/104], Loss: 0.0044\n",
            "Epoch [186/200], Batch [85/104], Loss: 0.0031\n",
            "Epoch [186/200], Batch [90/104], Loss: 0.0050\n",
            "Epoch [186/200], Batch [95/104], Loss: 0.0011\n",
            "Epoch [186/200], Batch [100/104], Loss: 0.0030\n",
            "Epoch 186: Train Loss = 0.3977, Accuracy = 100.00%\n",
            "Epoch [187/200], Batch [5/104], Loss: 0.0052\n",
            "Epoch [187/200], Batch [10/104], Loss: 0.0024\n",
            "Epoch [187/200], Batch [15/104], Loss: 0.0026\n",
            "Epoch [187/200], Batch [20/104], Loss: 0.0054\n",
            "Epoch [187/200], Batch [25/104], Loss: 0.0028\n",
            "Epoch [187/200], Batch [30/104], Loss: 0.0034\n",
            "Epoch [187/200], Batch [35/104], Loss: 0.0010\n",
            "Epoch [187/200], Batch [40/104], Loss: 0.0032\n",
            "Epoch [187/200], Batch [45/104], Loss: 0.0060\n",
            "Epoch [187/200], Batch [50/104], Loss: 0.0037\n",
            "Epoch [187/200], Batch [55/104], Loss: 0.0023\n",
            "Epoch [187/200], Batch [60/104], Loss: 0.0019\n",
            "Epoch [187/200], Batch [65/104], Loss: 0.0035\n",
            "Epoch [187/200], Batch [70/104], Loss: 0.0026\n",
            "Epoch [187/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [187/200], Batch [80/104], Loss: 0.0045\n",
            "Epoch [187/200], Batch [85/104], Loss: 0.0032\n",
            "Epoch [187/200], Batch [90/104], Loss: 0.0056\n",
            "Epoch [187/200], Batch [95/104], Loss: 0.0043\n",
            "Epoch [187/200], Batch [100/104], Loss: 0.0033\n",
            "Epoch 187: Train Loss = 0.4003, Accuracy = 100.00%\n",
            "Epoch [188/200], Batch [5/104], Loss: 0.0035\n",
            "Epoch [188/200], Batch [10/104], Loss: 0.0027\n",
            "Epoch [188/200], Batch [15/104], Loss: 0.0051\n",
            "Epoch [188/200], Batch [20/104], Loss: 0.0039\n",
            "Epoch [188/200], Batch [25/104], Loss: 0.0020\n",
            "Epoch [188/200], Batch [30/104], Loss: 0.0011\n",
            "Epoch [188/200], Batch [35/104], Loss: 0.0028\n",
            "Epoch [188/200], Batch [40/104], Loss: 0.0035\n",
            "Epoch [188/200], Batch [45/104], Loss: 0.0027\n",
            "Epoch [188/200], Batch [50/104], Loss: 0.0076\n",
            "Epoch [188/200], Batch [55/104], Loss: 0.0025\n",
            "Epoch [188/200], Batch [60/104], Loss: 0.0030\n",
            "Epoch [188/200], Batch [65/104], Loss: 0.0065\n",
            "Epoch [188/200], Batch [70/104], Loss: 0.0023\n",
            "Epoch [188/200], Batch [75/104], Loss: 0.0035\n",
            "Epoch [188/200], Batch [80/104], Loss: 0.0097\n",
            "Epoch [188/200], Batch [85/104], Loss: 0.0089\n",
            "Epoch [188/200], Batch [90/104], Loss: 0.0044\n",
            "Epoch [188/200], Batch [95/104], Loss: 0.0062\n",
            "Epoch [188/200], Batch [100/104], Loss: 0.0035\n",
            "Epoch 188: Train Loss = 0.3939, Accuracy = 100.00%\n",
            "Epoch [189/200], Batch [5/104], Loss: 0.0063\n",
            "Epoch [189/200], Batch [10/104], Loss: 0.0018\n",
            "Epoch [189/200], Batch [15/104], Loss: 0.0009\n",
            "Epoch [189/200], Batch [20/104], Loss: 0.0049\n",
            "Epoch [189/200], Batch [25/104], Loss: 0.0038\n",
            "Epoch [189/200], Batch [30/104], Loss: 0.0082\n",
            "Epoch [189/200], Batch [35/104], Loss: 0.0026\n",
            "Epoch [189/200], Batch [40/104], Loss: 0.0039\n",
            "Epoch [189/200], Batch [45/104], Loss: 0.0069\n",
            "Epoch [189/200], Batch [50/104], Loss: 0.0020\n",
            "Epoch [189/200], Batch [55/104], Loss: 0.0057\n",
            "Epoch [189/200], Batch [60/104], Loss: 0.0020\n",
            "Epoch [189/200], Batch [65/104], Loss: 0.0079\n",
            "Epoch [189/200], Batch [70/104], Loss: 0.0031\n",
            "Epoch [189/200], Batch [75/104], Loss: 0.0054\n",
            "Epoch [189/200], Batch [80/104], Loss: 0.0046\n",
            "Epoch [189/200], Batch [85/104], Loss: 0.0065\n",
            "Epoch [189/200], Batch [90/104], Loss: 0.0088\n",
            "Epoch [189/200], Batch [95/104], Loss: 0.0034\n",
            "Epoch [189/200], Batch [100/104], Loss: 0.0052\n",
            "Epoch 189: Train Loss = 0.3969, Accuracy = 100.00%\n",
            "Epoch [190/200], Batch [5/104], Loss: 0.0039\n",
            "Epoch [190/200], Batch [10/104], Loss: 0.0025\n",
            "Epoch [190/200], Batch [15/104], Loss: 0.0034\n",
            "Epoch [190/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [190/200], Batch [25/104], Loss: 0.0026\n",
            "Epoch [190/200], Batch [30/104], Loss: 0.0019\n",
            "Epoch [190/200], Batch [35/104], Loss: 0.0045\n",
            "Epoch [190/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [190/200], Batch [45/104], Loss: 0.0099\n",
            "Epoch [190/200], Batch [50/104], Loss: 0.0035\n",
            "Epoch [190/200], Batch [55/104], Loss: 0.0021\n",
            "Epoch [190/200], Batch [60/104], Loss: 0.0017\n",
            "Epoch [190/200], Batch [65/104], Loss: 0.0022\n",
            "Epoch [190/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [190/200], Batch [75/104], Loss: 0.0033\n",
            "Epoch [190/200], Batch [80/104], Loss: 0.0080\n",
            "Epoch [190/200], Batch [85/104], Loss: 0.0025\n",
            "Epoch [190/200], Batch [90/104], Loss: 0.0035\n",
            "Epoch [190/200], Batch [95/104], Loss: 0.0037\n",
            "Epoch [190/200], Batch [100/104], Loss: 0.0010\n",
            "Epoch 190: Train Loss = 0.3946, Accuracy = 100.00%\n",
            "Epoch [191/200], Batch [5/104], Loss: 0.0037\n",
            "Epoch [191/200], Batch [10/104], Loss: 0.0020\n",
            "Epoch [191/200], Batch [15/104], Loss: 0.0031\n",
            "Epoch [191/200], Batch [20/104], Loss: 0.0022\n",
            "Epoch [191/200], Batch [25/104], Loss: 0.0026\n",
            "Epoch [191/200], Batch [30/104], Loss: 0.0023\n",
            "Epoch [191/200], Batch [35/104], Loss: 0.0054\n",
            "Epoch [191/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [191/200], Batch [45/104], Loss: 0.0090\n",
            "Epoch [191/200], Batch [50/104], Loss: 0.0056\n",
            "Epoch [191/200], Batch [55/104], Loss: 0.0015\n",
            "Epoch [191/200], Batch [60/104], Loss: 0.0035\n",
            "Epoch [191/200], Batch [65/104], Loss: 0.0042\n",
            "Epoch [191/200], Batch [70/104], Loss: 0.0017\n",
            "Epoch [191/200], Batch [75/104], Loss: 0.0019\n",
            "Epoch [191/200], Batch [80/104], Loss: 0.0040\n",
            "Epoch [191/200], Batch [85/104], Loss: 0.0012\n",
            "Epoch [191/200], Batch [90/104], Loss: 0.0031\n",
            "Epoch [191/200], Batch [95/104], Loss: 0.0030\n",
            "Epoch [191/200], Batch [100/104], Loss: 0.0042\n",
            "Epoch 191: Train Loss = 0.3941, Accuracy = 100.00%\n",
            "Epoch [192/200], Batch [5/104], Loss: 0.0021\n",
            "Epoch [192/200], Batch [10/104], Loss: 0.0077\n",
            "Epoch [192/200], Batch [15/104], Loss: 0.0010\n",
            "Epoch [192/200], Batch [20/104], Loss: 0.0018\n",
            "Epoch [192/200], Batch [25/104], Loss: 0.0036\n",
            "Epoch [192/200], Batch [30/104], Loss: 0.0065\n",
            "Epoch [192/200], Batch [35/104], Loss: 0.0012\n",
            "Epoch [192/200], Batch [40/104], Loss: 0.0023\n",
            "Epoch [192/200], Batch [45/104], Loss: 0.0074\n",
            "Epoch [192/200], Batch [50/104], Loss: 0.0064\n",
            "Epoch [192/200], Batch [55/104], Loss: 0.0012\n",
            "Epoch [192/200], Batch [60/104], Loss: 0.0036\n",
            "Epoch [192/200], Batch [65/104], Loss: 0.0048\n",
            "Epoch [192/200], Batch [70/104], Loss: 0.0040\n",
            "Epoch [192/200], Batch [75/104], Loss: 0.0007\n",
            "Epoch [192/200], Batch [80/104], Loss: 0.0022\n",
            "Epoch [192/200], Batch [85/104], Loss: 0.0040\n",
            "Epoch [192/200], Batch [90/104], Loss: 0.0047\n",
            "Epoch [192/200], Batch [95/104], Loss: 0.0060\n",
            "Epoch [192/200], Batch [100/104], Loss: 0.0026\n",
            "Epoch 192: Train Loss = 0.3962, Accuracy = 100.00%\n",
            "Epoch [193/200], Batch [5/104], Loss: 0.0033\n",
            "Epoch [193/200], Batch [10/104], Loss: 0.0049\n",
            "Epoch [193/200], Batch [15/104], Loss: 0.0059\n",
            "Epoch [193/200], Batch [20/104], Loss: 0.0035\n",
            "Epoch [193/200], Batch [25/104], Loss: 0.0068\n",
            "Epoch [193/200], Batch [30/104], Loss: 0.0039\n",
            "Epoch [193/200], Batch [35/104], Loss: 0.0040\n",
            "Epoch [193/200], Batch [40/104], Loss: 0.0071\n",
            "Epoch [193/200], Batch [45/104], Loss: 0.0023\n",
            "Epoch [193/200], Batch [50/104], Loss: 0.0057\n",
            "Epoch [193/200], Batch [55/104], Loss: 0.0034\n",
            "Epoch [193/200], Batch [60/104], Loss: 0.0016\n",
            "Epoch [193/200], Batch [65/104], Loss: 0.0046\n",
            "Epoch [193/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [193/200], Batch [75/104], Loss: 0.0054\n",
            "Epoch [193/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [193/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [193/200], Batch [90/104], Loss: 0.0050\n",
            "Epoch [193/200], Batch [95/104], Loss: 0.0040\n",
            "Epoch [193/200], Batch [100/104], Loss: 0.0015\n",
            "Epoch 193: Train Loss = 0.3938, Accuracy = 100.00%\n",
            "Epoch [194/200], Batch [5/104], Loss: 0.0027\n",
            "Epoch [194/200], Batch [10/104], Loss: 0.0033\n",
            "Epoch [194/200], Batch [15/104], Loss: 0.0047\n",
            "Epoch [194/200], Batch [20/104], Loss: 0.0074\n",
            "Epoch [194/200], Batch [25/104], Loss: 0.0036\n",
            "Epoch [194/200], Batch [30/104], Loss: 0.0011\n",
            "Epoch [194/200], Batch [35/104], Loss: 0.0046\n",
            "Epoch [194/200], Batch [40/104], Loss: 0.0049\n",
            "Epoch [194/200], Batch [45/104], Loss: 0.0029\n",
            "Epoch [194/200], Batch [50/104], Loss: 0.0017\n",
            "Epoch [194/200], Batch [55/104], Loss: 0.0035\n",
            "Epoch [194/200], Batch [60/104], Loss: 0.0031\n",
            "Epoch [194/200], Batch [65/104], Loss: 0.0066\n",
            "Epoch [194/200], Batch [70/104], Loss: 0.0051\n",
            "Epoch [194/200], Batch [75/104], Loss: 0.0021\n",
            "Epoch [194/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [194/200], Batch [85/104], Loss: 0.0024\n",
            "Epoch [194/200], Batch [90/104], Loss: 0.0019\n",
            "Epoch [194/200], Batch [95/104], Loss: 0.0065\n",
            "Epoch [194/200], Batch [100/104], Loss: 0.0046\n",
            "Epoch 194: Train Loss = 0.4079, Accuracy = 100.00%\n",
            "Epoch [195/200], Batch [5/104], Loss: 0.0026\n",
            "Epoch [195/200], Batch [10/104], Loss: 0.0016\n",
            "Epoch [195/200], Batch [15/104], Loss: 0.0030\n",
            "Epoch [195/200], Batch [20/104], Loss: 0.0018\n",
            "Epoch [195/200], Batch [25/104], Loss: 0.0026\n",
            "Epoch [195/200], Batch [30/104], Loss: 0.0033\n",
            "Epoch [195/200], Batch [35/104], Loss: 0.0026\n",
            "Epoch [195/200], Batch [40/104], Loss: 0.0057\n",
            "Epoch [195/200], Batch [45/104], Loss: 0.0034\n",
            "Epoch [195/200], Batch [50/104], Loss: 0.0024\n",
            "Epoch [195/200], Batch [55/104], Loss: 0.0039\n",
            "Epoch [195/200], Batch [60/104], Loss: 0.0029\n",
            "Epoch [195/200], Batch [65/104], Loss: 0.0023\n",
            "Epoch [195/200], Batch [70/104], Loss: 0.0039\n",
            "Epoch [195/200], Batch [75/104], Loss: 0.0053\n",
            "Epoch [195/200], Batch [80/104], Loss: 0.0030\n",
            "Epoch [195/200], Batch [85/104], Loss: 0.0051\n",
            "Epoch [195/200], Batch [90/104], Loss: 0.0055\n",
            "Epoch [195/200], Batch [95/104], Loss: 0.0060\n",
            "Epoch [195/200], Batch [100/104], Loss: 0.0044\n",
            "Epoch 195: Train Loss = 0.3948, Accuracy = 100.00%\n",
            "Epoch [196/200], Batch [5/104], Loss: 0.0011\n",
            "Epoch [196/200], Batch [10/104], Loss: 0.0029\n",
            "Epoch [196/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [196/200], Batch [20/104], Loss: 0.0045\n",
            "Epoch [196/200], Batch [25/104], Loss: 0.0056\n",
            "Epoch [196/200], Batch [30/104], Loss: 0.0014\n",
            "Epoch [196/200], Batch [35/104], Loss: 0.0025\n",
            "Epoch [196/200], Batch [40/104], Loss: 0.0014\n",
            "Epoch [196/200], Batch [45/104], Loss: 0.0027\n",
            "Epoch [196/200], Batch [50/104], Loss: 0.0017\n",
            "Epoch [196/200], Batch [55/104], Loss: 0.0030\n",
            "Epoch [196/200], Batch [60/104], Loss: 0.0025\n",
            "Epoch [196/200], Batch [65/104], Loss: 0.0035\n",
            "Epoch [196/200], Batch [70/104], Loss: 0.0018\n",
            "Epoch [196/200], Batch [75/104], Loss: 0.0072\n",
            "Epoch [196/200], Batch [80/104], Loss: 0.0028\n",
            "Epoch [196/200], Batch [85/104], Loss: 0.0039\n",
            "Epoch [196/200], Batch [90/104], Loss: 0.0072\n",
            "Epoch [196/200], Batch [95/104], Loss: 0.0041\n",
            "Epoch [196/200], Batch [100/104], Loss: 0.0035\n",
            "Epoch 196: Train Loss = 0.3935, Accuracy = 100.00%\n",
            "Epoch [197/200], Batch [5/104], Loss: 0.0038\n",
            "Epoch [197/200], Batch [10/104], Loss: 0.0022\n",
            "Epoch [197/200], Batch [15/104], Loss: 0.0021\n",
            "Epoch [197/200], Batch [20/104], Loss: 0.0063\n",
            "Epoch [197/200], Batch [25/104], Loss: 0.0040\n",
            "Epoch [197/200], Batch [30/104], Loss: 0.0067\n",
            "Epoch [197/200], Batch [35/104], Loss: 0.0032\n",
            "Epoch [197/200], Batch [40/104], Loss: 0.0031\n",
            "Epoch [197/200], Batch [45/104], Loss: 0.0053\n",
            "Epoch [197/200], Batch [50/104], Loss: 0.0015\n",
            "Epoch [197/200], Batch [55/104], Loss: 0.0027\n",
            "Epoch [197/200], Batch [60/104], Loss: 0.0027\n",
            "Epoch [197/200], Batch [65/104], Loss: 0.0077\n",
            "Epoch [197/200], Batch [70/104], Loss: 0.0010\n",
            "Epoch [197/200], Batch [75/104], Loss: 0.0065\n",
            "Epoch [197/200], Batch [80/104], Loss: 0.0021\n",
            "Epoch [197/200], Batch [85/104], Loss: 0.0028\n",
            "Epoch [197/200], Batch [90/104], Loss: 0.0035\n",
            "Epoch [197/200], Batch [95/104], Loss: 0.0039\n",
            "Epoch [197/200], Batch [100/104], Loss: 0.0024\n",
            "Epoch 197: Train Loss = 0.3966, Accuracy = 100.00%\n",
            "Epoch [198/200], Batch [5/104], Loss: 0.0072\n",
            "Epoch [198/200], Batch [10/104], Loss: 0.0021\n",
            "Epoch [198/200], Batch [15/104], Loss: 0.0033\n",
            "Epoch [198/200], Batch [20/104], Loss: 0.0040\n",
            "Epoch [198/200], Batch [25/104], Loss: 0.0015\n",
            "Epoch [198/200], Batch [30/104], Loss: 0.0037\n",
            "Epoch [198/200], Batch [35/104], Loss: 0.0050\n",
            "Epoch [198/200], Batch [40/104], Loss: 0.0033\n",
            "Epoch [198/200], Batch [45/104], Loss: 0.0022\n",
            "Epoch [198/200], Batch [50/104], Loss: 0.0045\n",
            "Epoch [198/200], Batch [55/104], Loss: 0.0030\n",
            "Epoch [198/200], Batch [60/104], Loss: 0.0014\n",
            "Epoch [198/200], Batch [65/104], Loss: 0.0010\n",
            "Epoch [198/200], Batch [70/104], Loss: 0.0048\n",
            "Epoch [198/200], Batch [75/104], Loss: 0.0080\n",
            "Epoch [198/200], Batch [80/104], Loss: 0.0040\n",
            "Epoch [198/200], Batch [85/104], Loss: 0.0036\n",
            "Epoch [198/200], Batch [90/104], Loss: 0.0061\n",
            "Epoch [198/200], Batch [95/104], Loss: 0.0048\n",
            "Epoch [198/200], Batch [100/104], Loss: 0.0025\n",
            "Epoch 198: Train Loss = 0.3933, Accuracy = 100.00%\n",
            "Epoch [199/200], Batch [5/104], Loss: 0.0041\n",
            "Epoch [199/200], Batch [10/104], Loss: 0.0013\n",
            "Epoch [199/200], Batch [15/104], Loss: 0.0017\n",
            "Epoch [199/200], Batch [20/104], Loss: 0.0040\n",
            "Epoch [199/200], Batch [25/104], Loss: 0.0059\n",
            "Epoch [199/200], Batch [30/104], Loss: 0.0018\n",
            "Epoch [199/200], Batch [35/104], Loss: 0.0052\n",
            "Epoch [199/200], Batch [40/104], Loss: 0.0020\n",
            "Epoch [199/200], Batch [45/104], Loss: 0.0044\n",
            "Epoch [199/200], Batch [50/104], Loss: 0.0047\n",
            "Epoch [199/200], Batch [55/104], Loss: 0.0052\n",
            "Epoch [199/200], Batch [60/104], Loss: 0.0036\n",
            "Epoch [199/200], Batch [65/104], Loss: 0.0064\n",
            "Epoch [199/200], Batch [70/104], Loss: 0.0086\n",
            "Epoch [199/200], Batch [75/104], Loss: 0.0066\n",
            "Epoch [199/200], Batch [80/104], Loss: 0.0032\n",
            "Epoch [199/200], Batch [85/104], Loss: 0.0028\n",
            "Epoch [199/200], Batch [90/104], Loss: 0.0047\n",
            "Epoch [199/200], Batch [95/104], Loss: 0.0033\n",
            "Epoch [199/200], Batch [100/104], Loss: 0.0016\n",
            "Epoch 199: Train Loss = 0.4004, Accuracy = 100.00%\n",
            "Epoch [200/200], Batch [5/104], Loss: 0.0016\n",
            "Epoch [200/200], Batch [10/104], Loss: 0.0039\n",
            "Epoch [200/200], Batch [15/104], Loss: 0.0047\n",
            "Epoch [200/200], Batch [20/104], Loss: 0.0064\n",
            "Epoch [200/200], Batch [25/104], Loss: 0.0050\n",
            "Epoch [200/200], Batch [30/104], Loss: 0.0011\n",
            "Epoch [200/200], Batch [35/104], Loss: 0.0064\n",
            "Epoch [200/200], Batch [40/104], Loss: 0.0033\n",
            "Epoch [200/200], Batch [45/104], Loss: 0.0014\n",
            "Epoch [200/200], Batch [50/104], Loss: 0.0040\n",
            "Epoch [200/200], Batch [55/104], Loss: 0.0043\n",
            "Epoch [200/200], Batch [60/104], Loss: 0.0037\n",
            "Epoch [200/200], Batch [65/104], Loss: 0.0040\n",
            "Epoch [200/200], Batch [70/104], Loss: 0.0055\n",
            "Epoch [200/200], Batch [75/104], Loss: 0.0021\n",
            "Epoch [200/200], Batch [80/104], Loss: 0.0071\n",
            "Epoch [200/200], Batch [85/104], Loss: 0.0042\n",
            "Epoch [200/200], Batch [90/104], Loss: 0.0055\n",
            "Epoch [200/200], Batch [95/104], Loss: 0.0018\n",
            "Epoch [200/200], Batch [100/104], Loss: 0.0019\n",
            "Epoch 200: Train Loss = 0.3942, Accuracy = 100.00%\n",
            "Validation Accuracy: 96.85%\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "save_prompt_learner() missing 2 required positional arguments: 'model_name' and 'dataset_name'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cd44bcd7efe6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {100.0 * correct / total:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# save the model .pth file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0msave_prompt_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: save_prompt_learner() missing 2 required positional arguments: 'model_name' and 'dataset_name'"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "PRINT_FREQ = 5\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    # puts the model in the training mode\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    #iterate through train_loader batches\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # zero the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        logits = model(images)\n",
        "        # compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "        # backpropagrate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss, compute results\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i + 1) % PRINT_FREQ == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{MAX_EPOCH}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc = 100. * correct / total #accuracy of the model\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    scheduler.step()\n",
        "# set the model in the evaluation mode, over the validation dataset and get predictions\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {100.0 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPXJ0aDZ6XrL",
        "outputId": "e0dc0934-ae82-492e-e44d-7896ea73f63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt learner saved to output/ViT-B/16/caltech/coop_prompt.pth\n"
          ]
        }
      ],
      "source": [
        "# save the model .pth file\n",
        "save_prompt_learner(model=model,model_name=model_name,dataset_name=test_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijIwHBJOVQVH"
      },
      "source": [
        "#Prompt Interpretation\n",
        "\n",
        "Interprets the learned context vectors from the saved CoOp prompt learner. It maps them to the closest real words in CLIP vocabulary. It helps explaining what the learned context tokens represent in natural language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HDzYh7JKXTAm"
      },
      "outputs": [],
      "source": [
        "def interpret_prompt(fpath, topk=5, model_name=model_name): #f_Path is .pth file, topk: number of top closest tokens to show per context vector\n",
        "    assert os.path.exists(fpath), f\"Prompt file not found: {fpath}\"\n",
        "    print(f\"Return the top-{topk} matched words\")\n",
        "\n",
        "    # loads the clip model and a token embedding\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    clip_model = clip.load(model_name, device=\"cpu\")[0]  # Use same model backbone\n",
        "    token_embedding = clip_model.token_embedding.weight\n",
        "    print(f\"Size of token embedding: {token_embedding.shape}\")\n",
        "\n",
        "    #load the trained prompt learner\n",
        "    prompt_learner = torch.load(fpath, map_location=\"cpu\")[\"state_dict\"]\n",
        "    # trained context embedding\n",
        "    ctx = prompt_learner[\"ctx\"].float()\n",
        "    print(f\"Size of context: {ctx.shape}\")\n",
        "\n",
        "    # This is for the shared context\n",
        "    if ctx.dim() == 2:\n",
        "        #computes the euclidean distance between each context token and each real token\n",
        "        distance = torch.cdist(ctx, token_embedding)\n",
        "        print(f\"Size of distance matrix: {distance.shape}\")\n",
        "        sorted_idxs = torch.argsort(distance, dim=1)[:, :topk]\n",
        "\n",
        "        for m, idxs in enumerate(sorted_idxs):\n",
        "            words = [tokenizer.decoder[idx.item()] for idx in idxs] # here the tokenizer.decoder helps to decodes the closest token into actual works\n",
        "            dist = [f\"{distance[m, idx].item():.4f}\" for idx in idxs]\n",
        "            print(f\"{m+1}: {words} {dist}\")\n",
        "\n",
        "    elif ctx.dim() == 3:\n",
        "        raise NotImplementedError(\"Class-specific context interpretation not supported.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2OMS6xV_oF_"
      },
      "source": [
        "#Interpret Prompts Results\n",
        "\n",
        "NUmbers on left (1:16) context vectors learned, the list shows top-5 closest vocabulary tokens (Decoded) for thier respective context vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZL4BA0fX7dz",
        "outputId": "c66e1a98-46a3-4830-aff5-09b76fa48912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Return the top-5 matched words\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['Ę</w>', 'Ď</w>', 'ď</w>', 'Đ</w>', 'đ</w>'] ['0.5561', '0.5561', '0.5561', '0.5561', '0.5561']\n",
            "2: ['mikequind', 'ó</w>', 'ė</w>', 'Ė</w>', 'ĕ</w>'] ['0.6559', '0.6590', '0.6590', '0.6590', '0.6590']\n",
            "3: ['pknot</w>', 'Ç</w>', 'I</w>', 'Æ</w>', 'Å</w>'] ['0.6629', '0.6640', '0.6640', '0.6640', '0.6640']\n",
            "4: ['requi', 'commod', 'read', 'setti</w>', 'monetary</w>'] ['1.0789', '1.0815', '1.0824', '1.0830', '1.0831']\n",
            "5: ['polar', 'atility</w>', 'calle', 'coscino</w>', 'pational</w>'] ['0.8004', '0.8014', '0.8021', '0.8022', '0.8027']\n",
            "6: ['sundaywithmarsha</w>', 'kirstel</w>', 'ę</w>', 'Đ</w>', 'đ</w>'] ['0.5346', '0.5378', '0.5389', '0.5389', '0.5389']\n",
            "7: ['ė</w>', 'č</w>', 'Ď</w>', 'ď</w>', 'Đ</w>'] ['0.5942', '0.5942', '0.5942', '0.5942', '0.5942']\n",
            "8: ['pational</w>', 'sundaywithmarsha</w>', 'Ë</w>', 'Õ</w>', 'Ô</w>'] ['0.6781', '0.6787', '0.6798', '0.6798', '0.6798']\n",
            "9: ['tvmiaw</w>', 'accommo', 'ô', 'Ę</w>', 'Ġ'] ['0.7058', '0.7070', '0.7079', '0.7085', '0.7085']\n",
            "10: ['powe', 'Ė', 'Ğ', 'ĝ', 'Ĝ'] ['0.7158', '0.7186', '0.7186', '0.7186', '0.7186']\n",
            "11: ['snes</w>', 'earthand', 'gregor', 'capsul', 'conference</w>'] ['0.8358', '0.8373', '0.8389', '0.8412', '0.8415']\n",
            "12: ['coscino</w>', 'tvmiaw</w>', 'capsul', 'ù', 'Ė</w>'] ['0.7460', '0.7461', '0.7466', '0.7475', '0.7475']\n",
            "13: ['therine</w>', 'ãģ¨ç¹ĭãģ', 'undoub', 'degradation</w>', 'upcycled</w>'] ['0.8898', '0.8908', '0.8939', '0.8943', '0.8951']\n",
            "14: ['newprofile', 'sponsored</w>', 'ñ', 'decorated</w>', 'Ý'] ['0.8357', '0.8384', '0.8400', '0.8418', '0.8420']\n",
            "15: ['tomorrowspaper', 'accused</w>', 'token', 'sent', 'fully'] ['0.8424', '0.8431', '0.8460', '0.8464', '0.8466']\n",
            "16: ['contact</w>', 'tvmiaw</w>', 'pational</w>', 'Ā', 'ć'] ['0.6356', '0.6359', '0.6364', '0.6376', '0.6376']\n"
          ]
        }
      ],
      "source": [
        "interpret_prompt(path, topk=5, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qimQtfPolZSJ"
      },
      "source": [
        "#Comparison with CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dwU4ynYSlcJ1"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits = model(images)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i39xdogwldpI",
        "outputId": "e7d62e2c-8766-440d-9a40-5a1b25bb161a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "\n",
            "Classification Accuracy Comparison:\n",
            "Zero-shot CLIP   : 77.72%\n",
            "CoOp : 96.85%\n"
          ]
        }
      ],
      "source": [
        "# Load zero-shot model (no prompt weights loaded)\n",
        "clip_model, _ = clip.load(model_name, device=device)\n",
        "zero_shot_model = CustomCLIP(classnames, clip_model).to(device)\n",
        "\n",
        "# Load trained CoOp model\n",
        "trained_model = CustomCLIP(classnames, clip_model).to(device)\n",
        "ckpt = torch.load(path, map_location=\"cpu\")\n",
        "trained_model.prompt_learner.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "# Compare\n",
        "acc_zero = evaluate_model(zero_shot_model, val_loader, device)\n",
        "acc_coop = evaluate_model(trained_model, val_loader, device)\n",
        "\n",
        "print(f\"\\nClassification Accuracy Comparison:\")\n",
        "print(f\"Zero-shot CLIP   : {acc_zero:.2f}%\")\n",
        "print(f\"CoOp : {acc_coop:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUGT30QbqL4"
      },
      "source": [
        "#Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6xkdr1GBenIv"
      },
      "outputs": [],
      "source": [
        "def visualize_tsne(model, val_loader, classnames):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(next(model.parameters()).device).type(model.dtype)\n",
        "            feats = model.image_encoder(images)\n",
        "            features.append(feats.cpu())\n",
        "            labels_list.extend(labels.cpu())\n",
        "\n",
        "    features = torch.cat(features).numpy()\n",
        "    labels_list = torch.tensor(labels_list).numpy()\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced = tsne.fit_transform(features)\n",
        "\n",
        "    # Plot all points colored by class\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    num_classes = len(classnames)\n",
        "    cmap = plt.get_cmap(\"tab20\")\n",
        "\n",
        "    for class_idx in range(num_classes):\n",
        "        idxs = labels_list == class_idx\n",
        "        coords = reduced[idxs]\n",
        "        plt.scatter(coords[:, 0], coords[:, 1], s=5, color=cmap(class_idx % 20), alpha=0.6)\n",
        "\n",
        "        # Plot class number at mean location\n",
        "        center_x = coords[:, 0].mean()\n",
        "        center_y = coords[:, 1].mean()\n",
        "        plt.text(center_x, center_y, str(class_idx), fontsize=10, weight=\"bold\", color=cmap(class_idx % 20))\n",
        "\n",
        "    plt.title(\"t-SNE of CLIP Image Features CoOP\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "YapWMwBDepDw",
        "outputId": "ca2f1e9a-62a1-4588-cce5-be5ee9fda9da"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMWCAYAAADs4eXxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd8HPWd//HXzDbtrnqx5N67KY7pxTQDSSCEAJdCcgc5csldCCnkkhyX+yVwCTkg9S6XcCkE0kglhRCS0Hs3xoCL3GTJTV1abW8zvz/GWkuWZEm2VvX9fDz8kHZ2ZvY767Gs936/38/XsG3bRkRERERERERGnDnWDRARERERERGZrBS6RURERERERPJEoVtEREREREQkTxS6RURERERERPJEoVtEREREREQkTxS6RURERERERPJEoVtEREREREQkTxS6RURERERERPJEoVtEREREREQkTxS6RURkwtm+fTsXXXQRJSUlGIbBH/7wh7FukoiIiEi/FLpFRCaw5557jptvvpnOzs4hHxOJRPjiF7/IqlWrCAaDVFRUcOKJJ/KJT3yC/fv35/a7+eabMQyD6upqYrFYn/PMmzePSy+9tNc2wzAG/PPP//zPR32dh7vmmmt44403uPXWW/npT3/KSSeddMT9u7q6uOWWWzjhhBMoLCzE7/ezatUqPve5z/W65muvvZbCwsIjnuuee+7BMAxeeeWV3Lbu96r7TyAQYMWKFfzHf/wHXV1dRzzf7t27MQyDr33ta0O48olr3rx5A94biUQiL6/5la98ZcJ/IDPUe3e4nn32Wd71rndRXV2Nz+dj3rx5fOQjH6GhoaHPvsdyf4uICLjHugEiInL0nnvuOW655RauvfZaSktLB90/nU6zdu1atm7dyjXXXMMNN9xAJBJh06ZN3HvvvbzrXe9ixowZvY5pbm7mzjvv5NOf/vSQ2nThhRfyD//wD322L1myZEjHDyYej/P888/z+c9/no997GOD7r9r1y7WrVtHQ0MDf/d3f8eHP/xhvF4vr7/+OnfddRe///3v2bZt24i07c4776SwsJBIJMJDDz3ErbfeymOPPcazzz6LYRgj8hoT2YknntjvfeT1evPyel/5yle46qqruPzyy/Ny/nzL17377W9/m0984hMsWLCAG264genTp7NlyxZ++MMf8qtf/YoHH3yQM844o89xur9FRI6OQreIyBTyhz/8gQ0bNvDzn/+cq6++utdziUSCVCrV55gTTzyRr371q3z0ox/F7/cP+hpLlizhAx/4wIi1+XAtLS0AQ/qQIZPJcMUVV9DU1MQTTzzBWWed1ev5W2+9ldtvv33E2nbVVVdRWVkJwD//8z9z5ZVX8rvf/Y4XXniB008/fcReZ6KaOXNmXu+N0WBZFqlUioKCgry+Tr7u3WeffZZPfvKTnHXWWfz1r38lEAjknvuXf/kXzjzzTK666io2bdpEWVlZr2N1f4uIHB0NLxcRmaBuvvlmPvOZzwAwf/783NDP3bt3D3jMzp07ATjzzDP7PFdQUEBxcXGf7V/4whdoamrizjvvHJmGH8GGDRt429veRnFxMYWFhVxwwQW88MILuedvvvlm5s6dC8BnPvMZDMNg3rx5A57vvvvuY+PGjXz+85/vE1oAiouLufXWW0f8Orqdf/75ANTV1Q3ruO4h7M888wwf//jHqaqqorS0lI985COkUik6Ozv5h3/4B8rKyigrK+Ozn/0stm33OsfXvvY1zjjjDCoqKvD7/axZs4bf/va3fV4rHo/z8Y9/nMrKSoqKirjsssvYt28fhmFw880399p33759/OM//mNuSPLKlSv50Y9+NLw35Qg6Ozv55Cc/yezZs/H5fCxatIjbb78dy7KGfW2GYRCNRvnxj3+c+7dx7bXXAs40gv7um+5h1Ief52Mf+xg///nPWblyJT6fj7/+9a/Dej++/e1vs3LlSgKBAGVlZZx00knce++9R3wvjube/c1vfsOaNWvw+/1UVlbygQ98gH379vXa50tf+hKGYfDjH/+4V+AGWLhwIXfccQcHDhzge9/73hHbB0d/f4uITDXq6RYRmaCuuOIKtm3bxi9+8Qu++c1v5nqgqqqqBjymO7D+5Cc/4T/+4z+GNCT07LPP5vzzz+eOO+7gX/7lXwbt7U4kErS2tvbZXlxcfMRhxJs2beLss8+muLiYz372s3g8Hr73ve9x7rnn8uSTT3LqqadyxRVXUFpayqc+9Sne97738fa3v/2Ic7Dvv/9+AP7+7/9+0OvMh+4POSoqKo7q+BtuuIGamhpuueUWXnjhBb7//e9TWlrKc889x5w5c/jKV77Cgw8+yFe/+lVWrVrVa1j/f//3f3PZZZfx/ve/n1QqxS9/+Uv+7u/+jgceeIBLLrkkt9+1117Lr3/9a/7+7/+e0047jSeffLLX892ampo47bTTciG0qqqKv/zlL1x33XV0dXXxyU9+ctDrSafTfe6NQCBAIBAgFotxzjnnsG/fPj7ykY8wZ84cnnvuOW666SYOHDjAt771rWFd209/+lM+9KEPccopp/DhD38YcELl0Xjsscf49a9/zcc+9jEqKyuZN2/ekN+PH/zgB3z84x/nqquu4hOf+ASJRILXX3+dF198sc9ok56Ge+/ec889fPCDH+Tkk0/mv/7rv2hqauK///u/efbZZ9mwYQOlpaXEYjEeffRRzj77bObPn9/ved7znvfw4Q9/mAceeIB/+7d/O+JrHuv9LSIyZdgiIjJhffWrX7UBu66ubkj7x2Ixe+nSpTZgz50717722mvtu+66y25qauqz7xe/+EUbsFtaWuwnn3zSBuxvfOMbuefnzp1rX3LJJb2OAQb884tf/OKIbbv88sttr9dr79y5M7dt//79dlFRkb127drctrq6Ohuwv/rVrw56vatXr7ZLSkoG3a/bNddcYweDwSPuc/fdd9uA/fLLL+e2db9XtbW1dktLi11XV2d/73vfs30+n11dXW1Ho9EBz9ff9XS/xsUXX2xblpXbfvrpp9uGYdj//M//nNuWyWTsWbNm2eecc06v88ZisV6PU6mUvWrVKvv888/PbVu/fr0N2J/85Cd77XvttdfagP3FL34xt+26666zp0+fbre2tvba973vfa9dUlLS5/UON3fu3H7vi+7X+NKXvmQHg0F727ZtvY77t3/7N9vlctkNDQ3Dujbbtu1gMGhfc801fdpyzTXX2HPnzu2zvfvvsSfANk3T3rRpU6/tQ30/3vnOd9orV67s+4YMYjj3biqVsqdNm2avWrXKjsfjue0PPPCADdhf+MIXbNu27ddee80G7E984hNHPN/xxx9vl5eX5x4fy/0tIiK2reHlIiJTiN/v58UXX8wNS7/nnnu47rrrmD59OjfccAPJZLLf49auXct5553HHXfcQTweP+JrvPOd7+Thhx/u8+e8884b8JhsNstDDz3E5ZdfzoIFC3Lbp0+fztVXX80zzzxzVFWSu7q6KCoqGvZxR2vp0qVUVVUxf/58PvKRj7Bo0SL+/Oc/9xnGO1TXXXddr9EIp556KrZtc9111+W2uVwuTjrpJHbt2tXr2J4jEjo6OgiFQpx99tm8+uqrue3dw6Q/+tGP9jr2hhtu6PXYtm3uu+8+3vGOd2DbNq2trbk/F198MaFQqNd5B3Lqqaf2uS+6e+d/85vfcPbZZ1NWVtbr/OvWrSObzfLUU08N69pG0jnnnMOKFStyj4fzfpSWlrJ3715efvnlYb3mcO7dV155hebmZj760Y/2mmt+ySWXsGzZMv785z8DEA6HAQY9b1FRUb//3kb6/hYRmSo0vFxEZBJqb2/vVRTN7/dTUlICQElJCXfccQd33HEH9fX1PProo3zta1/jf//3fykpKeHLX/5yv+e8+eabOeecc/i///s/PvWpTw342rNmzWLdunXDam9LSwuxWIylS5f2eW758uVYlsWePXtYuXLlsM5bXFzcJ4zm03333UdxcTEej4dZs2Yd9XDmbnPmzOn1uPvvcPbs2X22d3R09Nr2wAMP8OUvf5nXXnut14cpPUN8fX09pmn2GWq8aNGiXo9bWlro7Ozk+9//Pt///vf7bWtzc/Og11NZWTngvbF9+3Zef/31AadH9Dz/UK5tJB3+/gzn/fjc5z7HI488wimnnMKiRYu46KKLuPrqq/utq9DTcO7d+vp6gH7//SxbtoxnnnkGOBS2u8P3QMLhcL/BfKTvbxGRqUKhW0RkErriiit48sknc4+vueYa7rnnnj77zZ07l3/8x3/kXe96FwsWLODnP//5gKF77dq1nHvuudxxxx0juuZ2Pi1btowNGzawZ8+ePkE1H9auXZubWz8SXC7XkLfbPQqpPf3001x22WWsXbuW7373u0yfPh2Px8Pdd989aAGv/nQXMvvABz7ANddc0+8+xx9//LDPe/hrXHjhhXz2s5/t9/nuJedG4toGCufZbLbf7YfXMRjO+7F8+XJqa2t54IEH+Otf/8p9993Hd7/7Xb7whS9wyy23DNjGfNy7ixYtwu128/rrrw+4TzKZpLa2lpNOOqnPcyN9f4uITBUK3SIiE9hA4eHrX/96r57Pw9fePlxZWRkLFy7kzTffPOJ+N998M+eee+6QKhsPR1VVFYFAgNra2j7Pbd26FdM0jyp4vOMd7+AXv/gFP/vZz7jppptGoqkTwn333UdBQQF/+9vf8Pl8ue133313r/3mzp2LZVnU1dWxePHi3PYdO3b02q+qqoqioiKy2eywRzEM1cKFC4lEIoOef6jXBgP/+ygrK6Ozs7PP9u4e48EM9/0IBoO85z3v4T3veQ+pVIorrriCW2+9lZtuumnApceGc+92F0isra3NVRTvVltbm3s+GAxy3nnn8dhjj1FfX5/b3tOvf/1rkskkl1566aDXJSIiQ6M53SIiE1gwGAToEyDWrFnDunXrcn+656Nu3Lix38ri9fX1bN68ud/hqT2dc845nHvuudx+++0kEomRuQicntuLLrqIP/7xj72WPGtqauLee+/lrLPO6nc5s8FcddVVHHfccdx66608//zzfZ4Ph8N8/vOfP5amj0sulwvDMHr13O7evZs//OEPvfa7+OKLAfjud7/ba/u3v/3tPue78sorue+++/r9YKZ77fRj8e53v5vnn3+ev/3tb32e6+zsJJPJ5NoylGsD599Hf+F64cKFhEKhXj2+Bw4c4Pe///2Q2jqc96Otra3Xc16vlxUrVmDbNul0esDXGM69e9JJJzFt2jT+7//+r9dw+7/85S9s2bKlVzX6//iP/8C2ba699to+9Rnq6ur47Gc/y/Tp0/nIRz4yyLsgIiJDpZ5uEZEJbM2aNQB8/vOf573vfS8ej4d3vOMduTB+uIcffpgvfvGLXHbZZZx22mkUFhaya9cufvSjH5FMJvusy9yfL37xi0csirZt2zZ+9rOf9dleXV3NhRdeOOBxX/7yl3n44Yc566yz+OhHP4rb7eZ73/seyWSSO+64Y9B29cfj8fC73/2OdevWsXbtWt797ndz5pln4vF42LRpE/feey9lZWW91jtOp9P9DrEvLy/vU3BsvLrkkkv4xje+wVvf+lauvvpqmpub+c53vsOiRYt6Bc01a9Zw5ZVX8q1vfYu2trbckmHbtm0DevcU33bbbTz++OOceuqp/NM//RMrVqygvb2dV199lUceeYT29vZjavNnPvMZ7r//fi699FKuvfZa1qxZQzQa5Y033uC3v/0tu3fvprKycsjX1n19jzzyCN/4xjeYMWMG8+fP59RTT+W9730vn/vc53jXu97Fxz/+cWKxGHfeeSdLliwZcjG2ob4fF110ETU1NZx55plUV1ezZcsW/vd//5dLLrnkiAXNhnPvejwebr/9dj74wQ9yzjnn8L73vS+3ZNi8efN61WBYu3YtX/va17jxxhs5/vjjufbaa5k+fTpbt27lBz/4AZZl8eCDD1JWVnYUf4siItKvsSucLiIiI+FLX/qSPXPmTNs0zUGXD9u1a5f9hS98wT7ttNPsadOm2W63266qqrIvueQS+7HHHuu1b88lww53zjnn2MCwlgw7fEmr/rz66qv2xRdfbBcWFtqBQMA+77zz7Oeee67XPsNZMqxbR0eH/YUvfME+7rjj7EAgYBcUFNirVq2yb7rpJvvAgQO5/a655poB279w4ULbto+8ZFh/79VgjrRkWM/XONLr9LfU2V133WUvXrzY9vl89rJly+y777673yWxotGoff3119vl5eV2YWGhffnll9u1tbU2YN9222299m1qarKvv/56e/bs2bbH47FramrsCy64wP7+978/6HX2t8Tc4cLhsH3TTTfZixYtsr1er11ZWWmfccYZ9te+9jU7lUoN+9q2bt1qr1271vb7/TbQa/mwhx56yF61apXt9XrtpUuX2j/72c8GXDLs+uuv77e9Q3k/vve979lr1661KyoqbJ/PZy9cuND+zGc+Y4dCoUHfM9se+r1r27b9q1/9yl69erXt8/ns8vJy+/3vf7+9d+/efs/71FNP2e985zvtyspK2+Px2HPmzLH/6Z/+yd69e3effY/l/hYREds2bLtH5RURERGZ8l577TVWr17Nz372M97//vePdXNEREQmNM3pFhERmcL6W3f9W9/6FqZpsnbt2jFokYiIyOSiOd0iIiJT2B133MH69es577zzcLvd/OUvf+Evf/kLH/7wh0dlmTUREZHJTsPLRUREprCHH36YW265hc2bNxOJRJgzZw5///d/z+c//3ncbn02LyIicqwUukVERERERETyRHO6RURERERERPJEoVtEREREREQkTyb8ZC3Lsti/fz9FRUUYhjHWzREREREREZEpwLZtwuEwM2bMwDQH7s+e8KF7//79qq4qIiIiIiIiY2LPnj3MmjVrwOcnfOguKioCnAstLi4e49bIeJVOp3nooYe46KKL8Hg8Y90ckWOi+1kmE93PMpnofpbJRPfz4Lq6upg9e3Yukw5kwofu7iHlxcXFCt0yoHQ6TSAQoLi4WD80ZMLT/SyTie5nmUx0P8tkovt56Aab5qxCaiIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikicK3SIiIiIiIiJ5otAtIiIiIiIikiejFrpvu+02DMPgk5/8ZG5bIpHg+uuvp6KigsLCQq688kqamppGq0kiIiIiIiIieTUqofvll1/me9/7Hscff3yv7Z/61Kf405/+xG9+8xuefPJJ9u/fzxVXXDEaTRIRERERERHJu7yH7kgkwvvf/35+8IMfUFZWltseCoW46667+MY3vsH555/PmjVruPvuu3nuued44YUX8t0sEREREREZhGXZPLqliR8+vYtHtzRhWfZYN0lkwnHn+wWuv/56LrnkEtatW8eXv/zl3Pb169eTTqdZt25dbtuyZcuYM2cOzz//PKeddlq/50smkySTydzjrq4uANLpNOl0Ok9XIRNd972he0QmA93PMpnofpbJZDLez49vbeLbj+4kkspQ6HWTzaQ5b1n1WDdLRsFkvJ9H2lDfm7yG7l/+8pe8+uqrvPzyy32ea2xsxOv1Ulpa2mt7dXU1jY2NA57zv/7rv7jlllv6bH/ooYcIBALH3GaZ3B5++OGxboLIiNH9LJOJ7meZTCbb/fyh+Ye+j+9az4O7xq4tMvom2/08kmKx2JD2y1vo3rNnD5/4xCd4+OGHKSgoGLHz3nTTTdx44425x11dXcyePZuLLrqI4uLiEXsdmVzS6TQPP/wwF154IR6PZ6ybI3JMdD/LZKL7WSaTyXQ/P1nbzI+fr2dHc5jOeJpCrxvLtrl4ZQ23vHPVWDdPRsFkup/zpXvU9WDyFrrXr19Pc3Mzb3nLW3LbstksTz31FP/7v//L3/72N1KpFJ2dnb16u5uamqipqRnwvD6fD5/P12e7x+PRzSCD0n0ik4nuZ5lMdD/LZDJW97Nl2Txe20xda5T5lUHOWzoN0zSO6ly7O5JE07CoppSXd7eTsg2mlwQ4b8V0/VudYvTzeWBDfV/yFrovuOAC3njjjV7bPvjBD7Js2TI+97nPMXv2bDweD48++ihXXnklALW1tTQ0NHD66afnq1kiIiIiIpPS47XN/PDpOpIZC5/bqZd8wfKjm389vzKIz20SSWSYVx7khNklXLSyhvOWThvJJotMCXkL3UVFRaxa1XvoSTAYpKKiIrf9uuuu48Ybb6S8vJzi4mJuuOEGTj/99AGLqImIiIiISP/qWqMkMxZLa4qobQxT1xo96nN1h+uR6DUXmeryXr38SL75zW9imiZXXnklyWSSiy++mO9+97tj2SQRERERkQlpbkWAWCrDU9tbKPK5mVtx9EWGTdM4Yi/5SA5lF5nsRjV0P/HEE70eFxQU8J3vfIfvfOc7o9kMEREREZHJye7xJ49Gcii7yGQ3pj3dIiIiIiIyMurbYgR8blbPLaO2MUx929CWMxqOTMbifx7bzl/ePEA4keGcJVVsb44e01B2kclOoVtEREREZBLoLn5W2xjG5zaZXxkc8df4n8e2c89zu0lmLDKWxZPbWphfWZiX1xKZLBS6RUREREQmgf6Kn420zQe6yFo2s8r87O2IU1Tg5kNnz1dVc5EjUOgWERERERnnLMvm0a1NPLK5CYB1K6q5YFl1r+JlgxU/GwkrphfzUl07jaEEPrfJ21ZN11xukUEodIuIiIiIjHOP1zbz9Ye2sbcjRsayeay2hfedHOKT65ZgmsaoVRP/+PmLAafHe8X04txjERmYQreIiIiIyDhX1xolnMxgGgaZrEVnNMUfX9vPCbNLuWB59ahVE3e7TW68aOmIn1dkMjPHugEiIiIiInJk8yuDFPncJNIWlm0T8Llwu4xc1fC61ijJjMXSmiKSGUvVxEXGEfV0i4iIiIiMc+ctnYZl2/zkud1sbYxQVOCiqtCXqxo+GpXLReToKHSLiIiIiIxzpmlw4YoaLlhW3WfuNoxO5XIROToK3SIiIiIiE8RAFcpHo3J5vg2lQrvIRKTQLSIiIiIiAL2qoM+tCABQ3xbLa0X0bo/XNvP1v21jb2ccsNm4N4RpTPwPE0QUukVEREREBKBXFfRYMgMGBLzuvFZE77arJUJLJIFl27hNg3Aio4JwMikodIuIiIiICJZl89CmRhraYyysKqQxFMcwDVbPKaO2MUxda5T7N+7ne0/uZEdzhAKPizMWVvBvb1vG3IpjL9wWTmSIpbIkMxYAxQUeFYSTSUGhW0RERERkHOk5xPtoh3VnMhbfemwbz2xrpajAzQdOn8uFy2uOeJ7Ha5vZuDdEKJ7i1YYOSv0eCn3uXEX0vR0xvvznLQDMLvfTGU3zlzcbeXl3Ow9+4mymFRUc03UXFXiYXlIAhkEoluKMhRUqCCeTgkK3iIiIiMg40nOI99EO6/6fx7bzo2d2k0hnAahrjeE2zSOep641SsDr4i1zyti0v4uKQg9nL6mipMDLnHI/N/3uTQDetqqGOz+whqauBBd8/UlaIym++/hObr5s5VFesWNBVZDqYj/JjEV1UQEXrzryhwQiE4VCt4iIiIjIOFLXGiWZsVhaU5Qb1j1cmw90kc5aeF0macumK5Ee9DzOWt8u9nXGSWctIsks63d38qGz51Pi99AeSwHw1lU1AFQXF7B6TilPb2/lyW0tw7/Qw2jZM5msFLpFRERERMYRJ/yauWHdRzOvecX0Yp7b2Zbr6R7K/OjukPuLlxoAOHleOduaItS1RplWfGjoeGWhr8/3+zrjw27j4SbDsmci/VHoFhEREREZR0aix/fj5y/Gwu41p3uw8/QMvT98uo5tTZFc6I+msv0eY9v2sNsmMtUodIuIiIiIjCMj0ePrdpv860XL+NeLhn9sf6H/1YaO3POtkWTu+7aoM+R8Zqn/mNorMpkpdIuIiIiIjGMjUc18OPoL/cfPKqUs4KEjluavbzbyzhNn0tSVYENDJwDnLKnKW3tEJjqFbhERERGRcWwkqpkfK6/b5DMXL+Pff/8Gf3mzkbPveIzOaJpIMkN50Mu/nLtwVNsjMpEodIuIiIiIjGMjUc18JFx96hwCXhfff2oXO1qc+d5vXVnD5962jOriY1ujW2QyU+gWERERERnHRqKa+Ui5fPVMLl89c8xeX2QiUugWERERERnHtH61yMSm0C0iIiIiMo5N1PWrLcvm0a1NPLK5CYDzl0/DNAzq22KjUhBOZLxQ6BYRERERkRH3eG0zX//bNvZ2xgGbZ3e0UVjgJuB1j1lBOJGxoNAtIiIiIjKODbZk2OE9yutWVHPBsuox70Wua40STmbwuU0MA7qSaWwDVs8pG9OCcCKjTaFbRERERGQcG2zJsMN7lDfuDWEaYz8kfX5lkCKfm1A8DdiU+r0U+tzjoiCcyGhS6BYRERERGccGWzLs8B7lcDIzLnqRz1s6Dcu2jzinW2QqUOgWERERERnHBlsy7PAe5cpC37joRTZNgwtX1HDhipoB9xmvQ+NFRpJCt4iIiIjIODbYkmGH9yivW1E9YXqRH93axC33b6Y5ksRlwGvjZGi8yEhS6BYRERERGccGWzJsKD3KI2Wwom7D9cjmJprDCSwbLKA1nBwXQ+NFRpJCt4iIiIiIDMlgRd2OhmkYWLaNZdu4TMbF0HiRkWSOdQNERERERGRi6FnULZmxjrlXet2KauaWByj2e6gMenn3ybMnzNB4kaFST7eIiIiIiAzJYEXdhuuCZdWYhjFiw9VFxiOFbhERERERGZLBiroN12Dz1UUmA4VuEREREREZEoVkkeHTnG4RERERERGRPFHoFhEREREREckTDS8XEREREZmCjnbN7ZFeq1tkslPoFhERERGZgh7d2sTX/7aNcDJDkc+NZdtcuKJm0OPysVa3yGSm4eUiIiIiIlPQI5ub2NsZJ5HOsrczziObm4Z0XG6t7upCmsMJfvFSA49uacKy7Dy3WGRiUk+3iIiIiMiUZWMYzteh6l6r+6Xd7bSEkwD88Ok6QD3eIv1R6BYRERERmSJ6zseuKvYxsyxAJJmhstDHuhVDC8zda3P/4qUGAE6eV862pgh1rdEhHZ/JWPzPY9vZfKCLFdOL+fj5i3G7NQBXJi+FbhERERGRKaLnfGyvy+DiFdUU+z25gmhD0XOt7h8+Xce2pgg+t8n8ymC/+x9eeO21hg5+/Hw9Wcvmpbp2LGxWzy5TYTaZtBS6RURERESmiNx87JoiahvDFPs9fOjsBUd1ru6Q3jMs9+fwwmuxVIasZVNT4qcxFOeZba2s392pwmwyaSl0i4iIiIhMEd3zsWsbw0fsnR6Knj3eR9In6Be4cZkGjaE4LtOgqMBNNHXo+aEOUxeZKBS6RURERESmiKH2To+kw4P+3582j9X7Q7k53StnFnPPs/Uj8kGAyHik0C0iIiIiMkUMtXd6MIfP0z7SPOz+gv5Fq2p6ncttmqP6QYDIaFLoFhERERGRYTl8njYMPA97sKA/Uh8EiIxXCt0iIiIiIjIsh8/TrmuNDqv3W2QqUegWEREREZFh6a8g21B7vxXOZapR6BYRERERkWHpb572j56t69P73Z/hDE0XmQwUukVEREREZFj6m4c91OXI+huaLjKZKXSLiIiIiMgxG+pyZCO5VrjIRKDQLSIiIiIix2yoVcjHYq1wkbGk0C0iIiIiIqNGS4TJVGOOdQNEREREREREJiuFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyROFbhEREREREZE8UegWERERERERyRP3WDdARERkKrBtm1isjlSqA6+3jEBgPoZhjHWzREREJM8UukVEREZBLFZHZ+d6LCuDaTr//QaDC8a4VSIiIpJvCt0iIiKjIJXqwLIy+HyVJJOtpFIdBINj3So5Etu26dhTT2PtZmLtrWRSSaxMBk+Bn6JpNcxefRLB8koA9mx4hfaGOqLtrdiWBcDp13wE061ftUREpjrN6RYRERkFXm8ZpukmmWzFNN14vWVj3SQZRMeeeva/+Rrhxv2kYlHcvgIKiktIxWO07d7Jmw/+gWw6DUDb7h3EuzrxFPjHuNUiIjLe6ONXERGRURAIzMe2baLRnYDTi2rbtuZ1j2PxUCdWNkP53PnEQx1UzFvEzONOpH79i+x97RUyySTxUAeFldNYftGleANB9mx4mT0bXh7rpouIyDii0C0iIpIn6XSI+vq7+n0uHH6TwsKleDxldHS8MOA55s69Do+nJF9NlH50DyuPdbSTTWeIhzrIptM01m6iadtm4qFOADwFfgqKSwHwBQvHrsEiIjKuKXSLiIjkiWG48Plqco+z2SjZbAzbzgJg2xZud1GvfQDS6U4sK4FhuDDNgr4nti1orYVYGwQqoHIpGJoxNlK6h5VnM2nAxl9ajmm6aNz6Zm4fX1ExKy58O26vd+waKiIiE4JCt4iISJ643YXMnn117nE0uovm5kfIZiOASWHhUoqKllBSclxuH8tKU1//QwCKilbgcvn6nri1FhqeASsDByuhU7U8n5cypXQPKw+WVxLraCNQWs7M405kwRlrSUUj7H7pOVrrdlD72EMc944rFbxFROSI9LG4iIjIKPH5arCsOADB4CIKCxf32Scc3kw26+xTWrqm/xPF2pzAXVjjfI215a3NU1FBcQnZdJqOvQ1k02kKip3h/YZh4CssYtaJzt9LrLOd1l3bx7KpIiIyAainW0REZJR0db2ObWcxDBdVVef1KaJm2zadnesBCLqn4T2wuf/h44EKp4c70uh8DVQ42xuegf3rIdEJ2TR4g1AyB+afD0XTR+kqJwsDMLAtm9CBfZTNmoPpcgHO8PNuzhB0ERGRgSl0i4iIjALbzhAKvQZAUdFy3O6+i3RHoztIpzsBKO0MQ/qN/oePVy51vvac0w3QUQepKPjLnR7waAs0vwkdO+GsfwOXhkH3p7twWjzUib+klHioE5fHTdG0aiKtzRzYtJGmrZsoKC4mk0qRikYAcHk8VMxdAEDtEw8RaW4ik0zmzrvhd78AYN4pZ1Axb+HoX5iIiIwLCt0iIiIj6OHm+9kVqwVgYWAZ66a9A4Cn9/yYGdkYNjZ/i24ktnsjADW+mbxzujPvu7uX22cE8actZ/h4pLHv8HHD7H8O96r3gstz6PHOh6DucUjHnQBePHOEr3Zy6C6cZmUzmC43RdXTMV1uYh1tuNweiqtnkIpFSHR1YVsW3mAhJTUzmHXiGgqKigFIRaMkwl29ztv9OJNKjfo1iYjI+KHQLSIiMkK2ht/IBe6ebNumIh0FIGS6KfRV0L3AVJnHGRoej+8nkdjvbPMvAXNH3+Hjg3F5oHkT7H4SskknaAN4ghCoPKZrm8y6C6cFyiqIdbTh9nqZserEXM932ey5g66nftwl7xql1oqIyESj0C0iIjJMtm3TEN9FKN1BiaeMOf4FdGU6ea79Uap9M4hkwkSz4dz+sdgufHYGgKKS1byrYm2fc3Z2vgKAx1NKsGYteGog2grZBMRaoWXL0JYGS0Wga8+hx/4yOOEacPdTBV0A8JeU5nq2TZcbf0kZ5XPmjXWzRERkklDoFhERGaaG+C5e73qZrJXFZbqwbIvXQi9iYHB+5SX8qfFXvfbv6HACdQib16Ov8VT4FYLuQmYWzOXksrNwZ1NEozsBKC19C4bpOjh8fMuRlwbrb73uWafCzFMgGYLtf4Gm1+GNX8DJ/6LgPYCy2XMBevVsi4iIjBSFbhERkWEKpTvIWlnKvZW0p1rZFN5Ac+oA51deQrGntM/+s2a9h/sP/JKW1AGKXEWkjAThTIitkdfZl6jn72Zcy6JFn+r7Qj2XButvbvdA63UbBhSUwrxzndAdbYLGjTDrlJF+KyYFwzDUsy0iInmj0C0iIjJMJZ4yXKaL9lQrWbI0Jw6wOLiCxYUrBjzmjPLzKPNW4DLc2LbNS51P81roRcKZEHWx7SwpXNn3oIGWBuvWM5SH9sLup5153MEqp9e7rcf88qyKeYmIiIwFhW4REZFhmuN3lokKpTsIZ7poTTWxK7aNuvrtAGRsZ+3mutg27qr/Fh+Y/S9U+qpzxxuGweLgcl4LvQhAJBOmXwMtDdatZyjPpiCyH0K7nW0uD6Rjzn4uH0zrJ9SLiIhI3il0i4iIDJNhGMwNOOsu14bfBCB7sFBaTxYWlm1h2RleD73OsqIT8JrOWtk7o4d6oYvcxQO80ABLg3XrGcrD+5yQbWedAG5lwFcCZfNh/nlOQTUZMsuy2LPhZaJtrQQrKpm9+mRMc5AidiIiIv1Q6BYRERlAf1XKAepjO6mP7QBgbmARH577r72WlPr5nu8RyXZR4ZnGyWVnkbbSPN/xBC92PEWxp5SMlSZysLp5qaeC+YElR9fAnqG8ZQskuw7N755z1pEDuxzRng0vc2Dz69iWTVeTs5Tb3DWnjnGrRERkIlLoFhERGUCvKuWGi+bkAWKZCHsT9SStBADNqQO9er4BMgd7vSPZLl7vepkVRatZXXIae+O76cp0krUzlHrKmRdYzInFp+A2R+C/48GGosuwRNtasS0bXzBIMhol2tY61k0SEZEJSqFbRERkAD2rlO9P7GVHZDOWbROzorgNNy7DRSqbIpTu6HXc6pJT2RWtzVU3j2bCnFJ2NqeUnZ2/xg42FF2GJVhRSVfTfpLRKIZpEKyoHOsmiYjIBKXQLSIyQT3W1sXudJb5fh/rKooxewxvlpHRs0q5jYVpuCj2FBNPxMjYGWxsClx+klaC10Ov5Iag9zzOZboo8Wg+9UQze/XJAL3mdIuIiBwNhW4RkQnq7n2tRDDwHSzudFFlyRi3aPLpWaU8aSU4kNhDKpukxF2K3xWkyF2M3xWkMbGXrJ3FZbr6HNdzLviosi1nHe+ew80NFQIbKtM0NYdbRERGhEK3iMgElbQslhcXsiUSpy6eHOvmTEo952r3V1TNMAxeD71C1s7mhpKH0h0Ygd5zvEdVd9hu2QyhPeDyOsuHgYafi4iIjAGFbhGRCcpnmmyJxPGZJvP9vrFuzqR3eLG0buNuKHlrLTQ8A7EOyMSgZA5kk06Pt4iIiIw6hW4RkQnqgzMq2NRZSzUHONEux7bPxdDw4VE3LoaS9xRrc5YNC1ZCqAGiLRAod4aYi4iIyKhT6BYRmaBOMDZSFf8RWSvF3rgX04DKyvPHullTzkA94GMmUOGs051JQkEZFM+CaSu0hJiIiMgYUegWEZmg4vEGslaKwsIlRCLbiMV2j3WTZDzob71ujYAQEREZMwrdIiITlN8/B5fpJRLZhsv0EgjMG+smyXig9bpFRETGFYVuEZEJqrz8LNxug1hsN4HAPCoqzs3r69m2TUdThEQ0RUHQS1l1IYbWBhcRERE5IoVuEZEJyjDMUZ3D3dEUYf/ONuysjeFywnZ5TdGovb4cBa3VLSIiMuYUukVEZEgS0RR21iZQ7CPWlSQRTY11k2Qw3cuHWRmnuBpo6LmIiMgo08fdIiIyJAVBL4bLINaVxHAZFAS9Y90kGUz38mGFNc5XrdUtIiIy6tTTLSIyReza9d/U7f6ffp8779xazIM9oeHIVurqvk1n50tkMmG8nnJKStewaqVzbM853VOFbdvEYnWkUh14vWUEAvMnxnz27uXDIo3OV63VLSMsuStE+Mk9pPaGsaIZAEovX0ThadMBsJIZuh6qJ7m7i2xHAitl4S714T++kqJzZmH69KuoiEx++kknIjKJ2LZFW9sTvYqrGYfN4fV4yvH75/Ta1h0gOztfYcNr12BZCVyuQoLBxWSzUVpaHsEwjCk7hzsWq6Ozcz2Wlcl9OBEMLjjm8z7//PO89tprdHZ2kslkCAQCzJ49m7Vr11JTU9Nr31AoxJ133kkikQDg/e9/P4sXLz7yC/S3fJjICLEtm+irTSS2dWB4Xf3uY0UzRJ7dD24DT1UAupJkWuOEH9tDel+Eyg+uGuVWi4iMPoVuEZFJpK3tCRoa7iJrpXCZzvDvw4utVVacy4oVX+1zrG3bbNn671hWgprqd7Js2VdwuQoAyGQi+W/8OJZKdWBZGXy+SpLJVlKpDoLBYz/v7t276erqwuv14vP5CIfDbN68mbq6Oj71qU/h9Tp/h5Zl8fvf/z4XuIdMy4dJntiWTdcj9cS3toHLwLbtfvczPCYlb59P8NQaTJ8bO23R8oPXSTWESdR2YMXSmAHPKLdeRGR0aU63iMgkEovtJmulKCxcQtZKEYvt7rNPc8vfePyJFTz9zGm8tvFDhMObAIhEthKL7QTAxub5F9bxxJMn8Oqr7ycWqxvNyxhTtm0Tje6io2M90egubNvG6y3DNN0kk62Yphuvt2xEXuuEE06gurqa0tJSysvLWbFiBQDxeJzW1tbcfs899xy7d+9m5cqVI/K6IkNlWzbxLW2En95LfEsbtuWE60RtO9HXmrFjGbAGPt5V5KVo7aFh5IbHxDvr4IgZA3BNgGkaIiLHSD3dIiKTSCAwD5fpJRLZhsv0EgjM6/W8YbjweisxDDex2E7a2h6no+NZTlrz214BvanpfgKBBWQyYTo6X+DVDe/n1FMexO+fNboXNAq652snk+3YdopMJkIy2Qi4cbmcHrhAYD5ArzndIyEUChEOh0kkEiSTSTKZzMHXC1BR4cy/3r9/P4899hhLlizhpJNOYtOmTSPy2iJDkahtJ/z0PshY4Hb6avzLK8i0xjFdJrbPjZ3IQKb/nu7DZSMpYm86Hyj5T6jSnG4RmRL0k05EZBKpqDgXoNec7m41NZcxe/Y1eDylALS1PcVrGz+IZaXYu/dnlJadmtu3vOwsTjzxbhKJfTz/wgVks1EOHLiPBQs+MYpXMzq652un0xEsKwaY2HaWgoLpWFbq4FDyBQf/HNtrWZbF9u3baW9vp7y8nLKysoM969HcPqWlpVx99dX4fD5SqRT33XcfgUCAs9e9jW/8bQsPJY4njoff/WwHq2a1cdsVxzOnIkAkmeEbD23jwTcO0BZNMr3EzxVvmcnHzluE26WBbXJ0Mq1xyFh4aoKkG6POY8Bd6cco9GDaNpYL3BV+0vXhI5+rLU7r3ZuwulJ45xZT9q5Fo3EJIiJjTqFbRGQSMQyzzxzubof3zlZUrMXjKSOd7iCR2E8ycSD3XCKxj7a2J6isPB+Pp5xUqoVEYm9e2z5Wuudru1w+stkIbneATCZKKtWO11s6YkPJAbZv387zzz9PNpvF5XJx2mmncfHFF9PW1kZBQQE7d+5k06ZN/OY3v+FDH/oQjz32GG1tbVz2d+/j6ns2sKc9jomHYiNBib+UVxs6aAonmFXm57p7XubFunY8LoPZ5QF2t0b51iPbaWiL8Y33nDhi1yBTi7vSD26TdGMU3KbzGChYWg44odxd6cc9LUDTV18Z8DzJ+i7afrIJK5qhYHk55e9bhjlA8TURkclGoVtEZIrYXf89aqrfQUHBDADa2p8hne4AoMA/E6fMhwlYpDNdxGK7icf3kU63A+A/bKj6ZNE9XzudjuQqvXs8xfh8NRQWLhqxoeQA7e3tZLNZpk2bRnNzMx0dHZx++um552fMmMGmTZtoaWnhjTfeoLGxEYB//+UL7ElXUmbGudBTS8BI4864WXTcMo6fVcJDmxt5sc75e/q/D6zhguXV3PNsHTf/aTO/27CPfzxrPqtmlozYdcjUcXi47n5smAb+5YeWoMu0D1zkL/ZGC+2/2gYZi8IzZlBy6QIMU3O5RWTqUOgWEZki9u37OTt3fhWfbzpgk0w6PdsuV4DZsz9IIr4Hf8Fs4ol60uk2GvbcRd3u/8G2s3i9Vcyc8d6xvYA86Q7V3XO6TdOL11uel7W4y8vLcblcNDc3A86c7kwmg9vt/He8ffv23L7pdBoA24Zd6VKnrSR5KLWEiO2jyEhyUavNe90unqhtAaDAY3Le0mkAvO246dz8p80APLmtRaFbjsrh4fpw8Tdb6fxLHWQPzenuerie8NN78c4uovTt82m/dyvYgMsgtSdMy50bc/uWXr4I78zCfF6CiMiYU+gWEZki5s79Z/btu5dYrA7LSmIYXjzuYubP/wSFwcUEAwtZsgQONP6Brq7XSKVa8HjKqay8gIULP4PXO/Av3hOZYRi5Odv51r2udnt7O16vlz/96U+88sorlJeXk0gk6OrqAsDl9vBaV4DozNPpKDiB1EbnA5J9VikBUnjJ0mEH+FUDnPPGAfaHnF7GsoAX82APYmWhL/e6+zrjeb82mTpsyyZR206mNU6mM0m2rXcvtxVNQzRNttiHnbGdwA2QtUnt6T3v20pkRqnVIiJjR6FbRGQKsG2LVLKJbCaGaQYAF6WlbyGTiWJZzi/M3fPBB5oTLkfv8AJqp556Kslkkrq6Ovbt20d7ezuWZVFcXExBWQ0vxqbxxJsdHAglsKxD6zFVB91clHHmzT7pP4OGzhQ/fm43Pk/fubEDrZsscqwOr2hecc2KI/aGz7rt7FFsnYjI+KPQLSIyBbS1PUFj059Ippx1piFLNLqLgH92n2XFZOQdXkANYOnSpVx11VV99v3h07sIv9FIcYFNQ3sMt2nQ3VVYURLgSx//IgA3/GIDDZ372dsR5+zFlQC0R1NYlo1pGrRFU7lzziz15/kKZbLq2avdPac70xrHzliYBS4yLXHim9ooWFquedoiIgPI6xoi//Vf/8XJJ59MUVER06ZN4/LLL6e2trbXPolEguuvv56KigoKCwu58soraWpqymezRESmnFhsN4bhxu0OYllpXK4iysvPZM6c63otKyb50bOAWjabpb29fcB951cG8blNuhJpPAeX+nIdnFveGU2RzlqksxZbD3Tl9j9nSRUAyYzF47XOfPG/vHGoGn338yLD1d2rHX+jlfDT+0jUtuOu9GOnsqTqw1ixDKk9YRK1A9/TIiJTXV57up988kmuv/56Tj75ZDKZDP/+7//ORRddxObNmwkeXOz0U5/6FH/+85/5zW9+Q0lJCR/72Me44oorePbZZ/PZNBGRKSUQmIfX6wQvl8tPTfU7mD//E7lq3ZJfPQuouVwuysvLB9y3uxDarpYIoUSa5q4k7ZEkT2xrZX8owdm3Pw5AY1cCl2nw0fMWcur8Ck6eV8bLuzv455+tZ055gLpWZ+3vd544Q0XU5Kj116tdevkivJvaSCQyeKoCWIlMbv1uERHpK6+h+69//Wuvx/fccw/Tpk1j/fr1rF27llAoxF133cW9997L+ec7cwjvvvtuli9fzgsvvMBpp52Wz+aJiEwKtm3R1vYEsdhuAoF5VFSc2ydMd/dmH2kfyZ+eBdTKy8tzj/tjmgYXLK/mguXVvba/srudrz1Uy8Y9IQo8JmctquTTFy1h9RxnHfEfXXsyX39oG3958wAN7TFmlPq54i2zuOH8Rfm7MJn0cr3aTTEAUnvCJLd34F9ZQaY9gZ3MYnhcufW7RUSkr1Gd0x0KhQByn/CvX7+edDrNunXrcvssW7aMOXPm8Pzzz/cbupPJJMlkMve4u9JrOp3OLa8icrjue0P3iEwGh9/PbW1PsXfvT8haaVymh0zGpqJibZ/jSkrOpqTEKWiUyWSB7Ki1WWDBggUsWOBUSM9ms2Szw3v/T5hZxE8/eFKf7d33QYELPv+2JXz+bUt672BlSVvj9+9aP5/HN9eCIsxZAdKpFO7KANlEhkRLhOCp0/Fnq8m0J3CXF+BaUKS/Q3Q/y+Si+3lwQ31vDHuUyptalsVll11GZ2cnzzzzDAD33nsvH/zgB3uFaIBTTjmF8847j9tvv73PeW6++WZuueWWPtvvvfdeAoFAfhovIiIiIiIi0kMsFuPqq68mFApRXFw84H6j1tN9/fXX8+abb+YC99G66aabuPHGG3OPu7q6mD17NhdddNERL1SmtnQ6zcMPP8yFF16Ix+MZ6+aIHJPD7+eePd2m4aa09C243SX4/XMoLz8LwzD79IbPmvUP/faGS/5ZlsXOnTvp6OigrKyMhQsXYppTd6i/fj4PX3pHE9aeDqeovQHm7DI8i6oHPe5o2ZZNcntHrlfbt7hMlcoHoPtZJhPdz4PrHnU9mFEJ3R/72Md44IEHeOqpp5g1a1Zue01NDalUis7OTkpLS3Pbm5qaqKmp6fdcPp8Pn8/XZ7vH49HNIIPSfSKTSff9XF19Hm63QSy2m0wmREfny1hWCpfpxe02qKw8n1SqHpsYxcVLiES2kUrV69/CGKmtreWll17KLR/mcrlYunTpWDdrzOnn89BZsQwmJobfg51IY8QyeX/vvKv6hvr+lhNTGHfofpbJRPfzwIb6vuQ1dNu2zQ033MDvf/97nnjiCebPn9/r+TVr1uDxeHj00Ue58sorAeeXkYaGBk4//fR8Nk1EZNIwDJPKSqcYZUPDj7CsFIWFTriOxXYDTvVyl+klEtmGy/TmbW3uHzy1i0e2NLGrNUoolqaqyMepC8r55AVLmFPhTAFKZy2+8/gO7nt1L42hBBVBH28/bjqfvmgJQd+olhoZEz2XD2tubu53+bBMxuJ/HtvO5gNdrJhezMfPX4zbPXV7w6U3s6iAbHsUO5F2erqLCsakHd3LiZGx4OD96V9eMSZtEREZz/L6283111/Pvffeyx//+EeKiopobGwEoKSkBL/fT0lJCddddx033ngj5eXlFBcXc8MNN3D66aercrmIyFEYKFz3V708H+55bjf7Q3EWVAYp8JjsaY/zu1f38fT2Vh779DkUFXj47G9f5/cb9mEaMK8yyJ72GD96to5N+0P84p9Ow5zkPWVDWT7sfx7bzj3P7SZr2bxU54TyGy9Sb7g4XIucZeWscAKzqCD3eLRlWuOQsfDUBEk3RrVsmIjIAPIauu+8804Azj333F7b7777bq699loAvvnNb2KaJldeeSXJZJKLL76Y7373u/lslojIpDVQuO7ZGz7SLMvm8dpm6lqjnDK/jBsvPI3Z5U6v9n/+aTM/eraOlnCSZ3e0MavMz+837APgi+9YyTVnzOORzU186Cev8GJdOw9tbuStq6aPTMMefgh27XS+X7gI1l0ItVvhiccHPuYdl8GMmQM+/eknPs1D9Q8B8NZ5b+Wr53y11/PRdJSr7r+KvZG9APy/0/4f71767l77DGX5sM0HushaNjUlfhpDcTYfGNqcMZkaTNPEXNL/NLzR5K70g9sk3RgFt6llw0REBpD34eWDKSgo4Dvf+Q7f+c538tkUEZEpIZ/huj+WZfOtR7bxx437cZsGVUU+tjWFc6H7lPll/OjZOgB8bpMnaptzx75tlRMazl82DZ/bJJmxeHJby8iE7q1bDwXungr8MO2wXsFIBGLOGsT4B14F4/fbf58L3AP5yotfyQXugZimOegc7hXTi3mprp3GUByXabBiugqFyvhTsNQZpdFzTreIiPQ1+SfPiYhI3jxe28wfX9tPaySZm49d1xoFIGvZ3PvSHgDmlAc4Y1EFD29pyh1bUegUxTRNg/KglwOhBPs6E8feqFAInnsGqqudQB2NHnpu7lznT0+/+ZUTumfNgrKyfk+5p2sPt710GydUnUBjtJGmWFOfff66+6/cv/N+Lp53MX/b/bdjuoSPn+/0fvec031EtgWttRBrg0AFVC4FQ3PAJb8M09AcbhGRIVDoFhGRo2JZNg9taiSczOB2mUSSGfxeF/Mrg8RSGT7+iw08ta2FqiIfd11zEj63a8BzDWFg1FAbBY89AoYB56+DP91/5P0bGqC7kNkJJ/a7S8bK8G9P/xumYXLb2bdx3d+u67NPY7SR/3z+P1lRsYIbVt9wzKHb7TaHN4e7tRYangErA+bB/9qrlh9TG0RERGRk6GNwERE5Ko/XNrNxb4hkOkMynaXQ6+KdJ8xg1YwS3vO9F3hkSzMLKoPc989nsLi6CIAZJYeqLLdFkoAT3jtiKQBmlh5jFeb1r0BzM5x1NhQPYUj2xtecrxUVMGt2v7vcufFOXm99nc+f9nlmFc3q87xlW9z09E1krAy3n307bnMMPs+OtTmBu7DG+RprG/02iIiISL/U0y0iIkelrjVKwOvipHnl7GiJctbCCt5+3HSuuPM59nXGOWVeOd//hzWUBry5Y85ZMo2vPbQNgL+82cg1Z8zjsa3NJDPWweerjr5BLc2w4VVYvMT5M5jWFtjvFHXj+BP73WVT6ybueuMuLl1wKZcuuLTffX62+We80vQKt5xxC/NK5rEvsu8oL+AYBCqcHu5Io/M1oCG/xySddj7AqauDWBRMEwoLnfvqhBNhW+0xFeQTEZGpRaFbRESOyvzKID63i3Aiy5yyABetrOGjP3+VfZ3OskGRZIZr7345t/97T57Ne0+Zw2UnzOD+jfu55U+b+Mnzu2lod4qYnTKvnItWHENF5vZ2Z5z6rp1Qt8vZlsk4X+t2wV0/gA/8A/icueRs3Oh8LSyERYuwbIun9z5NfVc9c4vncvass9neuZ2sneXh+od5tOFRABIZZ975I/WPcMrPT+H06acDcNtLt3HbS7f1atLtL93O/Tvv52dv/9nRX9dQVB4cit5zTrccvWeedoI1OPP8Uynn/nrxBXC7oaj4qAvyiYjI1KPQLSIiR+W8pdOwbJtHNjtFxSzbzvVYA32Wueruxf76u09gXmWQ3726l4b2GOVBL29bNZ1/vXjpyKzRnc323WZZzp9u4fCh6uarjgPT5Ok9T/KTzT8hmU3ic/l6HZ7MJvucMmNnyGQy2DgT0uOZvmsUp6xULqTnlWFqDvdIajzgfJ09G95+qfPhzT0/cu6tcNi5Z46iIJ+IiExNCt0iInJUTNPANAwa2uMkMxYN7bv5z3eu5ILl1Uc8zuMyufHCJdx44RCGgA/GtqF+t/MH4OK3OWHIOBjef/4ziIQPrdPd7Y3XnRDu9cLyFQDUd9WTzCZZXLaY7R3bqe+q5x9W/gOXL7q810te/NuL2R/d3+863QD7Ivt4631vBfpfp1vGCduGhnqn2n1JCczpcd/UTIeuLtizB379S6enO5t1th9/Qt9zDaEgn4iITF0K3SIictTqWqMkMxZLa4qobQznlgsbNQ318PJLTu8jOEXUDGDuvIGPSSZh6xbn++UrnOANzC2ei8/lY3vHdryml65UFz/Z9JPcUHNTS3BNLg318PpGJ0y7DlbW775v1p4D2LBtG3R0ONtM0ym45/P1PdcQCvKJiMjUpdAtIiJHzZnXbVLbGMbnNplfGRzdBoRCTi+k++B/Z6m0s63b+z/Q9xifD/7xQ302nz3rbMDp8e5KdbG+aT1t8TYydoY3Wt/goyd+FNMw+dtVR14ObGbhTN645o2jviQZJaGQE7jLK6C9rfd98/pGJ3DX1MBFb4VEHO7/I2x60+kNP/OsQ/sOoSCfiIhMbQrdIiJy1M5b6hSTqmuNMr8ymHs8akpKnJ7q7p5uv9/ZdhRMw+Sc2ecA8JNNP6Et3kYkHSGajvJg3YMcV3lc7nmZBEpKnB7u9jbna3GxM02hvd0ZPQEwf4FzT/n9UF3jPL9vb+/zHFaQT0RE5HAK3SIy4di2jdUSxoomMYM+zKoiDGMECnDJkFiWzeO1zb2C9gXL+77//e03IoXSepoz99C8boC5851tx2hu8VwydoZoOkrQE8RtuKnvqj/m88o40n2fdM/ptnF6uNNp554CaGlxvq/bBY2Nzja359A5+inIJyIicjiFbhGZcKyWMNm6VmzLxj4Y4lzTise4VVPH47XN/PDpOpIZC5/bCRn9FU8b6n7HxDBg3nznzwg6e9bZvNH6Bg/WPYjbcFPpr2Ru8bGHeRlHDOPQHG7bhqeecIqnlZaBx+OE7x3bnaHjicSh6vdVlYfO0U9BPhERkcMpdIvIuDHUHmwrmsS2bMyiAqxwAiuaxDUG7Z2qhlo8bcyLrB0D0zD56Ikf5bjK43qt2y2TSM/q5cmkU4QvmYSmRggGnWW/2tud3mzbhgI/eD1QUuocP0BBvqnOtm0a4rsIpTso8ZQxx79AI5FEZMpT6BaRcWOoPdhm0IdtGljhBIZpOI9tm2xjiOyeduxIEtJZ8LhwTSvGvbQGw+PCzlqkN+7B6opDMgOGgVHgwawpxr1wGoZLQ0OHYqjF08a8yNox6gq9xny7ntn+MFbyNep311JQMJ3y8tPw+apoa3uOjo4XBjx+7tzr8HiObn65jIKe1cvjcafnu7oGOjudAmprz3W21e/uXeW8u2bAAAX5prqG+C5e73qZrJXFZTofh84NLBzjVomIjC2FbhEZN4bag21WFeX27+4Rt1rCZN7cB5mDQ0D9Hkhlye5px+qK4z1tIVg2VnMYw++BwgLsZBo7miS7swVSWTyrZo7exY4THR0vUd/wf3R1vUE67awzvHTpl5g18+pe+4UjW6mr+zadnS9hZMJ8aEkJEXsFhTW3DVg8bcyLrB2jeHwP2Wwcj6cEy8qQTncQjW4nHt/DvHn/hNtdhM9X0+uYdLoTy0pgGC5Ms2CMWi5D0rN6+YH9zjDxdAqKiyAQdIaOl5TA7DmH9u9ez1sGFEp3kLWylHsraU+1Ekp3jHWTRETGnEK3iIwb/fVg98cwDFzTinsF8mx7tFfgds2twDWtmNRT27BDcazGEOb0EnwXrcA4WOzItmxST9Vix9NYHRNn6PNICkc20d7+LH7/7FzoPlxn5ytseO0aLCuBy1VIMLiYbDaKkXiB848wR9s0jZGfwz2KqqsvwTQP/TfZ1vYsHR0vYlkJUql2SkqOo6TkuNzzlpWmvv6HABQVrcDl6v/+lXGipMQpfHZgvxO+p02DqmnOEnTdIfzw9btlUCWeMlymi/ZUKy7TRYmnbKybJCIy5hS6RWTUDDZn26gsxOiMYYcTGEUFGJWFQz636fdgdZ/H6BvYrdYIrhmlYBik39iLFU5gJ9LOMHPALJtYQ59HyvSay5k5432kUq0893zf5bBs22bL1n/HshLUVL+TZcu+gsvl9OBmMpHRbu6oMk03kch2OjpexrJSpA/22LlcfrzevkEiHN5MNhsHoLR0zai2VY7CnLnOPO4d28F0OUPMS0udHm3L6n/9bhnUHP8CgF5zukVEpjqFbhEZNYPN2bZbI9gdMbBs7I4YdmsEej7fI7STzmK7TVyFBZhVRZizy2FXKyTS2LEUmW1N2PHUoWOT6UPtCCewQ/HcY3NGKe4VM/J56aPCti3a2p4gFttNIDCPiopzMYwjz1P3DNILFYlsJRZzlkSysXn+hXVkMmGKi1axaNG/UVx83BGPn+iy2RjJZGPusdtdwvTp78Q0exfNsm2bzs71AASDC/F6y0e1nXIUDMOZl+339w7Yh6/ffZTrvk9VhmFoDreIyGEUukVk1Aw2Z3vQ57tDezKNncyAzwM+58eYa1oxvtMWkq49gNUSxo4mMYoKsNNZiKWcX7AP8p2xCCubJVvfRnZnC9b+TjJ+D54lvefnTjRtbU/Q0HAXWSuF62AorKw8/5jOGYvtyn3f1HQ/gcACMpkwHZ0v8OqG93PqKQ/i9886ptcYz0pKTqC4+HgymTBtbU8TidTS1PRnZs16X6/gHY3uIJ3uBKC09KQxaq0MW38B+/D1uzWHW0REjpFK9YrIqDGDPowjzNke7PnuUI7HBTYYbhPbsp2eb8Dwe3DNKMUs9mMU+8E04GAPt3HYuey2KHZLBA5WLM/ubMHOWkxksdhuslaKwsIlZK0UsdjuYz6nbWdz38+Y/m5OP+1hTj3lAQzDRTYb5cCB+475NcY7wzDweIopKzsFgFSqjXB4a699unu5fb7p+P1TryDfhDVnLhx/AixY6HydM/fQ+t3Hn+B81XJXIiJyjNTTLSKjpr+q48N6/mChNTuZAQPsjIXpc+fCuRWKk+2KY1s2RqEPqzkMWRsA1/QSsq0RDI8Ls8SPFU1iZS2w7EMvkLVyIXwiCgTm4TK9RCLbcJleAoF5x3xOn+9QIbSig0PJ/f7ZeDzlpFItJBJ7j/k1xqNsNk40WkdR0VIMwxlvEYvV5Z637UPTFeLx/SQS+wEoK9Nc7gmlO2CLiIjkkUK3iIya/qqOH/58z+AN9Cq21uu5w+Z0A2T3tmPt6QAT7FAMDuZp17wKzNIA6e1NZHc0g9flhOv4oeBklPgxvBP7R2JFxbkAuTnd5eVraW19bFhzvA9XXHwCLlch2WyEcNcbMBPi8X25Suf+EQj245FlpWhu/istLY8cXDIsRSYTBsAwvASDi3P7dna+AoDHU9pru4iIiAgodIvIONNfsbXudbi7e8Dd8yp7VT3vZpYGsNqj2DGngJoR8OJaUIl7dkXuebs8iBVJQiLtDD/3uDArgrhXTvwhwYZh9prD3dLyCDt3fYNMJoLbXYhtW1RVret1THPz39ix87Zew8h37fomDQ0/oLj4RFat/CYL5n+C7TtuZf+BX9MZWk8q1YxtZ/F6q5g5472jdn2jyTR9FBYuJZFoJJ0OYdsWbncRfv8syspOweNxCvylUh1Eo06hudLSt/R7X4qIiMjUptAtIuNKf8XUgCNWPe/mmlmGa+bA1bhdVUW4DhuyfrjBljWbSFpaHyWR2Idp+kgkQrS0PtondGeyYeLxhl7b0ul20ul2fL7pAMyZ84+43YU07LmbeHw3Hk85lZUXsHDhZ/B6K0btekaTy1VATc0lg+7n9ZaxaNGnRqFFIiIiMlEpdIvIuNI9b7tnMbWBqpofTUAe7JjBljWbSIzDvjv8nbFtC6+nnMWLPj/o8PMZM97NjBnvzldTRURERCYthW4RGVcGKqZ2eBCHowvIgx0z2LJlE0ll5QWEujaSyUTweiuorLyg1/PDXWLsaNYBFxEREZnqFLpFZFzpr5iaUVmIi75BfDgBubuHO7OnHZJpjLIgdiTZ55j+etonqsrK8zEMs1dI7qnnEmORyLZBlxjLxzrgIiIiIpOdQreIjDuH90a7oN+q58MJyLlzJjOQzEBHFMPn6btW+CDLlk0khxdWO9xwlxgbTki3bIun9z5NfVc9c4vncvasszHVKy4iIiJTkEK3iIw7Q+3BHk5Azp2zLIDVEYOgF9fs8j7HDLas2WRy+BJjh/eEH244If3pvU/zk80/IZlN4nM5H2ycM/ucYbcxlo7xfxv/j0cbHqU51ozbdDO9cDrvWPAOrl15ba/5+NF0lKvuv4q9EWft8P932v/j3Us1D31UZVOw61Fo3gTJLjBM8JdBzYkQqIJ4O7RuhUQnpMJg27i9hRzn9UE6Bp6Ssb4CERGREafQLSLjzkA92P0VQRtqQO4+px1JYvrcuGaXT9gCaSNlsJ7www0npNd31ZPMJllctpjtHdup76o/qjbe+uKt3L/zfgAWlS4inAqzvWM731j/DbwuL+9f/v7cvl958Su5wC1jZOsf4cCrzvfBasgkINIIO/4K/grwBqFrL3gCTghPRzESHSzwgLXlt7DmurFtv4iISB4odIvIuDNQD/ZwC6f1DOlGwIs5rwI7lprww8aPxLYsdm14hY7G/ZTVzGDB6pMwzJEZ1j2ckD63eC4+l4/tHdvxuXzMLZ57VK+5oXkDAGfOPJP/W/d/JDIJzvrlWSSzSQ5EDuT2++vuv3L/zvu5eN7F/G33347qtWQEdO52vlYsgdUfhGwanvxPsDJOL3jhQrBtqD4e5p4FgPXSnZhdDRhdDQOfV0REZAJT6BaRcWegId7DrSzeM6QbpoFrfiXu+VX5bPqY27XhFdb/+Q9k0ylcHqfY2cI1p4x6O86edTZArzndR+Mt097CnvAent33LO/647sIp8Iks0neMu0tXLPyGgAao4385/P/yYqKFdyw+gaF7iOxbWioh1AISkpgzlwYyjr0tgWttRBrg0AFVC51ho4frnS+M4S8bRs8datznJWB4DRwFzi93i4PhBrgpe9AKoKZ6HReonhOn2XtREREJgOFbhGZMA4fdm4EvGSbuwZec3sSLf81VB2N+8mmU1TOnkfrnt10NO4fk3aYhnlUc7gP94XTv4CNzf0772dH5w4APKaHJWVLKPYVY9kWNz19Exkrw+1n347b1H9rR9RQD69vhGwWXAf/NcydN/hxrbXQ8IwToLvf46rlffdbfjkkOqBjF6QizjbDhLL5ULbImcsdqIADG5xh5gc1Z0soW/EeVGpPREQmI/12IiITxuHDzm3bPuJw88m0/NdQldXMwHR72LvlTbLZLMlIBNuyRmyI+Wj7yeaf8Kedf2L1tNV867xv0ZHo4Nq/Xssva3/pFFULTueVple45YxbmFcyj32RfWPW1nwO7R8xoZATuMsroL3NeTwUsTYncBfWOL3Vsba++9gWbP6dE7hNN1StgmgTxFph74tguGDpO5x9q5aDlYVoE/abv2JatBlrxwNw3HtH7lpFRETGiXH224CIyMC6h5175lfhmlaMHUvlerJty86t693NrCrCNb8Ss6bY+TpJ53H3tGD1ScxatpKslcXlcrHj5ed56Af/y871L2Fb1lg3b1jimTjf2fAdbGzWzVlHeUE5C0sXcuK0EwF44cAL1HbUAnDbS7dxys9P4V1/fFfu+Ntfup0PPPiBUWtv99D+7S88w/o//4FdG14ZtdcespISp4e7vc35WjLEauGBCidIRxqdr4GKvvs0vwmNzhx8bBu69oHHD4XVzrb2nb33N11QNAOr5iTnYdNGiLYc5YWJiIiMX+rpFpEJa7Ce7O6Qbh4sqJbZ3drvMPTJxDBNfMEALtOFDXQ2HSAZjxFqagTGZn730UpkEmTsDACb2zcDkMwm2dnphDe/25/bN56J9zk+ZaVIZBKj0FLHkYb2j5t1y+ccLGjXc073UFQudb72nNN9uObNh763LfD4YNbpsOMvzjaXB0J7nOJq5QucbVYGo7NHGM+mh3c9IiIiE4BCt4hMWENdp3u4Vc8numQ0RjTUQTqZxLYs/EXFZNOpMZvfDUcXOssKylhTvYb1Tev5864/80bLG0TTUdoSztDmyxZexnuXvZdbz7o1d8y+yD7eet9bgdFfp7usZgYuj5fWPbtxebyU1czIPTdS65YfM8MY2hzuPseZ/c/h7sn0AAZgO3+iLVD7x0Nzu6e/BaLNsPm34PZDQSkkOzHTzgcmdmENRlHN8NsmIiIyzil0i8iENVCV88NNtYJq3mCQYEkZNjbh1lbiXV2U1kzvFQJH29GGzv8+77+56827eLzhcZpiTXhMD8dXHs/7lr+PSxdcmu9mD8uC1c4w6Z5zuruN1Lrl41rVcgjVQ7LLmf9tZZywXjwbZp8O01c7xdMqlkD4gBPADQM7UMWOTjfzjr8Wz1j0/ouIiOSZQreITHpTraBa+fSZBMvKyaRTuNweahYtYdFJp/UKgaPtaENnia+EG9fcyI1rbhzS/jMCM3jplCfItMZxW/7ccnHdbMsmUdvuPF/pp2Bpea/nj4VhmgMO3x+pdcvHtaplTk/6kZYVK57lrN/dQyadZvODDzLP40dERGQyUugWkUlvqMPQJ4v+elzHuor2aIRO27LpeqSe6GvNmC4To9ADgH/5oaJfidp2wk/vg4wFbrPP8/kyUuuWj2tDGYIuIiIyBSl0i8ikN9Rh6JPFkXpcx8pohM5EbTuxjS3YkTRZlwmxNPFNbb16szOtcchYeGqCpBujzuNRMFLrlouIiMjEo9AtIiIjrr/CafkOnZnWOJgGuEzsRAZMg9TeMIna9lxvtrvSD26TdGMU3KbzWERERCSPFLpFRGTEdIftxxoe443WNwh4AqNWrdtd6cdV5MWOZbBdBu5pAQy32as3u2BpOUCvOd0iIiIi+aTQLSIiI6a7Svne8F5CqRAnVp1IJB0ZlWrd3QE6vqmN1J4whsvAOKw32zCNUZnDLTIY27axWsK9ak0YxsgU9RMRkfFFoVtEREZMd5XyeSXz2NiykbpQHbOKZo1Kte7uQF2wtLxPhXIZQbYFrbVHrlIug7JawmTrWrEtG/tgzQHXtOIxbpWIiOSDQreITDm2bdPRFCERTVEQ9FJWXagephHSXaU8koowIziD4yqP4/w5549qtW71ZudZ61bY8TBkkuD2ATZUrRjrVk04VjSJbdmYRQVY4QRWNDllij2KiEw1Ct0iMuV0NEXYv7MNO2tjuJywXV4zuZcRGy39VSk31Qs6edgW7HkeYi3g8kA6Ci1bFLqPghn0YZsGVjiBYRqYQd9YN6lfacvizY4IdeEEkUwGr2kyp7CANRXF+FzOv+32ZJrX2sK0JFIkLYtij5uVpYUsLgmMcetFRMYHhW4RmXIS0RR21iZQ7CPWlSQRTY11kyYNLY01ybXWQqTJCd/ZlDOs3B7rRk1MZpXzQV/POd3j0SP722mMpzCAUq+bSCZLbShGWyLNJbMr6Upl+POeVjK2jc80KPK46UhleKa5k5RlsbKscKwvQURkzCl0i8iUUxD0YrgMYl1JDJdBQdA71k0SmRhibeAuACvjhG5XAVQtH+tWTUiGYeCaVjyuh5R3JtM0xp0PJU+tKmF5aZBQKsPv6ptpTaapC8dpT6XJ2DYuA66cV43PZbK+tYvXOyK81h5maUkQt6npOyIytSl0i8iUU1bt9Lz0nNMto8uybR5p66IunmS+38e6imJMzasf/wIV4Dv478W2oOYEqFo2tm2aBGzbpiG+i1C6gxJPGXP8C0a9zoRt2+yNJgmlM5R43MwMeDkQS/bcA4CerToQT+I1+04f6W56yrJpTaao8Y/PofMiIqNFoVtEphzDMDSHe4w90tbF9/a0kLQsfAd/ab+osmSMW3VsGuNJXm+P0JpMk8xaAJxeVcKy0mBun3A6w2ttYQ7EUySyWYJuF0uKA6wqmyDF/CqXOl9VuXxENcR38XrXy2StLC7T6fueG1iY19e0bZs9kQQNsQTYEHCZNCZSWDa4TIOWhJfGeBKXAVkbXmzpYlsoRjiTzZ0jmrFYXB5kc2eUrA337W4m4DbpSGVy+8R67C8iMlUpdIuIyKiriydJWhbLC/1sicSpiycHP2ica0uk2R9LUuRxkewnZyQyWR7Y00oia+E2DEoOzn19pS1MLGtxatUE+NDBMDWcPA9C6Q6yVpZybyXtqVZC6Y68v+beaJL17WGiaedmdZsGHsOgJuCjI5mmPZnGsmFOsID9sSRZG8KZLNUFXkLpDOF0FhOo9nu5YEY5G9vDdCQzJLMWi4r87AjHATCZAB8miYjkmUK3iIjkzd7Nb/LSH39D487txMNdAKz70EeZv/pMfKbJlkicAtsm8NgD/PCVZwm3tREoKWHJaWdx5ns+gLfAP8ZXMHQLiwMsLQkSz2b57e7mPs/XRRIkDvaAXzq7kjKfh9pQlOeaQ2zpjLKytJBCz3ie4Sv5UuIpw2W6aEu2kCFNR6qV+tjOvA4zD6UzpLMWroPnt2wb24COZBqXaVDmddMQS9IcT+F3u1hTXsScIj8Zy+ZXdY1Ou73Or5GzgwXMDhbkzr0rHMuF7mKvftUUEdFPQhERyZumup3Uv/EaJdNqcqEbYF1FMeD0eLt/dRfNLz+LYZiUTp9BqKmRVx/8I827d/Lu//cVjH7mjI4nh8+FLfH2H5x7FvnuzlFGj+ca40kWebTE0lQ0x78AgN3R7TSnDtCebqOrqxPI3zDzEo8bj8skdbCnu9DjYk6wAK/LpMTjxrIt9sSSZLHx2DYYTjB/pbWLlOXczfOLnA/FGmNJagLOvO1IOsuGtjDgVDsvU+gWEVHoFhGR/Fmx9jyOv/CtxDo7+eEN1+W2m4bBRZUlNO3awc9efhaA8679J1a/9R3sXP8if7jjS+zd/CY7Xn6BxaeeMVbNH5K90SRvdISJZyxsbGYFCvrdb3bQx/pWg4xt88CeVorcLs19FcCpMzE3sJBQuoOOdFteh5l3f0jUmUozJ+AjfnD0xZxCp7e6u2f9zY4IbsPAZ7roTGd4rjmERShXr2BFaZCqAmflh4f3t+M2DQpcJuF0hqwNbsPgzGmlE6NWgYhInil0i4hMcA2vvsSeDS/3+9wZH/wXDNOkceubNO/YRrStBSvjBL3VV15NoLQsr23zFxUf8fm619bnvl986pkALFh9Mm6Pl0w6Rd3G9eM+dIfSGeIZi7Rtk7ZsGqKJfvcr8ri5eGYF69u6aE+miWUtFhcH2NYVA1A4GSesjEX4sQZSB6J4pwcpOn8Opnt0Rlt0DzNvT7XiMl2UeEb+3+feaJI3OyNkLRuXabCyJIhhGITSGfZGk8wK+jAO1hxwmQbWwRoEScvCwKDC52FZSYAlJcFcgC/2uIhksnSlMnhMg1kBHydWFFHu84x4+0VEJiKFbhGRScJdUEBBUf/FuDr2NBBta8FT4CcZCY9yywYWbmvJfR8ocdpumCYFxcVE2loJt7YMdOi4UeJxY+MEbo9p0DM7dyTT2LadC9TT/F7eNqsy93xzPJUL3SUe/Zc8HoQfayDy3H6wbFJ1IQBKLpo3Kq/dPcy859JhIy2UzpC1bMp8HjqSaRpiCcLpbC6EA8wuLGBW0Jfbv8TjzoXxnroDvMswKPd5WFVayOzC/kd6TFqpKDxxG2x9ALoO4Ha5OdcoxSzbCWd/ClIReOxWaHgOOvdAOgbFM2HVFXDmJ8CnlTREpgL9Dy8iMkmUz57H4rUX9PvcgjPOwev307yjlh1PPzbKLRuaTR1RSn1eZgS8JLLOUOvWVAbLtolm+i9O1u3E8kJWVxTTnkzzWluYlkSKpGVR7HGzsrSQxSX5mys9K+ijJRFgVziOafSeu70/nmRvNJkLIk3xJFUFXkzDIJm1eLnVCXU+l8mMgDdvbZShSx2IgmXjKvGRDSWdx6Oke5h5PnX3YHcXTMOmVwgPpTPMPtiW2YUFzKZv3YLuAB5KZ8hkLbwuF6FUmvpovN9wPqn9+V9h473O91XLIRGiJLwHHrsFfEFY8lZ48U5w+aByCYT3Q/tOeOqrsP81+MBvx7T5IjI6FLpFRCaJ1t07ad21HZfXR2FlFXPeciqFlVUA+ILBQY4eG0UVVbnvdx5owldaxlPtFslIBBewwxvkkbYuziotpKqg91DVVNYmlHaGyvvdLjqTaf68p5WMbeMzDYoOLsn1THMnKctiZVlhXq7BMAxWVxRh21Db5axX3C2czvJ0UwczIwWcU1PGc80hYhlnfe5wOkvGtjGAM6pKcI/zgnFThXd6kFRdiGwoCaaBd/r4/LdztA7vwbaxCXdGcyG8vxEXhw9JB6c3vMTjJgOEEikAWhPpXh8yTQkNzztfF62DD9xHJtaF+dUFuOw0dDaAuwAu/BKc9EGnVzudgB9fCntfhh0PQ7wD/Pmd5iMiY0+hW0RkMjAMvP4AhmESD3XQsaeezv17Of7SK3PBezyaf+Ianv3VTwHIbtlA9tTz2LXhZQKZNAAHFizjrr0t/OfO/exNpChxu7i4soR/XzCdLR0RQqEMzZkMX6lv4pVQlFAmi980WFNSyKfmVTMnY/F6R4TX2sMsLQniNvPTA2cYBsU+N0nL7rXdBpKWnSuSNjPgY3ckQSidwWUYzPD7OKG8kBq/Ly/tkuErOn8OQK853ZNJzx5scHqxDYxevdiHO3xIendv+Kygj/qIh7RlUer1kMpaueemjLlnQEcd7HgEvnMa7kQIw05jzT4N84wboKgazvz4of09BTDjLU7oNkwwj/Cr+K+vgc1/cL5feQX83d2QDGu4usgEpNAtIjLBVS1cwvSVx+PxOb1LHXsb2Py3P2FnsxzY8gaLzz5/zNq2/cXneOrnd2NZhypzP/vrn/PKn35PzaIlXPLxzzDn1LNoePEZdt/3U7xPPUygzRlG3jlrAS/NWExXRwSXAUsDBTQkUvxkfxuvdcW4urSIuGXx/eZOopaF3zSo9rhpyWR4siPMC6EI9yw7GKAsm9ZkKq/hdnFxgMXFgQGH4gKcUlXCKVX9z7uX8cF0m6M2h3s8ODyE9+fwIendveGGYTC30E84kyWVtQbsKZ/ULv0m2BZs/AW0bMEAsoYbpq2EgtK++0daYMv9zverrhw4JG/42aHA3VOsXcPVRSagKfaTUURk8vGXlPZ6XDZrDm5fAZlkglQkMjaNOigZj9HZdKDXtnhXiHhXiMKKCgAu/9iN3F8+jf0vPkW6rZlgcQkFJ5xM9JxL6Gpy1vb+0qKZ/OOsKnbFkpzx4hZej8RZ7vVQ7jaJWs4SRjcvmIGdSPNyNM59HWGSls0LbWFqDoaA0VqSayghRmQi6a+o2lCem5QsC7Y/5ATd8oXQtAk2/hJmnwbv/TnpUCPWXRfjW38XuL3wttsOHdu+C352FYQPOPtf+s3+X6N1J/z501A6xynUFms79JyGq4tMSArdIiITgGVZNLz6Ep379mB6nIJbtu0MZd678VWqFi7GV+j0mHTu20Mm6Sxb5Ssa26GGq85dx6pz1x1xn8aURWDdZcw//x24TCNXAXlLJM53D4Zu82BPcc/R4TsSKW6YXcUv2sN0ZrL8Z90BZvs87Iqn8BhwblGQs8qL2BGOO8cyBsWdUhHY9Ri0bnGGhbp9UDgdll/h/ML85q8gvM/Zz3CBr9jpIZt/Pri03JLk35FGZnQ70gdJU+5Dpu0PwXPfdgJ0Og6JTsCGFZdBsBK8JbQXLmZ6aAPseuLQcXtegl+81wnQS94GV/0IvP0UeMxm4BfvhmzaCc/hpt7PH8twdREZM/qXKSIyAezZ8DIHNm3EymSxTAPKZ9C5bw/T5i+kceub1L/yPL5gIabHQ7yzAwDT7WbGyhMA2P3Sc7Tt3kk2nc6dc/Nf78cwTaavPD6331gYaL7okmABy4IFbI0m+Pz2vfx0fysN8VTuuK6sxWmVxfyhJMA1r9dRn0ixNZYEYFaBh7+bXcVMvzcXuou9Q/svbyghZEhSUXj5u07Pk+GCQCVgQ6gBUl3g8UPrVmcIarAakl0Qa4HdTzjzNJe/a/ivKTJMAxVJkwG073QCd6zVCcbd6xXsf835mklQFN/nfN8dqjf9AX7/Ecgk4JSPwFtvg4EKJz55G7TtgPIFMGM1NG85cnuGOlxdRMaUQreIyAQQbWvFyloYLjP3O16iy1luatYJa2it20Gss51UuAtfYRHF1dOZdeJJBErLsG2bSGsLiXBXr3Mmo87Q80wyOarXcrhit4u0bbMvmsDjMil2uwBwGQb3Hr+AW3cd4KmOMPXxFKeVBNnYFaMlkyXoNvG4TD7xegP1iRRfXDiDt5YV8beOMDfv3M9HNu3mczMqKTFNSr1uyoYYuocbQgYM6TsfcgJ3sBre8o9OLzaA5VRcx3DBeTcf6pmysvD8151jOuuP6r0UGa6BPvSSAZQvdMKzlXGGemcSgA1v/Br2vYI7GcGTOri84Qnvg64D8JtrnX1cXti3Hu668ND5Lvk6zDjR+X7fq/D0N2De2c7j5s1wpBE6Qx2uLiJjTqFbRGQCCFZUEjqwFyuTzY2xLih2CnLVLFtJzbKVAx7bsacesCmaVo3pcjNj1YmUz5k3Cq0eIuPgr5U9vnabUeDlOyvm5h5v74px4avbATiuOMDvmzrZeLAn+33Ty/lTfQueg8PubWBjNMH5xUHOnFY65N7q4YaQfkN60AdNbzg7FJTAqz+CeDsEKmDeOVBz4sFrd8Pm+yDSCIkQpMLO9tJ5Q2qryLEaqEiaDGDxRbDgXNj6Z8ACbyFMW+78++7aDy4v7YGFFF/4r7hXXw0dzs9fALIp2PdK7/Mlw4e+b94CdtYZKo4Ntn2wNx2nN/vWGfDpLc7PlKEOVxeRcUE/WUVEJoDZq0/Gtu3cnO72NJTOHFp/VDzUiZXNECirINbRRjzUmd/GDlNXOovbMKgKFNCRTNOVPlTw7PVwjAV+H4VuF1nb5uYd+0gcDNULAgU81BrK7bsxHGd2sIAn2w/16Nf4PFwyu5Jy39DnRw83hPQb0tMZyDgfBtC2zenl9vidcP3mr5xe7urjnOcjTdC199AJa06Epe8YcntFjsWUK4R2rEwTrvgBPHk7NL0J1avgnM+By/k5kUmnefrBB3n7qrc7+5fNhZtDRzhhPzKJvtusjPPHtoc3XF1ExgWFbhGRCcA0TeaddBqcdBrpdJq6Bx8ccs+tv6QU0+Um1tGG6XL3qXY+1o4Ucn9xoJ1fHGhjnt9HcypN+8FAfnFFMY+0ddGZzmDg9CP9/eu7WBjwsSvuDJcvdpt8ctGMYQVuGH4I6bf99qG55wSnwak3ON+/+G2INsOe5w+F7lM+6vwy3bWX7Iaf42p8jX37bRqSJ+DxuSmpDDB7aRUFQaeAXiKWYs/WFkKtMdLJDD6/h+q5pcxYVHF0c89lSptyhdCOxuEVyxdfBOd/fmTPv+2v0PACrP4HWPo2WPJWJ0h/8zinDkT3Ot3DGa4uIuOGQreIyCRXNtsZnh0PdeIvKc09Hi+OFHJXFwd4tiNCfTwF2Bxf5OfaGZWEs1n+1NzJW0qCZG0bDIPOdJadsSSVXjcnlwS5cV4N1cMM3DD8ENJv+22P05ttZ6Gw5tC87cIaJ3QnOnqfxHRD6TxaM3OpNjYxw3ydpoITSMTStOwJ0dkcZfUFC7Etm9efrCOTymK6TPyFXmLhJPWbm0klMsw/rmbY1ysig9j+EDz/v5BNOutjAyx968ie/7EvQWiP8wnivvVOJfL+XiPr/CzMfX+k4eoiMm4odIuITHKGYeRlDnc2m2X//v20tbWRTCZxu92UlZUxZ84c3G7nv5dXX32VVCrV59jKykoWLVqUa99AIffdNeW8u6a8z/aHWkP4TJMtkTgVXg8fmV3FRZUlg7bZtm1isTpSqQ683jICgfnH3Dvcb/sNF5TNh/YdzpBy6+CQ+Uij8zVQ6Tzn9kPxTGdbJkmZtxXSYGDzlnPnUre1iwO72kknM4RaoqSSGTIp51zHr51HoLiApt0d7Nx4gAN17cxYVIHPr6XGREZU+04ncE9b4azLvfXPvXu9j2Zod8/e8+atB5cUPFiwMRl2tgN86o3exx3NcHURGXMK3SIiclRqa2vp6nLmTwcCAZLJJM3NzUSjUVatWtUrzPr9flwuV+6xz3ds80bXVTiVwOviSeb7fbnHg4nF6ujsXI9lZTAP9j4HgwuOqS0DWnghdNQ5PdvP3uFsS3Y5PVjzznWeq3sUPEFnzne8HW/2YCX5yuXgCVBckeHArnYATJeR6+ACoPv97X6bbQi1RJk2pzQ/1yMyVZUvdHq4mzc7SwHuW+8s93csvd49e8+TEcBw5mjbOFNSyheO5BWIyBhT6BYRkWGLxWK5wD1v3jxqamqIx+Ns3LiRaDRKW1sblZWVuf3nzZtHScngPdFDZRrGkHq2D5dKdWBZGXy+SpLJVlKpDoLBEWtWbyVzYM2HYOfDzrBRlwfKFzlhvGSOM4+7bIETyqNNzhDzwukwbSXMXYtt2zTVO8PQfQEPJZVB/IUZ6jebWFmLN56qwxfwEA8fWvItlcjk6WLGXiLcxfpf/3TA52evPpk5bzmFPRteob2hjmh7K7ZlAXD6NR/BdOtXHjlKiy9yvnb3SrdsdXq9mzcf6pEeDstyess7dkPlEufxjDWHPkhb+rZDrykik4L+BxIRkRHRs2c7FAr1Ct3bt28nm83i8/koKytj5syZuSHo+WbbNg3xXYTSHRTZSTyGi2SyFdN04/WW5ffFS+fBmn/q/7nKpc6ffmQzFtte2kNncxSPz83yU+dgukwKgl5WnjGH+i3NRDsTpBIZquaU0lzfCTid6JOV6XJRWFXda1s2lcxV4/cGnOWS2nbvIBEJ4ynwk4pFR7uZMhmZPeZX1/7VGaXSvNnp6T6aHuntDzm95fFO2PsSlMyG5ZeO7DxxERlXFLpFRGTY/H4/fr+feDzO7t27aW5uJpns0ePaYx63y+XC6/WSTqdJJBIcOHCAcDjMypUrR6XadkN8F693vUzWyuIyTFYF5lBqFOTmdI83qUSGLS80EA0lKAh6WXH6nFzlcoCi8gCrzpyXexxuj+VCt79w8i735A0EOeGyq3pt2/ncU8RDnbi9PqoWLgFg+UWX4g0E2bPhZfZseDlv7bEsi721rUS7EgSLC5i1tBJTyzZNfj17vbvndA9X+07wBmH2KdC6HWaepJ5tkUlOoVtERIbNMAyWLVvGnj17CIVCJJNJioqKiMfjJJPJXJhesmQJwWAQwzCwbZudO3fS2tpKJBIhHA5TXDy0udjHIpTuIGtlKfdW0p5qJezyMb9kTd5f92jEuhJseWEPyXia4ooAS0+Zjcfr6rVPV1uMonI/hmGQSWXZvakJALfXRUllvsbKjz7btunYU0881ElBsTOVINEVylXgzySTNG/fAkDN8lW4PM4HE75g4ai0b29tKwd2tWPbNl2tMQDmLJ82Kq8tY8gcoKr4cJQvBLcPUmGnMNqyS7TOtsgkp9AtIiJHxefz5SqQg9Pzt379esDpCQcoLDwUgAzDoKKigtbWVoB+q5rnQ4mnDJfpoj3Vist0UeLJ85DyY7D1pb0k42nAGWK+5YWG3HPVc0upnlvGzo0HSCXS+PweEtEUVtYGAxaeMB2X+9h/cbeSWcJP7SX+eguZjiSm341/RTklF8/DDHjIhJKEH2sgubuLbCgFloWrrIDgmmoKz5yB4RqZ8NCxp579b76Glc2QTWewbQtsG8uyqGppwjBNrEwGw+Vi+orjRuQ1hyPalcC2bXx+D8l4mmhXYtTbIBPUSPSWi8iEotAtIiJHJRqNUlBQgMvlwrZt6uvryWad5awqKiqIxWJEIhEqK51ht7Zt09bWljv+WCuYD9Ucv1OdPJTuoMRTlns8HtnWofLk0VDvEFc6LZj72rY/TDySwjQNSqsCzFxSOWK93G0/3kRyVwhM8EwLkulIEH2xkdS+CNP+5UQyrXGiLzZieF24KwrItCfINMUIPVhHpj1B2eWLBn+RIYiHOrGyGQJlFXTs3YOVSWOYJtl0mpYd28iknOkMVQuX4A2Mfg9/sLiArtYYyXgawzAIFheMehtkghqJ3nIRmVAUuuXY9Fxn8ljWqxSRccu2bTo7O0kkEhQUFFBSUkIoFGL//v1EIhEKCgpIp9NkMk7l7JqaGgoLCwmFQuzatYu6ujoKCgrIZDKk004vbnFxca9e8HwyDIO5gYmx/M6aixYPus/8VTXMX1WTl9dPN0WdwA2UvmMhhafPIN0ap+lrr5DeGyH+Rgvu6iBlVywm8JZpGG4TK5am6X9fI9ueILahecRCt7+kFNPlJtbRhtvrJW1lyKbTuL0ep/f7YOgurKhi3xuv5Yadj0adAIBZS51CgT3ndIuIiPRHoVuOTc91Jo9lvUoRGbc6Ozs5cOAAlmVhmmZuPnYqlcIwDBKJhNPTFwxSXV3NtGnOvFa/38/06dNzc75t2yYQCFBRUcH06dNHLRzJMPRaB/zglx5/TYntnZSfOA3v9EM9y2bAg6c6QLY9gTECw9u7lc2eC5Cb0x1paaJ11w4wDFLRCACFldNo270TK5vBdDm/0pTPmTdibTgS0zQ1h1tERIZEoVuOTftOJ3Afy3qVIjKuJRIJLMsiEAgQi8WIRqNYlkVJSQmxWCwXog/n9XqZO3fuGLTYYds2e6NJQukMJR43s4I+Bf1BuKcFcFcHyDTF6Lx/J9EXG8m0Hxrmnu1K9jkm3RIjubMTgOApI9cDbxhGr+BdWFVNYVU17fW7aN6+FXBCd7S9hUBZBbGONuKhTmqfeIhIcxOZHtX0N/zuFwDMO+UMKuZNjFEPIjI5rP/zH9n0xMN0tbaQSSXxF5cwY/EyTrvyvVTNdVbQSMVjPPvrn7PthWeIhUIUVVay4uzzOe2K92C6XIO8gkwECt1ybMoXOj3cw12vUsPSRSaMgoICTNMkFothmibBYJBwOJx7XFDQ/1xW27Zp7EwRTWQIFripKfWOaujdG03yZmeErGXjMp3XnV2oebdHYpgGlf+4iq6/1JHY0UmmPYFvfgnplhjZtkSfImmpPWFaf7wJO2XhX1lB8bqR/ZClZzE10+VmxqoTSXQ5w98Lq6ZRNnsu8VAHsY42TJcbf0kp7Q11JMJdvc7T/TgzSsX7RES67d3yBrFwFyXTasikU3Ts38e2F5+lYdPrfPg7d+P2evn9Hf/J3s1vYrrclFTX0HlgP8//9l5CTQd428c+PdaXICNAoXuq6w6/bdsh0QUFJVCxaOgh+GgrcGpYusiEUVpaCtBnTnf34+7nD9fYmWLHgSiWBabp9DpOLxu9daRD6QxZy6bM56EjmSaUzjB71F594nKX+Ch/77LcYzttsf/WF5znKv257fFNbbT/cit22iJ4Sg2lly/CMEf2Q5WexdS6e7KPu/SKQ22z7dx+3XO6R2t4uYjIUFzy8c/i9npzj5/91U954Xe/IhEJ075/L10tzezd/CYAl33631m45hRe/cufePye77H56cd5y9vfSfWCkamVIWNHoXuq6w6/kUaINEOwGoqqneeWvnXwHumjrcB5+LD0th1Q+9dj6/lW77lIXhiGQVlZ72W2Dn/cn2gig2VBccBNVyxDNJEBRi90l3jcuEyDjmQal2lQ4tF/eUOR2hfBXVmA6XNjWzadD+7CTjhV6QMnVAEQfmYfoT/vAqDkbfMoOic/H2f0LKbW3ZPdk2EYCtkiMq65vV62v/QcL//xPpLxGB379wHgLy6hbPoMNj784MH9fCxYfRIAS049g8fv+R4Auze+qtA9Ceg3kKmuO/wWlELXfvCXOI+752bnq0f68GHpidCxv456z0XGlWCBG9NM0hXLYJrO49E0K+j8HOg5p1sGF32lkejLTbgrCrAiKayoU5W+8MwZeGcXkazvIvSAE7gNn4v4m23E3zy0FFzF36/AVezt99zD1XNOd3dPtojIRBMLdXJgR23uccm0ai7/7Bfw+gOE21oBKCgqwjjYWRToMYKsq7V5VNsq+aHQPdV1h99II5huiIecnu7uudn5KpR2+LD0tu3H/joq6iYyrtSUOsGr55zu0WQYBrMLC8Z+SLltQWstxNogUAGVS8EYv6NwvLOKSO4MkT1YQM0zs5DC06YTPNkpkmZnrNy+djJLak+41/E9nz9W6skWkYnGtix2bXiFjsb9lNXMYMHqkzjhwrdz/Lq3EW5r4amf3U3t80/zwLdu5+ovf22Ak4xumyX/FLqnuu7w29+cbjj6QmmDOXxYei3H/jr5aquIHBXDMA7O4Z7iPcyttdDwDFgZ58NNgKrlY9umIwiuqSa4pnrA5wsWljLrtrNHsUUiIhPHrg2vsP7PfyCbTuHyOB82L1xzCoZhUFw5jVPf9W5qn3+atr0NbH32KYoqKgFIdHVhWxaGaRILdebOV1yppQknA4XuqS4XfgcYhp0L5TucIeBt252APNLzpY+2INtIn0NEprZ89ErH2pzAXVjjjCqKtQ1+jIiITEgdjfvJplNUzp5H8+6dbH32SeadsBqX2wM4obxbOplg3olreOOxh8ikU+za8AoL15zCthefy+0z74S3jPo1yMhT6JYj6w7ltX/N73zpoy3INtLnEJGpLR+90oEK51zd03gCFcfeThERGZfKambg8nhp3bMbw3Sx9dkn2fHyC5RW15CMxQi3tQDg9ftZfMoZFFZUMHPZCvZt3cz9X/8KpdU1dBzYD8CyM89REbVJQqFbhkbzpUVkKoi1QTYN7gKItjg/7461t7ty6aFzd/ee54Ft23Q0RUhEUxQEvZRVF47quugiIgLzT3gLjTu20dJQR1nNTIorq2jctZ3OpkasbIaiiipmrVjFqZe/m+IqZ+j4uz53M8/++qdsf+FZOpsaKaqsYsXa8zntiveM8dXISFHoliPrXoareSskI9C0yfllVPOlRWQyClRANgXRg9Viu/Y6vd/H0tttmHmdw90dttsPdBHuTOAyDUy38yFBeU1R3l5XRET6qtv4Kvu2biLa2UHr3gaWn3EOl3z8M7nK5P3xBQKcf+1HOP/aj4xiS2U0KXTLkXUvw5VJAobzi+OySzRfWkQmp8ql0LIZMikIVjo/+3rOwR6Hlcg7miLs39lGIpomm85SVOYnm7FIRFNj2i4RmRjSyQx7altob4yQTqRxeVwEi30sPHEGBUEv0a4Ee7a2EOmIk05l8Rd6mb6gguq5pWPd9HGpo3E/0c4OkvEYqXicLc8+Sc2iJSxcc8pYN03GkEK3HFn3sPLqg8PKpy0buXnT3b3oPQufjWRxNhGR4TJMqFoB8faDNSwOm4M9DiuRJ6Ip7KxNoMhLV3ucWCRJQdBLQXB0l2gbL17peJb1oef6fe6f5n4acxwv1yYy2tLJDK8/VUcylsYwDQoKnbo94Y44qUQGK2vzxlN1WFkbt8eFP+gl1pVk52v7yWayzFioGhWHK6uZQdbKkorH8fr9uFwuOhr3j3WzZIwpdMuR5XMZru5e9KEUZ1NAF5HRcqQ52OOwEnlB0IvhMsikLQr8HgrL/JRPL6KsunCsmzamCkw/xe7SsW6GyLjWsLWFZCyNv8jHyjPm4C1wKmxblg22TcPWFqysjWEarF63CI/XRf3mZvZtb2XP1haq5/1/9t47vI7zvNO+p5yO09ALQQAkSIpik0SJ6lZ1t1zlEntdEsd2vo2T2E7ZOG2duutkN4mT2I6TOGsrcUvkyE22XNQsiRIpUaQkNhAEQPR2ei9Tvj8GOGgHhSQ63/u6JODMvDPzzsHhnPnN8zy/J4iiiPux6Wy79npGzp/jzDNPoigK7kCQYH3jWk9LsMYI0S1YmJVsw3Ux5mwXI9AFAoHgclioBnsdOpFPimthoDaTra5t3FXzhrWehkCwbjFNk/BgHACHS+XU4T7yGes60rSjmpotfjDN0vjJq8rk5UXXDFLRLP5qzyrPfH0jyTK3vPO91LfvJDoyRLC+kW3XXr/W0xKsMUJ0CxZmJdtwXUwU/WIEermouECwykSjUT73uc/Nu/6OO+7grrvuKr2Ox+N88YtfJJfLAfC+972PHTt2rPg8BRfJKjmRXwySJAnDtDL0ZM7R1duBQ3ZQba/jhsBtVDvq1npaF41wpResFMWCjlY0AIiNpbE7VVSbQiaRp/PYIJIsUdnoY6g7gmmYvPjoeexOlUwiX9pHIaet1fTXNZIsixpuwQyE6BasHQtF0WcL52Db0gV6uaj4tntW7jwEggkMw6Czs5NIJILL5aKpqWnG+lwuRzhspSN7vd4Z2z300EMlwX0l0DmUZiSaJ5nTKWoGDptMtc/O7i0ePE7rq6lnNENfKEc8XUS37gu590AVXtcafnVJMmb1VWQyPRQKUeyZC7jdbasugoQQWxgJCZfiQZZkYsUIfdluBrO9vLXhfRtOeE8a5Zm6iaRYf+P5HrLommEZYg0nKeSKSJKEw22jZoufxvYqJEkilynQf3aceChDMa/hcNmoawmU1guuIIypKLarws6Bu6x7q5ce7yKbKjDSHWHvba1cdaiZgXMhMok8WkGnptnPeL8VIZdk8ZkRCJaCEN2CtUOWLaE9Ka47fzJVqz1bON/03+Hmj1vjgm2Wg/Czny9f310uKi5Et2AV6Ozs5Nlnn0XXdRRF4VWvehW7dk1FQh9++GHC4TBOp5N9+/aVlh8+fJgLFy6wZ88eTp06tRZTX3W6RzJkCgYVTgVFVsjkdfpDOcbiBV59oAqbKjMaKxBPF3GoMpmCsdZTLpHJ9BCLHcMwNOQJMzWPZ9uqzuFihNh6wjRNov29ZOMxXP4AweaWZT9Ge8Vu9vquw6m4AOjP9vDD0QfR0TmVPM4djo1VmlQyyvM5yCTyJVf62Q9eArUezhzpIxHKAODyOtCLOplEnt7TY8iKTHWTj5ef7EEr6MiKjKvCTiZprS/kNNr21a/lqQpWkHIP6lSHiiRLmIaJx+9EnhDQHr+TbKpAPlMErGvL9OvL+EC8JLpdFVemYaNAcLEI0S1YmJU2MJuvVnu2cI72wM2/aq3veGTh+u6VNH8TCBYgEomg6zq1tbWMjY0RiURK6zKZDCdOnADghhtuwOGwPrtDQ0M89thj7Ny5k+uvv/6KEd2tdS6aq124HQoAL19I0jWSIV80GE8UaKx0cqDNi9Pmp288x4vdiTWe8RSFQhTD0HA4qsnnQxQKUTyrXNI4nxBb70T7exk6eQJD15AV6xbE29C0yFYXR8BWOeN1s6sNp+wiZ2RJaevnc7RUJo3yMok8kiKVXOlnP3hJRbMkI1kAFJtMy9W1+Gs8PP/DDgzDJJ8pEBpKoBV0APa/qhW3z8nohShdLw0z3BOhsb0Kh8t22XM+Hk5wIpIqu+6D7Q3IkkSyqHEinGQ4WyCn63hUhZ0+N3uDImtjJZjvQZ2vyk18PE06kbfM04D0RPq4c0JQx0PpUt12Pluk/+w4YD3YcXsdq30qAsGGRIhuwcKstIFZpAu0HDh8EDoHZx+2hP1Cwnmx+u5yaeu6vnxzFgjmobKyEkVRGBsbQ1EUKisrSynnzz//PMViEUVROHTIqvMqFAp8+9vfxu1285a3vIWxsbE1PoOVxTTNUlr2lkAQl72ttK7KZ6NrxPpdnrjhdtmVtZjmotjtQWRZJZ8PIcsqdntw1ecwnxBb72TjMQxdwx2sIhMNk43Hll10n4gfYbtnN17VB8BA9gI5wxKjFap/WY+1GpQzypt8Pf3BSzqRQ1FltIKOXjToOjGEJEkYhomvyk1jexXhoeTUjieF7aS+NSE+nqZ2a2DROZmmyUA6T7yo4bepbPE4ygplhyLjs839d5zTdH7QHyKnG6iShN+mEi1ovBBOktENbqzZeH+n9c58D+q2XlXDyXCGbDLPiz/tBCbqtCXYsqMagDPP9SErMnaHSjZdwDRMZEWi/ZoG8YBEIFgiQnQLFuZiDMwuhcrtUEjD+FkwgcFjU2nmk8efXe+9WCS7nPmbEN2CFWJ6HXcgEKC5uZmxsTHq6upob2+ns7OTw4cP09/fD0BLS0upnvvRRx8lHA7z/ve/H89qh0pXENM0GYkVSOc0PE6V+oAdSZLmTcvW9Tzj44fZEejGrqRJRhzkUm1saXgVqupCzz3PvuoXABgdhNFZx2tp+TA22+rdpLvd1sOCQiGK3R4svV5N5hNi6x2XP4CsqGSiYWRFxeUPLPsxTiVOcCT6cyoUH6psI1a0fBRUycZ+38FlP95KM59R3uwHLx6fE62ok00W0Ao6xbz1vSfJEm6fA9WmEKyroPe0jKEbvPLzHhxuG9nkxZtiDaTznIyl0A0TZSIlubnCOWdcs9vB7fVzH0r1pHLkJowa3tRcTdBhoyOe5vBYnDOxNHsCFVSUEeuCS2fOgzq3jchIkly6QPNVNcRGU6RiWWRFxl/jYetVNXgr3YAVEY+HMmRTeRRVwVfnpnlXDR7/3L+5QCAojxDdgoVZ6VTtHa+xotv5FFTvgHzSEtoLuaavZBszgeAimV7Hnc9bN68Oh4O+vj7Onz9PJBIhkUhgGNYNZkNDQ2nbkRErtPvNb34TsMTqJN/61re46qqruP/++1frVJaNkViB88NpDANk2XpPGoKOsmnZDqfJue5vU6GMYJoSRTOAaqTQ8qfo6x+nrfW9SLKHTNFqzeV3q8iyRLEYwzBySJKCLK/ujZ8kSXg821Y9pXz2HDZCDfdsJmu4p9d0a9ryuh9fG7iJ7nQH0WKIZDGNV/VT52jkYOCWOannG5lArYdUNEs6kcPjc9K0s4r8S0WS4Sx2p0ptSwCbXaW/Y5yRniiSJNG2r549t2yl98wY6ViOQk6jZmuAsd4YYHXLWwrxooZumAQdNqL5IvGixpZp0e94wfqbXkjl6Dk/hF2WqXLYuK7KR5XThjltX2UC7oxk87Tb3MvxNgkmmP2gzjAM+s+Ooxd1FJvC1qtqqGrwld125/VbVm2epmHQffyFGa22pOUsaxQI1gghugULs9ICV5bhqjdaNduFJKhLEPYr2cZMILhIptdxnz9/HqAU7Y5EIgSDQZJJK6XT5XKxdevWOfsoFotzlmmaVnb5RiCd0zAM8LlVEhmNdE4DHHPSsk2pkmfPdNPgtB4+6OrNjKd34nelqTD/E0MfI5XqQLZfTVfcuum7t7UKj8Okt/dfAPB6r0ZRRE3hRkGSJCq3tq7oMa72HuBq74EVPcZqsZBLfWwsTSKSwdRNEpEM7hEHoYEJR2lFIjZqRaLlifrd2HgaAG+lm723tpaOkYxkSqLbVbG0f0t+m4oiS0TzRRRZoqAbPDMaYzxfRAWyhoEEuFQZGYl4UWMgk2c4O84bt9TQ7HFwLCShmSY/6A/hVRWihamHLxlNZKctN7Mf1J0/PkQhU0SWJQrFItGR1LyiezXpPv4Cxx7+DnqxgGKzymaW0norPjbKv/zah+ddf/P9v8At73wfhWyGZ/7ja5x77mky8Tje6mquvv1ubnr7u5EVkV0hWDmE6BYszGoIXBG5FmxAJtPKx8bGyOfzjI6OlszRZtd0T0bybrnllhm9t3/xF39xxj57enr46le/CmzsPt0ep4os50lkNGSZUguw6WnZBaOSo90VGHqEholAtdtpQ85CKqczee+fyfSB1Dpj/8nkaXTdqtENBDZeuvCmY6UNN69gFnKpn12jm00VmEyW0YsGLredTCRTMscyJ34mwhm8lS4kSUIr6Fw4ZRVsqHalZJa1GFs81j/QeFEjk83xwjM/J9R1nmI6iepw4mtu5cZX3cmhphoABtM5fjIUIZ9O8cW/+SraRFbQrtfeh1HbSEY32OFzcy5hOa+LOuHVwCxlHJjT/r/WREeG0IsFqptbCfVfIDoytKTtFJuNhvZdM5blMmmiQwMAeAKVmIbBQ3/5JwycPomsqPjr6okND/Hsg18nPjrM6z/+m8t+PgLBJEJ0C1aHhW7KRORasAGZTCufFNQ1NTWl9mDRaJTKykp27NjBV77yFQCampq4/fbb12q6q0p9wIpOTK/phplp2T89ESJb0AEfBSOAXY6RT/2cSuVlbPKU6/FoNMJAeur1M2cibK04il0Gj2c7dvvmSRfesCzFcDMdgic/Cx0/guQIOLxQvxfu+zswZfb3/T/Uf/pfkBwCXYPAVrjmvXDT/wfK5btpb1QWcqmfU9Ptd+KrcpMIZ9AKOrFQeoaOcvusv03XS8MUckUcLhu5dAFDN0GC7QcaUNSlPSyRJInmCifNwOe//S3GB/pBkrD7g2jpJOGOUxyNhTn40Y+gKApNHid2SWLk8BMlwQ1wfbWPHdutkpuxbKEkuv02cXu60lTWe0lFc2iagdMhr5tylWB9I4rNTqj/AorNTrC+cUnbVQQree+f/98Zyx791y8SHRrA6alg9+13cv755xg4fRKAN//m77H94CFe/NH3efwrX+L0U49z3RveQt229mU/J4EAhOgWrBYr7YIuEKwyk2nldXV1jI2NUVtby1VXXTVn3C/90i8teZ9tbW185jOfWcZZrg2SJNEQdADzp6pORt9Apit2F/Xu41TYR1ClJCiNqHIcTYujGRL54lSPbht92GUrXT8QuH4Fz0KwZBYz3EyH4Z/vhlgvKHaoagdM6H8ekiNIxRxt4ccx7R6o3AbRXhg/Az/9Q4hegDf99Vqc1bpgIZf6cmZ6vko3A50hQgNxCnnrgaCsSDhctpIreaDWQ3goSTZVQJYlAjVumnZWLznKDVbae38qx+mBQUtwA8033kbV7n24cykOf+MBQqMjnDp1iv379zOYzjF+8jj50SHqtu9gtMtyyTYm9hPOF+lOWtkrDkWm0b0x3Pg3MsF6L0jSujNj3HatdV2fXtN9KWSTCU4+8TMADrzmDdidLnpOWIacqt1R2u/OG2/h8a98CYALL70oRLdgxRCiW7A6rLQLukCwypRrD1aO6e7mk9FvWaTe8trramYtmXIANwyNnp4vABB0xanz/5xA4CAezzYGBh4nlwOHowGXa3lbTQkukcUMNx/7U0tw1+yGD3wHvPXWcq0AmJgjZzje/Evsfe+fYnNVQDYKX7rD2uaV/7yiRfdCLvXlzPRUu0Lrnjpa99SVrQcHaNtbT9ve+sua10A6z7FIkkh6KmrtsakcrPLhziscnlj22MunOe+vZ2xkhNjLL+BuauG2G2/k2xOi+4mRKG7ZizaRFy8Bt9T4UTfhNTKbHeDws3fMu76t9dfZtu03AIhGj3Kh9wskEi9hGDns9hqqq+9l184/Wrb5rFczRkmWl1TDvRgnfvIwWj6PYrNx7evuAyAZDgHg9HpL5mzuQKC0TSK0udt2CtYWIboFq0O5m7KVqgMst1+BYJmZrLeeLqZhrsg2TZPnnnsOXddRJkxaJtPQr1Sm9+u224MoihubLUguN0g+HyGX68c0rSid01mPpmUoFKLI8hC5nFXfFwyKWu51w0K+HKYJpx6yfvc3wQNvtcR05Ta47ZOw736o20Nf9Z3sVScyI1xB6wHtZGR8nVPM6zz/gx66T4yTjuWRFQlvlZOdN9Zz7au3IkkS8fEMR7/fw1BnjEyygN2hUtno4cA9zWy7ZvYDqCkuRxitpKiKFzWKuoHLH8QeqKQQi9Dx9BNEz50mFouVxhXTKRLZHJFnHsPudPGBd76DbDQ8NUdAM01kQJEktvtctHpdKzLntUaW7fh818xYpmkJMpluAOwO63MwOvowp05/EtPUsdmCeNw7KGpxwuEngOUT3ZsZrVjkxI8fBmD3bXfhCcxtW1difZSyC64AhOgWrA7lbspWKuW83H633XP5+xUIpiHLMjt27CgJ7M7OztLryRZiiqIQDAZL7uaTjuZXOrP7dUuSSibTiyQpmKYOWOnkilKBpmWQZRW7PUgsZqUG2mwBPJ6NaTK3KZnty2EY0PGIdb13V0MuZi0//zPwNoIzAKMn4dsfBlmFnW+cub9QJ/T83Pr94AdX4wwui59/o4Ozz1kO/JWNHgpZjfBgmmf/qwvVJrPvzi1873MnSIRyKKpMZYOHZDjHUGeMofMx3v37N1C9ZXFxvJCT+cWMWQ78NhWbIlMwTBrvej3xl46SGxkiGo3S0tJCKBQiGo3SWOGisucUFxIx3v/+91Mf8NEzTXTvC3qJueylft9b3Ju377PDUcsN1397xrKOjs+QyXSjqn7q696Crmc42/FHmKZOy9aPsm3bbyLL1q26pqXK7VZQhtM/f5RMPAaSxPVveltpubeqGoBcIoFpGEiybI2bwFddu8ozFVxJCNEtWD4u1ixtpVLOy+1XiG7BCjBbYMPMFmJjY1aqmizL9Pb2ous62WwWwzCuyBTzyQh3PP4KxWIKl6uJQiGMzeZGUZwlR3JJsuF2t+Lz7SlFw222IOm0dY0IBK4T7sbrmekPPqdHkap3wa88bf3+j7dBqAOO/vNM0T14DL7+HiimYfd9cOfvrerUL4WhLqtN19Y9ldz3a9egFXT+5TefQi8aJMM50rE8iVAOgEP3tXHda1sY6Ijy3b85DiakIvklie6FnMxhos767DjjA3FkSUJ1KnPGLBdbPA5M00tfJgcVTra+/e00e5xIkkSxWOT//J//A0B1dTVDQ1Z2yje/+c3SPCd59Lv/xdb2HVz32jfit6klV/QrgWIxytDwgwBsaXovquphfPynaFoMgEIhxDOHb8UwNAKBg+xo/z1UdX3UXa8H5uvnbZomL/zgO4BVH161pbm0Tes1B3nlsZ+gFQt0H3+B7QcPce7I4an1B65b7dMQXEEI0S24fCbF9tmHYfAFsHtAnXhavVDkerE6wEtlpfYrEMxitsCeTCmfXuu9a9cuhoaGOHnyJLIs09fXR2dn5xWZYp7J9BCNvkCxGEPXs2QyRWy2AF7vbrze3TOi3z7fnpLT+STt7Z9cu8kLls70B5+jJ0FSwNShbg90PTYRAa+yxsb6SptJHT+E7/4KFDNw8EPwxr8Gef33zW1s95MYz9J3KsI3/uQIhayGXjRoaPdzzau34vLa8de4iI9nOfr9HjpfGCUZziHLErtuqmfr3qolHWchJ3OwRHloME4xr6GoCuQgm8oTGWHZI9+SJLHV62Kr18XQ0BBVNjeSJGEYBj/96U/JTziU79mzpyS6i8XinP1omobNNNgbvPLE5MDAv2MYWWTZzpYtVkbHZKo5wPDIQ3g87WSz/YRCj5JMnuKmGx9BVddfHfZa0PXi8zzzrX+jkMlgd7sxTZP262+k69jRUpuwG+57x4xt2m+4iaarrmbw7Gm+93//gkBdPdFh6/N51a13CBM1wYoiRLfg8pmMakR7IReFLYegkFw8cn2x/bl1zWo5M3oS6vbCHf8DlDIf4XL71fWLPy+BYBHKmamVq/WORqN4PJ4rPsW8UIii6xkmw5+GUcTl2lLq3z05xm4Pzlgm2GBMf/CpuqB2t3Xd7noUxs6A3W39BKjaBsC2sR+jnPiGVQN+7x/DbZ9Yu/kvAdMw6T0ZJjaWoXV/NaYBHUdGiAylAZBVieqmChxuFVmWeOunruWHX3yF8b4koX4rTdjltVGz1YssL00EL+RkDpawliQJxaagF3VkRUIvGgtGxy+F2b4V58+f5/jx41RWVpJKpchkrLZfew5eT8wT4N53/gJbPI6S2O/p6eGrX/0qAO973/tK18wrCcPIMzD47wDU170Vx0Q9t1VeY7Gt7RO0tX2caOx5XnzxPeTzI4yN/5jGhvvXZM7rja5jR0iMj6HabOTSKbqOHaH9+ht54fv/BUB9+062XL13xjayrPC2//EZnvmPf6PzuWeIjY7gra7h6lfdzU1vf/danIbgCkKIbsHlMxnVqN4B/UchdA6CrYtHmJfSn3t6yvrAC3D+UTA16H3GWn/37y9tv0J0C1aAcgJbluU5UeylOp2vZ0zTZCRWmNF7u1w96XSDNLe7bcYYuz2IaeoYRgFFcSJJdmR56mZ8emTbNE2Go/kFjydYp8x+8Bk6Bz89Bbk45BKWOZqeByRovR2p7wj7Br9mbWP3wpnvW/9N8p6vTTmerxMuvBLiyPe6rai2ZpBJFGnY7uf1v7KPbLLIQ//3RV55chBJkbjt/h088fUOxvuS7L97Cze9ZTu9J8P8+J9P8vNvnsMTcCxopjbJQk7mYIlym0MFNBRForrJj2KTF4yOXwqzy2rq6+sJBoNEo1FM06S6rp6aq/agbttFTyKDqlilNM0Vm7de+2IZHn6IQiEESGzd+uHScoejrvS7z7cfAP/ET4BcdnDV5rjumaxSmPxemHj9nj/+7IKbOdxu7v7Qx7j7Qx9bubkJBGUQoltw+UxGNfJJCGyFpoNw1RuXxzV8em1g6DzoOQi0QGLQipwIBGtIOYFdjvmczjcSI7EC54fTGAbIspU6avXinmK2QRpYQnoSt7sNr3eEROLMhDu5jmHkMU1zjqBeyvGuFP6pf4xvDkcYyBfI6SZVdpWDPjefaq3n6grL6fnlZIa/vjDC8USGaFHHpyrs87r4jZY6bgqscequJEHtVZAYtkzV9IJloOZtgO4nJn3zLApJq0xpOlqe9UbPSyES4RyKKpFLWU77266tweW14/Laqd/u58LLIQbORhk4G6X3Fcs87KqbGrA5FNoP1vL4vykUcjoDZyJLEt2LuZGXE+XR0RSSkpo3On4pzC6r8fv9/Oqv/ioA/akcJ2MpkkWNdFGnxmmnaBjEixqTlbVtbW185jOfuex5bFRM06Sv/8sAVFfdhcczldIcDN4MyIBBIvkKVVWvIpF4pbTe7W5d3cmuY9qvv5GRrnMUshnc/gDt19+41lMSCBZEiG7B5VMunXu5TKKm1wbGBqxav8SgdcNWt3fx7QWCdcB0cZ7NDjAy8h1yuVEMwzIOq6m5B7//AACJxCnGxn48774aG9+Jzbb6Ub90TsMwwOdWSWQ00jkNmCmCC4UohqHhcFSTz4coFKIzarIlSaKy8mYAkskzSJJCNjtAJtMzQ5wv9XhXCs/G0oSLGi1OBznDoCuT5wfjcZ6JpnjhlqvRDJN3nugirul4FJldHifnM3kejyQ5HEtx7OY9VNtX6Ou+nIHm7A4SW2+2XMztXiikweaEfAK23gLjZzBtbr577QO84Q1vwGazrcw8V4ppRnHjfUkAtKJOZNhKM1ftCvmsVhoz1pugZquX2GiGQt7KwFIdy1O3Xk6ULxYdvxQWytyJFzV0wyRgtzGWLRAvFPHaVfy2K+920zQNxsZ+yuDgv6FpKSorb6Ot7dcJh58o1W5v3fqRGds4nY1s2fJ+Bga+Snf33zA6+gOy2X4APJ4d1NYuQ4eXTcK2624ASZphpCYQrGeuvKugYPlZSpr4pTK9NrByG1TcBFpuqqZbINhg5PNjZDJ92Gz+kuiejqK4cDhmimpNS6LrEzfxqmfONquBx6kiy3kSGQ1Ztl7Pxm4PIssq+Xyo1OZrNpIkIcsOFMU9rzhf6vGWC9M0icVi5HI5nE4ngUBgxVLZDdPkZ+EEPdk8bS4H91b5kBc51hevbsGpTD3I/Gz3MH/TO0pU0zmfyZPTDeKaJeD+765m3loX5BvDYT55tp+8YTJeKK6c6C7XonF2BwmnH27++JQwNw147gswfsbaJtgGoctPe15N2g5UM3ohQSGnoeR1dM3k3NFRRnsSFPI62YR1PlfdVM+WXUEcbpV8RuPJr3fw8uMDJMI5MEFWJHbcULfI0cqzlPZgK9Gre3bmjqtxKyejKfw2FZ9NQZElCrpBhU2h2mmjxeO6olzJJ4lEnqar+0/J5YYBSKXPIUkS0egRAHy+AwSDh+Zst3PHH+Bw1DE09C0ymQs4HHVUV9/JtrZPIstX3vs4H5Iss/3g3PevHPM5nQsEq4kQ3YLVZaG2YuVYySi6QLAGeL278fn2oesZenu/PGe9Vdc8M+rb1/cAup7G5WrBbq8s6wK80tQHrLTU6TXWs5k0P1vMDK2cOJ9dM17ntwGeBY+3XMRiMYaHh9F1HcMwSmJiJcT3z8IJvtQ/Tt4wcExcy15T7QfmF+ROReaH4zH+oW+MpKbTlbHSratsKttdDjTTJKAqxDSd3+zo5wt9Y3Rm8rhkiY8217J7IgX9sil3/S7XonF2B4mq9rl9vCV5amzrXdD5yPLMcRmZbpYWqHXTsrcKacL0rHVfNZIk0XV8jP4zEQo5Db1okormUO0KdW0+9t25hV03Wg/Q3v7bBzn2owsMnY8RH8vicKs07Qhw/RtaqWm+NFG8WAuxlWJ65s5kOvlkn+09fg97AxXEi1qpBdiV6sWQzfahFVOAjCwrGIZGMnmWgwe/ueB2kiTT2vIxWltEzfFy0X38BY49/B30YgHFZn2XLFWwCwTLhRDdgkvj8N9DxyMQ7oRsFCrqoPU2K/pcuYDrcLmoyEJR8pWMogsEa4CiWAJoqd5+6XTPhOEOBINrlz4nSdJETfX8kRZJkua0+SpHOXE+t4bbs+jxlotcLodhGKiqSjKZRNf1UsujYHAqWt/f38/gYHkjoxtvvBFJkjh16hTJZHLOeq/Xy549e+jJ5skbBrsrXJxJZenJTtUrLyTIxwsaLyYypbFbnXYe2N9GhWqlJn/nunY++HIPvbkCL6esDIotTht7lktwQ/nrd7kWjTteY0WzO35kjTENJv6w1uvZ1/U1eIi0FHpPhjnxsz40zUBVrbm37q8GQJIlWvdX031inEJOR7UpgEH7dbXc/YHdc/ZV2eDh1b+0Z1nnt1gLsdVgMp086LARzRdJaDp7gxWl+u0rGZdrK6qtAk1PYhgGsuzA671qrad1RRIdGUIvFqhubiXUf4HoyNBaT0lwBSJEt2BpzI5wHPkSxAcsx3LVCbFeeOkbVg/Wj78ATl/5/ZSLiggEgnmJxSxTKbu9Bre7ZY1nszyUE+drWcPtdDqRZZls1hKrLpcLXdfJ5XJlx6uqitO5sBOzw+GYUZ/sclnit83lwCHLnEllccgyba6pc1xIkH+wqZoPNFYxmC/yp11DfHcsxsdO9fLwdTuQJPiNM3305gr8z+2NfKCpin8bDPOZriE+duoCba6d7PO6L/n9KVHu+n3j/ze1bno2kiRDtMcaH+2xXm+wB6ixsQyaZlDVVEF4MEVsLLP4RqvIYi3EVgO/TUWRJaL5IoosXZG12/NRWXkbsvyHM2q6W1t/ba2ntWlZKIU8WN+IYrMT6r+AYrMTrG9c49kKrkTE1fFK5GJTvGFuhKPlFrj7DyEw8Tz7kU9bNXqpUeh5EnbfV34/5aIiAoGgLPn8WMlEJxg8uMazWVlWs4Z7NoFAALBqVFOpFLquI8vyvMI6EAjQ3t5edt0kTU1N1NbWzll+b5X1QHJ6CvkkCwlysB5WbHHa+Y2WOr47FqMjneOhsSgSEi8lrQcGv9BQiUdReE9DJZ/pGsIEnoqmlkd0l7t+z5eNtAkesAZq3aiqTHgwharKBGrdc1LOW/dXMdaboJDVcPvstB2oXrX5rYRJ2sUyWas9PZ1cYCFJMrW1r6a29tVrPZUrgoVSyCdN1oTpmmAtEaL7SuRiU7xh7g1Uw4EpwQ2WO+1zX7B+Vxb40i1Xoy0QCMoSjVpRblX1UlGxudMSl1IzvlJIkkQwGCQQCMwxVCtHJBLhyJEjyLKMzWbD6/XicrlK0WyA3t5eenp6sNvt+P1+tmzZgt1uR5akUsr4bMoJ8khR49FwgrfUBrBPPBx9NJwobZPRDfRpDtovJbPcUektiXAAt7JMPhgXc/3eBA9YW/ZWAcyo6Z6dcn7gnmZuesv2GWNWi5UwSbuUOTRXOEU6uWDNWSiF/GJM1wSClUKI7iuRS4lATL+Bku2QjcGzn7eWb78Hjn3FGueqtHqqGhONV889MlXXt+v1sPN1q5NiODua33rXyh9TICiDaZpkMj0z6peXYixULCZIpc4B4PdfiyRtbgPBpdSMr8Ycptdwz4fdbscwDAqFQikN3W6343Q60TQNWZax2+2l2vCxsTHi8Tj79+9HUcq3hypnogbwo/EYv9kxwCfP9lFpU9FMk0jRMgSoUGTeUBMgpxv87+5hCqbJ+1/uZrvbQfdEarpPlXn9PCL/orkYj41N8IB1sm57OrNTzuPjWa65d+sazVAgEEwiUsgF6x0huq9ELiUCMf0GKhuDvmct4S6p8PO/hMFjoNjA3wRHv2T9DvDYn0GsDyRg8IXVq+ubHc2fHgoSCFaRTKaHWOwYhqEhyyrZ7BDpdAemOfWZjEQOE4u9gMPRQH39GwCIxV4ELPMdv3//Gs1eMJ3q6moaGhpQVZXh4WFGR0dLdd+GYWAYBoFAgObmZmRZxjRN+vv7GRoaIp/PE4lEqKmpKbvvciZqAP8xHKVCkcnoBuMFq99zpU3h7kofv95SR7PTygh46Np2/r5vlBOJLF2ZPNV2lRv8Hj7VWk+dYw16X29SE8xyKecCgWDtESnkgvWOEN1XIpcSgZh+A/Xs5y0xG2yFMz+AYgbc1eBvhMbrZkbP80mwOad+X626vtnR/GgP0LQ6xxYIplEoRNC0NLLsmPgZo1iMzxij61l0PYuieCde50kkTgLg8+1DllffIOmKYbFODPkkPPbn0HcYV6zfut75mgi0v4F401uYtFqz3IllfD4f8oRoliSJ6upqhoasNMdCYaa79PTodkcqV9ZETcdkl8fJ2VSOqyqcSMB9tQE+1jyzXvyg38NX9s1sNSdYfsqlnAsEgrVHpJAL1jtCdF+JLCUCsZDZWuV20DU49R3QC1BzFdz+2/DiV+ZGzx1e60ZWAjw1q1fXNzuaH2yD0Oq3UxEIDKOArqfRtCSSJGO376Sh4VMLbqMoDrZv//gqzXATo2vw5Gdh9CTU7bWEtDLra+/IP0G8f/5ODNkoHPkiKA4Gr/4Y1QOP4Iicw3X076m5Okls6y8Aljt5TU0N2WwWn89XSiMPh8OlQzkcM1Pnp0e307pVkjPbRM0hy4zmi6gyJDSdWrttjsGaYPUol3IuEAgEAsFiCNEtKM9CZms7XgM/+IQluMFKJT/yRcjFrHruXa+fip5P79U6fflsLsVRfSFmR/Nb74LORy59fwLBJSJJdmTZjaI40PU8kiSi1qvGk5+1yl0MDXqfsZbd/fszxxz8AOx/z/ydGLYcglf/KVz/i4ye6qS/6Y3YCzGUQpysx9pGlmW2b9+OLMucOHGC/v5+HA5Hqe4bLFFeWVk549DTW4SdTmbYVeFil8c5x9W8K5MjoRn4VYVt7pnrBAKBQCAQrH+E6N4MLLdghYXN1mQZ5Gk1giOvzNzWVTl1/KveYP23GJfiqL4Qs6P5xeKl70sguAwcjkpstgoMQ8Nmq8DhqFx8I8HyMHrSEty+JkgMWq9h5jWzbp+1fpLZnRi8dXDrrwNWG7BIJEImmiWnNODIjuFt2kXT1tZSb++mpiZisRj5fB7DMHA6nVRWVtLY2FhKO59keoswp6Lwumr/HGdz6/UyGaEJBAKBQCBYE4ToXguWWyQvJlgv5XiLma198pXy200er+ORizteOZG/Eg8TBIJVxu1uA5jhXi5YJer2WhHuxCDIqvUa5r9mGvpUJ4ZgK2y7Y+bu6uqo88jww7dBchj2vRPufmtpvaIoNDc309y8tAZKC/XsFggEAoFAsHkQonstWO6o7mItwC7leDteMzM13DQsEbwU0Xspxysn8pf7fRII1gBJkvB4tuHxrPVMrkDu+B/Wz+k13VD+mllIw4Mfhq5HLTO1X/gWqLNqpyPd8O/3W4K7+SZ4099c1vQW6tkt2HyYpklftpt4MYrfFmSra9uS2gcKBAKBYOMjRPdacCl9shdisaj05PFqdkPvs/DiV63lC0WOZdlq7xXtsbaN9iy93delnF85R/UjX1ze90kgEMxhUwsBRYU7Pz2VMXP+Z9a1ZfY101UF/+8NMHwCqtrhfQ9azuXT6T8K33gPZMKw8/Vw/7+CXbSLEiydvmw3LyeeRzd0FNky2mtxr5K5qEAgEAjWFCG614JL6ZO9EIu1AJs8Xu+zkB4FybQiyLCwiL7UhwOXcn7lHNWX+30SCARz2PRCoFzGzPRrpmKHx//McjDfegu852vgnlV3f+o78NDHQMvBoY/B6/63KHURXDTxYhTd0Km0VxMphIgXo2s9JYFAsEKEw4eJRp8ru2779k+QTJ5hbOzH827f2PhO3O6llSoJNgZCdK8Fl9IneyEWawE2uf8Xv2oJ7q23wPiZxUX0pYre5Tq/5X6fBALBHDa9ECj38HD6NfPvD1qCG6CQhK+9c2rb6z5gXXf+80OAaQn0wWPw5VdPjXnj/4XGa1bpZAQbGb8tiCIrRAohFFnBbwuu9ZQEAsEKI8subLa5ZUSK4sLhqJ+xTNOS6HoaAFUVNWmbDSG614Kl9MleqeM9+w+W4F6KiL5U0btc57fa75NAcAWy6YXAYg8PtcLU77M7MQRboe0OwLRe6wUYfGHmmHxyuWcs2KRsdW0DmFHKIRCsdwzD5PGOMXpCadqqPdy1qxZZ3iQlSKuAx9NGXd3ce1nL62XmNaCv7wF0PY3L1YLdLjqdbDaE6L6SWEhEz+cUvl5Er3AyFwhWhE0lBMpdJxZ7eDi7E0PHI1Pp6KlRGD8Ln4mvzvyXGcM0+Vk4McMdXd4s9fobEEmSNlfphuCK4PGOMf7lqR7ymoFDte677tldt8azunRM0yAcfoJM5gJudytVVXciSSt3P5lMdpBMnkVRnDgcdVRV3YrDUTtnXDrdQ6EQAiAYvH7F5iNYO4ToXm+spLicLqJnH8c0rN6069UpfCXaogkEgs0lBOa7TlzMtWypXhYb4Jrzs3CCL/WPkzcMHBNzE27pAoHgYugJpTnUXENrsAKA8ag2tW40Q18oRzxdRDesZfceqMLrWjt5sZioPnv2Dxgd/T66kQcMbDY/wcDNtLZ9HG/FVaVx0ehRLvR+gUTiJQwjh91eQ3X1veza+UdLmkdxolTLes4po+sZMpkestk+tmz5hTnCOxazsqjs9hrc7pbLeg8E6xMhutcbq9Uma/Zxgm3r2yl8JdqiCQSCzcVydIZYqpfFBrjm9GTz5A2D3RUuzqSy9GTzaz0lgUCwwWjyecjZpl57HFPSYTRWIJ4u4lBlMgVjDWY3l3D4Cfr6voxuFFBkOwDV1XeX1scTxzHMIjabD13PUizGGRv/EZHos9x261MoipvR0Yc5dfqTmKaOzRbE495BUYsTDj8BzBXdpmmSyfRQKESx24O43W2oqg+7vQ6ns5Z8PoTdXk0y+QqmqROPn6C2dirrKp8fI5u1vEWCwYMr+v4I1g4hutcby91ObKnHgfXtFL7Utmjr9aGBQCBYeS7W/HF6tDo40SIs0g1bbwanD6p2zO9lsQGuOW0uBw5Z5kwqi0OWaXM5Ft9IIBAIJkjlNIp5CUUxKWgGiiRT5526jhxo8+K0+ekbz/Fid2INZzpFJnMB3ShQUbGTVOocmcyFGeu3tX2CgYEHSqLcZq9ibOxhNC1GOt2Fx7Odsx1/hGnqtGz9KNu2/SaybMklTUuVPWY63U0k8gy6XkBR7JimicvVRD4/Qj4fQpZVKiq2k053Yhg5isWZXiDRqBXlVlUvFdOi7YLNhRDd643lbpM1Xwrk7OPser3Vh3u9OoUvtS3aen1oIBAIlodkAr7+tfnXt7wP/JGlXcemR6vzKUACh8e6htz88YUj1xvgmnNvlQ9gRk23YPNjmiaxWIxcLofT6SQQCCCJWn7BRWKYJi+cTyBJcPe+ap4+HbGi2dM+Sr/1ny/x8CvD3LC1ivde11ZaXtQNPv/4eb794gAj8RxVHgdv2NfAb75m54xI+UrgdreiyHZSqXMosh23u3XG+pqaV5NMvsLIyHcxMcnHjgJgs1XidrcRiTyDpsUAKBRCPHP4VgxDIxA4yI7230NVK+YcM53uolhMIEkqxWKOdLoLmy2A17sbXc9jtwcBGcPITRxr6lpcLCZIpc4B4Pdfu6L15YK1RYju9cZyt8maLwWy3HGWsx5xoXrHS6mFXGpbtPX60EAgECwPsgK1s0xoCgWIxazft+6Hq/csbV/To9XnHwUkaL5haZHrDXDNkSVJ1HBfgcRiMYaHhzEMA3niuzUY3GRdCQQrztmBNNFUkevbfXicypz1//FCPw+/Mlx229958GUeOj6ILEFrtYf+SIZ/faaHU0NxvvGRm1bU/byq6k6AGTXd05EkGYezkVx+qLTM6WzmwP5/QlUryGS6S8uHRx7C42knm+0nFHqUZPIUN934CKrqXXQeicTLaFoCVfUiSTaKxcjE8W0EAteVxsViLwIGsuzA799/6ScuWPcI0b3eWG7H8PlSIFfamXw+sW8Y8MT/glf+ExQVPHVT6y6H2edjGJYL8eQNcetdl7d/gUCwPvB44G3vmLns6acs0e1wwI6dS9/X9Gi1wwtIS49cr6fuDgLBNHK5HIZh4Ha7yWQy5HK5tZ6SYIMRTRU5N5imudpJc7VrzvrecJo//t4prtsaYDg+8/PVOZrkoeODAPzP+/bwwVta+dnpUX75gRc40hPhJ6dHeN3ehjn7PPyfX+PZB79Rdj6f/Pp3kRUFXdM48tB/cPrnj5IMh3H7/ey86TZuffd/w+605ilJ8owa7nJsaXovTY2/QC43yOnTv0Us/jwvvfRhDh16GNPUS+O2tX2CtraPE409z4svvod8foSx8R/T2HD/jP15PNvJ50fQ9Tyq6sbj2Y7TWU8qdY5CIYyux1FVH05nI5WVN5Xagel6nkTiJAA+3z7kiRp0weZEiO61YqWcb2fvN9i2NimQ84n9zp/AyQchPTZxk8vK1ELOFv26ufzHEAgEa08uBx1nrd+v3gM228LjpzM9Wj1Z0x3tWbeRa4FgKTidTmRZJpPJIMsyTqdzrack2AAYhsHZwQzxdBEAExgM5xiKWAaMumHdRw1F8lwYy+KyKXzuPdfynn96bsZ+jvZESr+/fm89AHdfVYtDlclrBk+eGy8ruidxeX0E6matnwiM//gfP8eZpx5HkmQCDY3ER0d48YffZexCF+/6w79Auoj7aEmSSKfPoU+kfOfyQ3R3/zU+377SGJ/Pijz7fVMR6Fx2cM6+PJ5tSJI0w0hNkqRFI9eK4mD79o8vec6CjY0Q3WvFUpxvL0WYz97vTf/dqk1c7RTI+eodI10gqWD3Qj4JqmtlHgTMFv3RHqBp+Y8jEAjWllMnQdNAUWDvvvJj5ruWimi1YBMSCAQAyGaz6LpONpstLRe13YL5ODuYoWskg2lavgAAhgmYM4MWpgk2ReYdB7fw41Mj5Ir6jPWjyanI90v9Me7ZXYcsS1R67AzHcwzGFs682HbdDbzuv39yzvLR7vOceepxAO760Ee49nX30XXsCN/5yz9l4PRJzj//HDtuvGXWXA1CoccIhR7FBIKBGzExqK97E7JsJ5O5QKEQLo3P5YdpCX4UkAGDRPIVqqpeRSLxSmnM7BpxsAS82209uC0UohPj2sS/N8EMhOheK5bifHspLWnKic2bf3XqpvPIF1enp+x89Y6V28E7kVJud8Pe+1fmQcBs0R9sg1Bh+Y8jEAjWDl23RDfAjh3gdpcftwHaewkEy4UkSaUablHbLVgq8XQR0wSXXSZbMKj327j5qqnPy49fHCdTMDg+GOFcOIaExNW1AX7llp04lKma7/ZAgN+7dy9PdY/x5ad7kGWJe3bXzdbu83LuyGE6Dj+Fw+Ohbls7t7zrv1HXtp2eE8dKY3bceCsA2669AdVmRysW6Hnp2BzRHQ4/QVf3X5PLWdHpWOwFstluOjr+EJdrK4VCqFRrDQo11ffidDayZcv7GRj4Kt3df8Po6A9K7bw8nh3U1pb/7shkeojFjmEYWsnt3O1um9NKTAjxKxchuteKpTjfLibMy0Vv5tvvUm86lyvtfb4I0kobuM13nNa7oPOR5T+OQCBYO851wEQUj/3XzD9uA7T3EiwdI1Og8GTHvOuV9lrUbTUUX+rHSGQhr4EkITltyPU+1O21SMrmdAjWekLog1HMbAGnbrBVhqJLYdyjk8vlMHXjinxfBIvj99gIJYtkCwaSZL2exDBMskWrD7dpwgsXohimybaAl2rPzPIFt10t/TeUzNMTSmMYJtGMFfhoCsxf7iDJMp5AAFlWiAwN0P3i81x46UWued19RAb6p47h95fGO30+UuEQydA4pmkQDj9RMlFLZ3rQtBSy7ARMdD1LRcXV6FqKbLYP09Sw2SpxOhtpbHgXDQ2WX8jOHX+Aw1HH0NC3yGQu4HDUUV19J9vaPoksl2+9WChEMQwNh6OafD5UinjPFuIez7aL+bMINhFCdK8VS3G+XUyYlxPS8+13qTedKxERWqn69YWYLfqLxZU9nkAgWF1ME15+yfp9awssFMHbAO29BEtHkiUk/yxzJ83ATFu1p5JDBcPEGEsiuWxQ4cTMFzHTefSucSjo2PZuznIjI5rGLGhIbjtGUUPJaqgpnaa0yag3TUyN4prnfTELGukmj2g1doVyVZOVKRRPF/F7bKXXAI93jPGtl3oYimXpjWRKy7/24gW+9uKFGfv5wzft5k9/cAaA1io3bdUeHjs7Rl6zRPsdO2vKH//WO7n29W/GVWH5/Vw4cYxv/6//iaHrnH3qcaRp0fQZTAuhh8NP0Nf35VIP7kDgelS1glwuDoDTWc32bZ9c1GhNkmRaWz5Ga8vHFhw3Hbs9iCyrpb7cdnuwrBD3eJa8S8EmQ4jutWKxWkLDANOYMvfZ9fq5wryckJ5vv0u96VyJiJBI7RQIBMtN74WpNmEHrll47AZo77WcmKZJtL+XbDyGyx8g2NyyqcST5LThuKV9xrLiqUH0dB5sCkpjABQZx2uuLhkrmYZJ4ecdmNkiRjS9BrNeHWwHmkvRatM0Sb3Si20wiWJKpEMxxsNh8JuYErS1tVFbU1t6X7LDETpSA3P26fV62bNniW34BBsWWZa5unluD2qAnlCavGZwS3s1VSNJXr+3nm01HnpCab74RBfhdIE37W/gH957HYZh8pNToxzpidAbyfAXPzxD34RQP9RayWuuri97jMrGmQ/CWq85iOpwoOXzyKo64xqWicepCFZiGga5ZBIAb3UNmcwFdKNARcVOkskOsrlRfN79OOzVOJyN1FTfO6eF2OVgmmYpfdxmC+D3X0exGCulkgNzhLjgykWI7vVK50/guS9MCVVJnhsdvpjozVJvOlciIiRSOwUCwXLz0kSUu7YWGhsXHnuFGaZF+3sZOnkCQ9eQFetrvnJr69pOagUxCxr6oJXKqWytRFInImKSRPGVAYxkDjNXtNKpATm4eUNNkiKjj8TRusdBM7BNRP81ySSLlU5uAs1pO56zEfKnIqX3JWOzIoaT9d+qqmK323G55raMEmxeTNNkJFYgndPwOFXqA3baqj3YVYmjPWE0wySZK3LXrlru2S3x/565MGN7WZb491++kb9/7Dz/9eIAfZEMlR47t9TKvK16hGf/82s4PG6CDVvYdu31pQdjR7/7IFfd+ip81bUAXHj5OFre+vwauoa3qoZkOARA55FnuPZ199F9/Hm0opW23nbgIG53DkW2k0qdwzAyJJMvoygeFNlObc1rFo1wXyyz67gDgYMEgwdL66ebq00X4oIrEyG61ytLEaoXE71Z6k3nSkSERGqnQCBYbt7y1rWewbolG49h6BruYBWZaJhsPLbWU1pR9L6w1RZSllBbqmasM5I5zHi29FpuDKBevchDmg2OWdBmnLPhUBgK6rhUD5IkkUqlcOoyakErjckHHQzbUlAEm82Gw+GgoaFBGK9dgYzECpwfTmMYIMuW6L1rVy0v9cf47okhVEXiaE+ExzvGuGd3Hc/87lwha1NkPvXqnXzq1TsB6Dp2lGMPf4euUxHS8SgefxBP0OpVvf3gIQBe+ukPeeobX8VbVY3N4SQyZGVdKDYbV99xL1uu2sOZp5+g4/DPefwr/8yJHz9MbHQEgKar9tB+w02l1mKZzAXS6U5S6U4qKnaSSp0jk7mw7O/VYunjkiTh8WwTKeUCQIju1eNi65qXIlSXI3pTbl7LHRFaTiG/FvXhAoFAsIFw+QPIikomGkZWVFz+wFpPacUwdQOt13IfVhoDSI6Zfdodt7Rj6gZmPEvhRB/GUAzNZce2s24tprsqqFurUJorIVek2DECw3G2xFXi7T4Uu41UKkWnL0dbSyvOAigd4ziieapcEsNOKBaLFAoF8vk80WiULVu2YLfb1/q0BKtEOqdhGOBzqyQyGumchhx04HPZqKpwsKveS8dIkp7Q0ss0oiND6MUCDk8FyUgYh6cCvVggOjJUGnPjW99Fx3NPEx7oIz42gq+6lqZdu7npHe+hsnELAK0HriPY0MTpnz9GbHQEt8/Hjptu5bZ3v78UMZ+MZodCj5HN9pFKnUOR7WVbfV0u5eq4BYL5WBei+/Of/zx/9Vd/xcjICAcOHODv//7vOXTo0FpPa3m52Lrm1apBXI166+VM7RT14QLB5sY0oa8X4nHw+y2TtE1Uj7waBJtbAGbUdG9W9KEYTERslbbqsmMkRUaq9KA0+NEvhNG7x1C312xqp25JksBlR91WQ2E4jpzVqDFcaL5pbfVkiYzdQHJBIAW1WYVRp4TD4UDXdfL5PGNjY8Tjcfbv348yn5GVYFPhcarIcp5ERkOWrdcAbdUeHKpMx0gShyrTVr308G2wvhHFZicdjSArMvl0CllVCQ/00XXsKNuuvZ79976O/fcufD+nqCq3vut93Pqu9y16zMna7Ukn8+m13LNdzquq7kSSLv56INLHBRfDmovub33rW3zqU5/iH//xH7nxxhv527/9W1772tfS0dFBbW3tWk9v+bjYuuaVimLPjgpvtHrrjTZfgUBwcfT1Wq7kug6TN/ktrWs6pY2GJEmbuoZ7EtM00XusGk+5xotcMdWKSA+lkGwK8oTLuanpGJOuyyagG7DJRLdZ0NDHEiRdkCvkcTqdVESmde7QDYhkcGkyWdVyknba7MhWSSwSEtvbtlFVV4NpmvT39zM0NEQ+nycSiVBTU951WrC5qA9YWQ3Ta7rBSjEHy1StrdpTer0Utl17PQCR4UEK6TSpaJiRrk7C/b3EJ1LEJ9PML5fZgrq5+UNzBPVsl3Pgkuq9Rfq44GJYc9H913/913zkIx/hF3/xFwH4x3/8Rx5++GH+9V//ld/93d9d49ktI8tZ17zUFOulRIU3Wr31RpuvQCC4OOJxS3BXVkEkbL0WXBbx4SEGXz5GMjSGlssBsO2WO2jYvbc0ZuTsScbOnyMdHsfQrMjxte94L+7A+k2XNMaSpTZh6raZgtCIptHPj4FdQXLYMDMFS3QCcq0Xyb7mtz/LjqkZaK8M4pBAVkExrH9KACgySr0P7cI4u5IuipKJfCaCUjBgYowedBKsqSIajZLL5WaklBcKhdU/IcGaIEkSDUEHMLMftSxL3LP70soyJFlm+8FDTN6xvfDwd4gM9lPd3Mp4Xw/nX3iO6MgQwfrGGeZql8JSBPV0l/OVqvcWCGazpt86hUKBY8eO8elPf7q0TJZl7r33Xp599tmy2+TzefITboYAiUQCsGqQiuu5F3PrXZbRS7THagPWetel947u/Bkc/Scw8iA7rP3uuHfuuFC3JdBr9kHorPV626xjLue8VgLDgK7Hpua37c5Lmu/kZ2Ndf0YEgiWyqT/PFRUgK5bglhXr9Rqcp2maxONx8vk8DocDv9+/YdtuJcZGiA324/D6SqLb0PUZn59w3wXS4XFUh5OClgJA07RV+Ywt9Hnu6+vj8OHDDA8Pk8lYkerXve51HDx4EL17zBrkc6J77WRSKf7lX/6FWCxGm7+ON+y9GS8KZioHsgQVDqRaL7RUbsp/OyYGeb8NKVXApklIpoluk1GrvEhtVWiqhO6xkVF1nLqMnNVAsd4Xo9pN1GOSGh9nbGwM0zRnvEeqqm7K92wl2NTX52XCV1uP4nARGuynqGmM9HQRGRpEsdnRDYO2aw4uvpN5SCZ7KWomHs9VpNNdJJO9+P0z/xZ2ewsSbhKJHhTZjd3esmn+XqZpYoRSGJk8stuBXF1xWd9d4vO8OEt9byTTnNZVfpUZGhqiqamJw4cPc/PNN5eW/87v/A5PPvkkR44cmbPNZz7zGf74j/94zvKvf/3ruN3uOcsFAoFAILiSkQwdU5KRDZ3KqJXKmfIEyLmmevLKuo4hyzjyGbwpq/1WNFCHrtrK7nO1GBsbY3BwEIfDUXrg3tzcTHX13Prt3t5eIpFI6fV8465EHA4HFRXWzfdkbbZhGBiGQbFYJJVKUVNjpZXruj5jnKZphMPhtZy+QCAow3YlSLtSPhvpJ4VuTGCPUkNQduLA+vdcQGfcyNClRylirOJsNy+ZTIb3vve9xONxfD7fvOM2XH7Vpz/9aT71qU+VXicSCZqbm3nNa16z4IluKmZHug99tHyke3aUePvd8zt9X8zY1eToP0PHD6D6Kitav+tNcOgjF72bYrHIT3/6U1796ldjs63tTaRAcLmIz/PKMzo6SiQSweVykc1mqayspK5u4zpeRwf6GHr5OJN+w21bGmm78bY548bPd9Bz+EkAXnXHHcvufN7Z2cnRo0cxDANZljl06BCtra3zfp4zmQw2m410Os3nP/95APbs2cPBgzMjYadPn+b48ePs3r2bM2fOzDtuszNfhkYoFKK3t3fGWFmWkWWZQCDAzTffzMjICNFotPRwQ5Ik/H4/LS0tqOqGu11cMzbL9fnJjjG++mwvBc3Arsp88OYW7riIOu6l0nPiGCd+/DB6sYBis3PNa994WZFu0zSIRJ4mm+3D5dpKZeVtl2SSthEwusYxe8JgU8BlfdbMggZFnXtbr8FM5ZFTBSSHCqoCRQ01p9Gi+GmpbUS5tnnRY2yWz/NKMpl1vRhrehWtrq5GURRGR0dnLB8dHaW+vr7sNg6HA4fDMWe5zWZbvx+GxWqwJ9eHOyGXAKcfqtrnr9W+6rVWSthS2mZd/fqlzbHjETj6+an6b0VaH67g1dugU4bxV6x5VW+Dy/g7r+vPiUBwkYjP88rh8XhIJBLk83lUVcXj8azb99o0TaL9vTPcymenExZTSTD10ms9nyt7PtMdqlVVXfZzjsfj6LpObW1tyRl78hjlPs9+vx9gRlmZoigzxsXjcX70ox/R0NDAvffeWxLds8ddKZQzPGtoaKChoWHB7VpbW2lpaSEWi5HL5XA6nQQCgQ1bVrHWbPTr84VonnQRdtX76RhJciGa594VOJ8dBw+hyPKy1XQD1Nffs0yzW98UFQUdy6fCvt8S0PpYAr0nhJnRkFQV+UAdar2/tE3+uS7MaAZi2Yv6fG70z/NKstT3ZU1Ft91u5+DBgzz66KO89a1vBax0p0cffZSPf/zjazm15WUxQ7PJ9akRSI2Bpw68dXPHTbKcLbgmWa+u4KvVOk0gEAimEQgEAGaIj/VKtL+XoZMnMHQNWbG+1me7l7v8AWR5SlDbnWtTjlVZWYmiKIyNjaEoCpWVlRe9j56eHnw+Hzt27ADgv/7rv9B1nXe84x3I6yFDawMjSRLB4Po1zxMsDcMwebxjbIbTuCxf3MOTy2kRdjFMmqwJLh1jJEFu+CTYFGSfC7nOh2mayB4Hco2X4rkRjFDKioJnrfpjOShKclebNc8X+tSnPsUHP/hBrr/+eg4dOsTf/u3fkk6nS27mm4LFBO3kemcAEkPg8luvZ49bqmv5pbBeXcFX4gGDQCAQLMJGEh/ZeJRCJoNit1PIZMjGo0DrjDHB5hYK2QzJcSuzzH0JYnc5mBTK4XCYXC5HOBxG16ci8IZh0NnZWVrvdDqpqqqaEb2dbqoWiUTo7e3lzW9+M9XV1USj0dU9IYFgHfJ4xxj/8lQPec3AoVr3iRfrPH45LcIEl45pmhjjSYx0viSaF8w2kbDSxyUJM523tg2nsN+0faplYqaAGc+WNpGrKrBdu3WlT0UwizUX3e9+97sZHx/nj/7ojxgZGeGaa67hkUce2dC1c3NYTNBOrk+NWH0+snEr0j173PSIuWyHwWPgCiyPAF+uiPJKPhgQCAQCwRy0QoFCNo2ZTiHJEsV8nkjfhTnp5oHGLdO2WpuUYVmW2bVrFx0dHTz77LPouo6iKKUHHJ2dnTz77LOkUilSqRQejwev18u+fftK+/B6vei6TiQSYWTEMob70Y9+xI9+9KMZx3rkkUd46aWX+OVf/uXVO0GBYB3QE0qT1wx21XvpGEnSE0ovvtEsLqdFmODSMcaTVnq4YWJOZCcoteU9q5TGAGpLVakFoj6epPjCBTBM9L4w8j7rmm+/ZivmfhMzlaP4Uj9GOEXx1BD2A4vXdAuWjzUX3QAf//jHN1c6+WwWE7STr8vVdE9nesS89xk4+SB4qufvwT3JUoTwckWUl9IbXCAQCATLhmKzY3e5UewO9EKeQiZdSjfXixqDJ0+QjUWRp9Vr9794lKFXjlNRW8euO1/DhaOHCV/oQp/W+uT0I99DkmUa9uyncc+BZZ1zJBIp1XaPj4/PWe50OkkkErhcLnRdJz6tX3s4HMZut5PNTkVuyrVs0We1RduImKZZqrGe9LPJ5/Oi3nqZMQ2T3pNhYmMZArVuWvZWIV1kOvZ6YrVSwwXLj5HOYxomsteJkcxhpPMo84yVPTM9rpQaL0WbAkUdI56l2DM+FS2XJSSfC6W5Eu3MMMZQDKO9ds4+BCvHuhDdm57FBG1p/SLidHrEXNdAti2tBns1hfB6rQ0XCASCTYo7EMTu9mDoGna3B0mSMHQNxeYgE42QT6fQC/kZ2xRzWYq5LHZPRel1LjnTgTWfnujXnZ+57XIwu7YbrNTybDZLOp2mWCwiy3JJcE5vBZbJZMhmsxw7dow3v/nNvO1tbyuti0ajfO5znwPgjW98IzfccMOyz30lmS6ynU4npmkyMjKCYRilNHxFUUq16xulBGK903syzImf9aFpBupEOnbr/vXRbm72Z2IpD1tEavjGRfY4MGUJI5lDkqWSKC6Xdq53h1Aa/UguOwB6KAlF6zphFjT0gSiGJKFiRctNw8AIp6YOpomWYauJEN0biekR82wM+p5dWg32agrh9VobLhAIVg/ThN5e6O2xXre0Wv9Nv1E0TejrhVgMCgWw2yEQgK0tM8cJFiXY3AJQSic3TZPhZIJMzKpv9tbUohcLVLW207j3wBync4Adr7qHHa9aPcffydruSCSC3++ns7OTrq4uent7kWUZRVHYutWqOTx16hSappW2NU0T0zTRNG2GGN8MxGIxhoeHSy3VHA4HhmHgdruJxWKAlV6fyWTI5XJrO9lNRGwsg6YZVDVVEB5MERvLrPWUSsz+TMDiD1tEavjGRa7xAswQ16ZponWOog/HkSQwHDZUQOsPo50bAacNSZEx0xMPSCUJ7CqSTcGMpCke70PzODBzxZIol7xOJJ9zjc7yykSI7o3A7PTwG/8/a/nslPH5WE0hLNzGBQJBXy88fwSSSev12Jh1E9DSOnPMyy9BOgPZDLjd1n8wc5xgUSRJmuFWbpomkiQR7u0mNT6KVsijqDacPj99Lx4l1NWJJMvYXNb7PdvpfDWYrO0GKzW8s7OTaDSKYRi0tLQwNjbGli1buPnmm3nnO99Z2m52Lfhs9/NgMMhnPvOZ1TyVZSWXy5VE9qRZnCzLZDKZUkZAJpNBlmWcTnHDvFwEat2oqkx4MIWqygRq14+z8+zPhHjYsrmRJAml1jcjpVwfS6APxaCgY6oyEkWMdB51Wy3GSBwjlcPMFpBcNqSgBznoxhiOYxQ0q4c3YKbylumax4Fc60XdXivKU1YZIbo3AvOlhy81RXw1hbBwGxcIBPE4FIqgTnzFFArWstljdB0cDsikwe6wXs8etwEwTZPoaIpcuoDTYydYV7GmNzOTIjzY3DIjqm2aJuNd5yhms6h2q69oNh5bs3nOJhgMLthOzDAMTNMsRfl27dpVipivdwzT5GfhBD3ZPG0uB/dW+ZDLfEacTmdJZMuyTDAYRJKkeWu6BctDy94qgBk13euF2Z8J8bDlysNI5637a9UETcc0wUzlkOv82G5onfN9Y5omktOGtFQHdMGqIET3RuBy08OFEBYIBKuJ3w92GyQnIjIul7Vs9hhFsSLdkgSFvBXpnj1uAxAdTTHUFcbUTSTFurGprPeu8azmRsAHXzlhpW7bbGiFIrJiw+UPrNn8ZrN9+3YURSESiVBZWTlHUHd2dvLcc8+VotySJG2Yvtw/Cyf4Uv84ecPAMTHn11TP/ayX6w8vbpZXHkmW1k0N92zKfSYEVxayx4FhVzDBKs2SwUjmIRMC5rqbl4uWC9YeIbo3AqJOWiAQbCS2toDJzJrurS1zx0D5mu4NRi5dwNRN3D4HmUSeXLqw1lMqi8sfKKWUy6pK9fYdpZru9UBXVxfxeLwkuGcL6umO52NjYxuqnrsnmydvGOyucHEmlaUnW96cbiP1hxesDuIzsQSyWTj2AvRegEzG+j6prII77oRzHda6+Xjv+8BbviXXekGu8aIy4WyeymEk8yg+16Lu5oL1hRDdGwFRJy0QCDYSkgStrdZ/C41paYX1o/kuGafHjqRIZBJ5JEXC6bGv9ZTKMttwbbJ/93rh6NGjpSg2UKr5nmS24/ns9PP1TJvLgUOWOZPK4pBl2lyiTY9AsCxks/DQty0PEVmeypYaG4V0GjweqJ3l3h6PQz5vZVvZ1/+/xemRa30sAZnQHHdzwfpHiO6NwHKnhy+lb7dAIBCsNyYdz+Nx68ZqnTidB+ustlvTa7rXI7PTzdcbhmEsGMWe7nheLv18PXNvlRVJm17TLRAIloHnj1qCOxiEN95niWywPEIAGhpg99VT4zUNvvZv1u87dlq+IhuISTdzY8xq8TjZzWE9PUAVlEeI7iuR1ezbLRAIrlyWWyRPOp7ruhWhgHXhdC5J0prVcJumOacF2Ea9+ZJlecEo9nTH842GLElla7gFAsFlYJrQPeFzVFEBD3/fEuA+P1x7LbSXeTB3rgMmHeAPHFi9uS4TkiRZ1/hMEdMwMTLhUiRcsL4RovtKZDX7dgsEgiuX5RbJk47nlVUQCW9Ip/PlJtrfy9DJExi6hqxYX+nrOZq9EIcOHZpR0y0QCAQLkstaaeIA/f1WlNvhsL4fHv2ZlcW5bZoPkmla30lgfRcFNmatvJHOYxomstcp6ro3EEJ0b1QuJ0VcGLMJBILVYLlF8qTjeSRs/dyATufLiWmahC90k03EcQeCaIX8umoBdrHs2LEDm8221tMQCAQbBcOc+j0QhPvfaf3+4H9CLAonT84U3Rd6pr6HDlyzatNcbmSPA1OWRF33BkOI7o3K5aSIC2M2gUCwGiy3SJ50Np+ern4FE+3vJRUaQy/kSY6N4KjwrqsWYEvFMAzAMlKrrq4u61wuEMyHFokQ+vwXSD7+GNp4CMXjwXHVVTT86Z9gb24GIPP884S+9E9kX34ZM5dDra6m4p57qP/931vj2QsuC6fTCjgZBlRVTWVUVVVZojuZnDn+pYkod22dVeu9QZFrrHImY1ofbsH6R4jujcrlpIiLvt0CgWA1WG6RPOl4LgAsF3LFpuKtbSATi1JRU7euWoAtla4u6/uro6ODzs5OYK5zuUBQDi0a5cK73k1xYADJZsPR2oJpmmRPnEAbG8Pe3EziRz9i8Ld+G3QdJRDA1r4dI54g9fMnQYjujY2iQEMjDA5YD3cnzdMiYevn9Ae9IyMwOmL9vgFruacj+nBvTITo3qiIFHGBQLDeESJ5RXH5A8iKil7M4/L5qGrZtiFN1KLRKADV1dWMj49vqP7b6wHTNInFYuRyOZxOJ4FAYEN+DubDNAxSTz5JobcXe0sLFXfcgTSRCTH+t5+jODCAY0c7zV/+MraJ1lBmoYAJGJkMI5/5Y9B1qn75w9R84hNIqnXrq6fSa3VKF4VpmhjjyRlRzUv6+67T7g+XzQ2HYHgIolH4xtesZem0dW7XXjc17qUT1k+fH9q2LbzPdfReLdvfX7DmCNG9UREp4gKBQHBFU67v9kYkGAwSCoUIhUIbrv/2eiAWizE8PIxhGKW0/GBwYxpElSP15JNEvvJVzHweaaK9k/euuzBNk8QjjwCg1jfQ/+EPUxgYxL51K1Uf+Qj+N72R5FNPoU/U8GqhMJ133gXFIq7rrqPud/8HSoVnzc5rqRjjSfSeEKZhYsqW2Lokp+p12v3hsqmrg/vebLUOGxsDVYWmLZYYr6uzxsTj0HvB+n3//sUFdO8Fa3+FAtjtlghvbVvJs5iXZfv7C9YcIbo3KiJFXCAQCK5o1nvf7aWyfft2Ojs72bVrV6mmW7B0crkchmHgdrvJZDLkJtshbRIKvb2Y+TyOnTvJnztHobcXAD0SwZgQ1OmnnkKtq0Px+ch3dDD0W7+FpKoUB/pL+4l/97s42rdT6B8g9fjj5E6fZtsPvo/iXd/1sEY6j1HQQDMw80WMSJoioO5pRN1aVRpXfHkAI5rGzGuAiWS3Idd6Udtrkezquuj+MDw8zPj4OPl8HsMwsNlsVFRU0NTUhGeiv/aLL75IoVCYs211dTXt7e3ld1zfAPe9Zf4D+/3w0V9Z+kR7L1j14Kpq/ey9sHaiWziVbxqEU4lAIBAIBII1YzI6e+jQIXbt2iVM1C4Sp9OJLMtkMhlkWcbpdK71lJYVe0sLksNB/tw5JIcDe4uV0WFq+tSY7dtp/+lPaP/pT7Bvt8rtol/72owxNb/+a2z7/vfZ+i//DIA2Okrypz9b0hxMwyD5+OOEv/IVko8/jjlh/rfodqZJNBpleHiYaDSKaZqLbzQL2eNAMkzIFReM0OpjCTBNJI8DbCpmtoDeG6b40sSDh3XQ/SGRSFAsFnE4HDidTgqFApFIhDNnzqDr+oyxLpeLioqK0n8OxwZz6DYNGD8DvU9bP82lfWZmI3scSMKpfFMgIt0CgUAgEAgEG5RAIAAwo6b7clhvNeIVd9wBMKOmG0CtDCLZbJjFIs5du5DsdgCcu3ZR6OqiODiIOpleDDj37rN+7t9fWlYcHFzSHOZLcV+MyPAwo3/3d/DcEYhEGLXZsDc14X/Lm6n8pV8qva+5jg5Cn/8CmeefR0+lUINBXNddx5a//RvkGi+KpmNki8h2Fe1k+Tk77roKSZl6YJV/rgszmsGIZqwF66D7w+zOBP39/QwODqJpGtlsloqKitK61tZW/GvVFrKlzUpVLxTB5bJeXyyhDuh7GgwN5Am5VbP7oncjnMo3D0J0CwQCgUAgEGxQJEla1hru9VYjLsnyDIE7GXUu9PZi376d/Nmz5M51YBaLAOTOdQBgb23Bc9ONpZZSuVMnqbj9NnInT5b2ZW9dmvCcL8V9MaKf/UukH//YmndzM+Ry5M+dY+yv/g+S3UHl+/8bmWPH6PvwL2PmcsgVFTja2zEyaVKPPmqdvyShNlrvv5GZm3Zdep8UmeK5EYxQCrOgQdZ6P+Sge2LA2htbyrJMJBJhaGgIXdfJZrMAqKqKy+WaMbazsxNd13E4HASDQZqamlDVVZItLS0gcXkPKDJhS3BX1ENqxHp9CQin8s2DEN0CgUAgEAgEAmD914hPjzqjKKCqFM53cf7eVwNW2jiKQtVHP4atoYHg+95H9N/+jfHP/R2Jh39IYWAAAHv7dryvfe2SjjlvinsZZ/XpGBMC37zmGvijP6Q2WMnY616Hmc9THBrCNE2G//CPMHM5fPfdR8Of/gnyRHnApbirm5kCZjxbei1XVWC7dutF72clKRaLpFKp0muHw8GuXbtQlClZqSgKdrudYrFILpdjeHiYZDLJnj17VifrYjkeULirrAh3asT6OXYSOh+21gVaofpq0HMQPge5uNUG2DTB4YXqXbDtXrC5L/dMBOsIIboFAoFAIBAIBMD6rxGfHXUOvOfdFM51kn3lFWSHA88tN1PzG7+Ba6IXc92nfxe1tobYgw9SuHABta6OijvuoObXfw15IiV9MeZLcS+Xdu687bap7Q7dQPyhQaQTJ1B/67cIZ7KY+Tyu6w9S+YsfIt/RQaG72xpsmnS9/g0YySTOPXuo/e3fxrV3z0W9N/ZrtmLuNzFTOYov9WOEUxRPDWE/0HxR+1lJ6urqqK2tpVAo0NfXRzgcprOzk71796IoCjt37sTj8SBJEqZp0tXVRSgUIpVKkUwm8fk2iHN39S7rZyYMuSj0H55alx6HwvNQSIKWB1kBhx9MHbIR6H8WMiG49pfWZu6CFUGIboFAIBAIBAIBsPw14svN7Khzxa234v2DP5h3vCTLVH/kI1R/5COXfMzZKe6TlEs7nxTdqaeewr59O+4bbyRz5Ahad4+1L5sN585dKH4/2WPHSvtK/OAH2NvaMJJJMkeO0PeBD9D2ve9ha2os9WmW1MWTjCVZQvK5UJor0c4MYwzFMNpr15UBlyRJOBwOGhsbCYfDZLNZQqEQdXV1M+q6JUmiqqqKUCgEUNbVfN0iyVYNdyYMR/4O/FutiHY+DpigOq3fvVtAlqB2H7TcBi98CWIXILa0EgbBxkGIboFAIBAIBAIBsPw14svNfFHntWC+tHOA6Ne+jt7djTY8jH37dlr+7QH0SITe//Z+ol//OqgKrn1Tpm7++99B45/9GYWBAbpe+zqMTIb4Qw9R+e4PlPo0M4/7uRHLYOoGSpUlWE3DwAhPpXCjXZpz9nJSLBaJxWJUVVWVvAJisVhpvWEYZDIZUqkUVVVVJBIJstnsjDEbzsHc0OHktwAJ9r4bjv3zxAoJtJwlzPUcFIsweAT6n4FczBoSaF2bOQtWDCG6BQKBQCAQCDYhpmmWoqSTzsdr6US+HMwXdV4Lyj0A0CZaXxmZDNrICGCJc7WyErWyEtd115F67DEyzz6L9957S/tyTbir27dsQakMoo+HKA4OWn26c0UoaKBPiW6tcxS9J4TkdyFXe9FeGaBoU5CcNsxcEYrWPCSvE8m39iUCuq7T1dVFT08PDocDXddLkWtFUaisrCSXy9Hd3U1PT08pvXyyzZrP55sRBd8Q9DwKiX7Y8y5wVU4t99RM1XSrThg/bUW3J6lsh33vXfXpClYWIboFAoFAIBAINiHGeLIUJTVlS2wrtRukJnadU85ETZJlmBDdks1WikwbqaT1M58n39kJWEZpejSKXFGBkUqRO3USeDfFwUH0SBSw3NVljwNDAnN2tLqgYxZ0cNqQvQ7k6gqMZA4zlQcJJI8DudaLur12XTxoUVWVqqoqUqkU+Xwe0zSx2+34fD6amppwOBxIkkRDQwOhUIhisYgkSciyjM/nY+fOneviPJZMYgAuPAn110DDtTPXOXzQevvU65bbrah4ehRO/gdEzsPZ78Led63qlAUrixDdAoFAIBAIBJsQI53HNExkrxMjmcNI50XroWVisd7dlR/6IOHhYYr9/WSOPs/5174WI51Bn6hPlhSF6Ne/gfd1ryX+4LeJ/eeDZI69iDY+DrqOUlNN4F3vQg56UWHRbAX7DZfQS3oVUVWVHTt2LDjGbrfT0tKCz+eb0baurq5uRn/vDUFqFEzDci1//LS1TLfauFnL/ifc/mkr0g2WmZq3EZpugHM/gJHj0HaXFRUXbAqE6BYIBAKBQCDYhMgeB6YsYSRzSLK0rsy0LpV5I8yrzFJ6d/vf8XZyp09TONdJcXQUyW5HbWxAUm14brqJ/LlzONrbafizPyXy1Qco9PaiVFbivfsuaj71KdRKKyX5SuvTvN7N/C4KQ5u7zDRAL1gu5noRKrdNjY10TY2bFOmCTYEQ3QKBQCAQCASbELnGC8yMki6FfNHg7ECK4WieXNHApkj43Tau3ebF41Q5O5BiJJonntEwJsqM33yoFkVe+fTfxSLMq8VCJmpgGanJ6TSSw0Ht7/wO7r0HMdJ5Mi89T+yhb83YznvXXQTuv3/Vz2G9st7N/JZE40Hrv+k8/VnLKK1uP+z7BRg6BqcfBNUFzgDkY1Cc6LNe0QDe+lWetGAlEaJbIBAIBAKBYBMiSdJFR0nzRYMnTkbI5HVkCSqc1taRVIFswcDjhKFInkxex26TyRVW1xl7KRHm1WAxF/Xpc8yd6sDhacE0TJx1Owm87d1osbE1d18XrDEVdVC1E5LDkB4DSQJPrdXju/Uuy91csGkQolsgEAgEAoFgEzDdrZyijqnKKBXOi3ItP92fIpPX8boUbtsdxGm3RLdhmEx6Z9+8K4DTLnN2IM3ZwfQKnU15FoswXyqGYfJ4xxg9oTRt1R7u2lWLvEDkfjEX9elztNU0zKitdx+4AVvb5dXqbgRnetM0icViM9LEJ13Jyy0Hq3XY4OAg6XQaj8dDU1PTxqvnno/b/sfM174tcO0vrs1cBKuOEN0CgUAgEAgEm4CSW3m+iJnXwGEDh3WrtxTXctM0GQznAHDbFZ4+EyWT1/E4VXY2ummudgHgcqxdhfFK9el+vGOMf3mqh7xm4FAtkXfP7rpL3l/wfe/F6OvD3tKCa/e16D0hkk/9HG1sCMeB3fhaXndZtegbwZk+FovNMEQDCAaD8y4HGBwcZGRkBNM0SSYt1/fm5ua1OQGBYBkRolsgEAgEAoFgEzDpVo5NgZyGpMqYhjmva7lpmozECqRzGh6nStCjUpzoBT0aL+C0y9hUmURG44XzCWRJoqlqbXs+r1Sf7p5QmrxmsKveS8dIkp7Q5UXwK26/HZvNBljvc/q5wyQe/z6mrpHtfhnZ77qs89gIzvS5XA7DMHC73WQyGXK53ILLAdLpdKmdWKFQIJ1e3UwKgWClEKJbIBAIBAKBYBMw6VZu5jWY6O0sO9R5XctHYgXOD6cxDJDlPFtrXKV1XpfC3fuqAHjslTDJrE73SGbFRXdWz3AsdpjeTBcZPYVddlBpr+GOqtfiswX4Wv+XSOmJOdu1e3ZzT82bLvm4bdUeHKpMx0gShyrTVu25nNMAZjqt58+fB9nEedXuZalF3wjO9E6nE1mWyWQyyLKM0+lccDmAx+MhmUxSKBSQJAmPZ/6/Qy6X48SJE/Oub2pqorm5mRdffJFCoTBnfXV1Ne3t7Zd+ggLBRSBEt0AgEAgEAsEmYLpb+eya7nKkcxqGAT63SiKjUSha5mmGCX63Wqpp9rtVklmdTF5f0fln9QwPDf87SS2OjILfVgmYjOWHSOspfLZAaWzAVoVdspde+9XLc7u+a1ctwIya7qUwu4WZ45ZbSuumO60bmQwmLFst+qU6068m87X+WqglWFNTE8CMmu75kGWZioqKGcs0TStFzu12+4x1LpcLRZnKB3A41t+DCsHmRYhugUAgEAgEgk3AxbqVe5wqspwnkdGQZahw2ajy2RmPF6x2YBP9wOIZq9ewx7Wyt43PR58mqcUJ2qp5Y9078aiWoNJNHUo2bha3V95Lo2vrsh1blqVLquGe3cLMZ07Nc7rTeq6jA+eOdhzt7ctSi34pzvSrzXytvxZqCSbL8pJruO12O3v37p2xrKenh1wuh6IoVFdXz1jX2tqK3+9f4uyvcEwDQh2QHJ96LbgshOgWCAQCgUAguAKpD1iRwMma7vqAHYcq81QiQjKr8+MTIQByBQMJ2NVopfo+3xknmipS0KZuxB99KQzAnpYKmiqn0oVN06Qv2028GMVvC7LVta2sy7ZpmnRnOgCoUL08PPqfJLU4PjXAtf4baa/YPWP8T8a/i2YUqVB9tLrbuS5wM3Z59SOXc1qY9fXBhKCc7rQuO51477lnTXqKXykUi0XGxy2RWFdXNyOqDdDR0YFhGNhsNqqrq2lqakJVhRQqS6gD+p4GTWe7aqK88AXQslDMgMMLwTZouxfcldb4Uw9C7AIUkmCa1pjqXbDtXrC51/RU1gvikyYQCAQCgUCwCujFAoOvHCfUfZ58KonicFC1tY2W629CdVhCtf/4C0T6ekhHQpiGJWpv/uDHkFdAHEiSREPQAUyJ1UqvjduuDnK6P0U0VUSRJWr8dq7eUkGl1zIGyxV00rNSzSdfa9rMiHRftpuXE8+jGzqKbImgFvf2OXPJGRnyhpUW3J/twaNU4JAdRIrjPBr6AbIks82zCwCbZMejVJAlQ1yL8lLieYbzA7y1/n2r3jZrTguzrVthwnV7pZzWL5alPvjY6IyOjmIYBpIkUV9fP2PdpEu6JEkUi0WGh4dJJpPs2bNnU74Xl00mDIYGnnq2xU8gZQrgrgFFhWwUho9D+Dzc8ilQnRA6bf1010AxDdkI9D8LmRBc+0trfTbrAiG6BQKBQCAQCFaB0z/9IYnhQZAk3MFK8skEI2dPkQqNs/++dyDJMuEL58mlkticLgqZtXFurvLauf3qynnX375n/nWziRej6IZOpb2aSCFEvBgtO86YlpYdsFVxf+MHAXhw6KvEimFOJo6zzbOL19S+hSp7LbIkY5gGT4R+RGf6NGP5YUbygzQ4tyx5bsvBbGHtuOUWeOQRYOWc1i+WpT742MgYhsHo6ChgGaRNr+feuXMniUSCSCSCy+UiGo2iaRqpVIpkMonPt75ara0L3FUgq5AepVero33PQWwtN1rrOn4A/c9YUe1IF9Tugds+DYptavsXvmRFvmOXZxi4mRCiWyAQCASCZcA0DLqPv0B0ZIhgfSPbrr3+svrwCjYXmWjEEtzAtptup+HqfWTjMV588GukQmOEes5Ts30nu1/zJuxuD/3Hn6f/+PNrPOvLx28LosgKkUIIRVbw28rX8joVFzIKBjpV9hoUyRKHVfYaYsUwSS0OQI1jKoIpSzLbPVfRmT4NQEqb62q+0swW1sVicdXnsBhLffCxkRkfHy+9942NjTPWVVRUUCwWkWWZbDaLzWZD0yyfgnKu5gKs1HCA5DjnRqK0N1wztS7YaolugImHOCg26PoJhDuhkIJczFoeaF2d+W4AhOgWCNaAZ2Mp/r53lBPJDJGilZL32Z1b+GDTlOnHhWyev+oZ4dlYilBBo0KV2el28ivNtbyuRhiBCATrje7jL3Ds4e+gFwsoNivKsv3goRU9pmEaPDXwFL2JXlp8Ldy+5XZkSQj99Yg5LZLLZDrrtLTW2GA/Ndt34vBUsNrM7tddH7AvW8rtVtc2gBmpzeVQJIUG5xYGc71ECuMT5mkQKVg1un5bkEghxFh+iB0VV6NIKoZp0J3uKO3Dq4rvxnIs9cHHRsU0TYaHhwHLoXzSFV2SJDKZDKlUiqoqq/1dNpslFouRzWYB4WA+L5IMNbsh0A6v/NB6DZah2sBR63ebG3TNWibJVkp6YmBqH5XtsO+9qz/3dYoQ3QLBKmGYJj8LJ+jJ5ulI5/h5NEmL01ES3dMxTZN3neiiL1fAIUvs8jjpyxV4Lp7mSLyHn92wiz0VrjJHEQgEa0V0ZAi9WKC6uZWx/gv8eCyK0j9Gm8vBvVU+5BWoG3xq4CkeOP0A6WKa8ew4Rb1IspikwlZBrbuWO7bcwYGaAyUxPpIe4R3feweJghUR/OK9X+S2ptuWfV7lME0TYzw5o8XRlVRL6Q4EcQcryUQjdD/7c0bPniQ3UfsLrFkqOczt1w1M1HpfPpIklU1lnl1n3OxsY4uzlaFcH9FimH/r/wKqpJLWU0hIXOu/iZye4cnwj3k6/DN8tiA5PUPWyADQ6NxKnaNxznEES3/wsVGJRqOlNmFASYAHg0GKxSLd3d309PTgdDrRNK0UEff5fHNajgkWQC/AK9+ESCdICjj8MPCsFe2u2Q37fgH2vAvSo3DyPyByHs5+F/a+a61nvi4QolsgWAamC+o2l4O7K708FkmWXt9b5eNn4QRf6h8nbxjENZ231wW50e/hNzsG5uxvOF+kL2elPP1Waz2/1lLH09Ek95/owgQGcwUhugWCdUawvhHFZifUf4GOuhZe8NQgj8VwTKSYv6Z6+aNwvYle0sU0FxIXSBfTKJJCm7+NVDHF+eh5dFPnpfGXALh9y+38/tO/XxLcq40xnkTvCWEaJuZE/2el9sqppZRkmatfex+9zz9LbKifXDKBr76RbCxKLplY01KE2f260zmN6eZqK8HsOuOx/DAXMp2lFPO8kcOQ7DQ5W7gheBt1jkYyepr9vusZyPaS0hKYGFTaqmn3XM0+33VX1EOci2G+Bx+bhUmRLcsyXq+XTCZTimSn02mCwSD5fJ58Po9pmrjdbqqqqmhoaBCfmdlMtgrLhK26bv/EA5pCEk5+HZKDYPOA0w++LZAascZOIivgbYSmG+DcD2DkOEbbnTwVOcOD5x5kIDVALB8jno9T7armhvob+JUDv0Kzd2abuHQxzf3fu5+BlHWP/Ic3/SHv2rWxxbsQ3QLBMjBdUDtkmeOJDEfj6dJrgJ5snrxh4FUVOtI5DkdTdKXzZfdX57DR5rLTky3wfy6M8L2xGH25AqoE76yv5J6qK+dGVSDYKGy79nrAinh3B7cgOyrYXeHiTCpLT7b8v/XZXGy6eIuvhbHMGOliGpfq4g9u/APe3P5mHjj1AI9ceIT2QDvd8W56E72cP3meoyNHeW3ra/nxhR8vyzlfDEY6j2mYyF4nRjKHkc6v6x7Di2GaJtH+XrLxGC5/gGBzy6I38A5PBTvvfHXptaFpHP3G/wPA5Q+s5HQXZHa/bo9z5W8PZ9cZhwvjFIwCdtkqzVBQ2ec/yH7/9aVt3IqHmyvX3phMsL7Ys2cP0WiU4eFhMpkMsiyj6zrDw8MYhlHq/R0IBIjFYjPSzwWzmGwVZmiWkVqjjlfKoL74z5CPWTXaTTfC0POW4JZVwIRIN1ROCHRDswzWJjg+dIQHur/D8bHjFI0ide46at21DKYG+V7X9zg8dJjvv/X7VNinsg7+4shflAT3ZkGIboFgGZgU1JM32KdT2RmvJyPeDlnmfNpKgdrhcRIqlDdcUSSJb1/TzodO9vByMssrKeuJbZVNZV+FC0V8UQgE6w5Jlks13JFQnBf7xzmTyuKQZdpcS4saTqaL5/U8DsXa5o7m+dsM3dZ0G+milZbc5mvjX0/9K3925M+odFbiVJx0x7txKA5kSeYfTvwDd265k3fveveaiG7Z48CUJYxkDkmWkD1rW0t5saJ59njTNBk+9RKGriEr1u1U5dbWBY+ZCo3j9PlR7XZMw+DC84fRJ4ycqrftWLZzuxhMw8Bz4jB1oQyFui1U7dlZ6t+9ksyuM66y15DUYiQ16zvSpbo3Xe2xYOUIBAIAJUGdzWbRdR1VVclms0QiEcu7YGSkJMTBSkEXTGOyVVhFvSWqsxEOOc8h5SfS97U89B+2enYbGlTvBtUFL/6z9dMZsMR50bpvpaKBk7kw45lxKp2V2GU77971bj6494N89uhn+fcz/04oG+LI8BHuabkHgEcuPML3ur63Zg+IVwohugWCZWBSUE/eYF9d4eJoPD3jhvveiej0I+NxjiczJIoa9nkiWIZp8jvnBng5meUjW6r53W0NPBZO8pFTF/i9zkEaHDZeXxNYxTMUCAQXw+S/9+klJkuhN9FLXs+zI7iDzmgnvYmF263E8jEymlXTejpymlp3LV67l8GU5ZL95m1v5s7mO/m7439H0BHkT279E87Hzl/GmV06co0XYEZN91oS7e9l6OSJJYvm2eMdXh+GruEOVpGJhsnGY4sec/TcGcbOncbp81PIZtAm6lAb9hzAW1MHQMcTPyE1NoqWn8qOOP5f3wCg9dAtVLUub5pw6skniX7lq9jzeRwOB653vJ3Rl14m+fhjaOMhFI8Hx1VX0fCnf4K9uXnxHS6R2XXGzc42auz19Gasz2eLu33T1R4LVg5JkmYIaNM0KRaLpNNpJEkiOeGfYBgGbrebTCYzow5cMMFkq7DJKLYziIwxtT41PHO8zQ3eeqjaCclhSI9ZBpGeWssBvfUukie/TCgXKpWSJIvW3+Jg3UH+/cy/W7uZaDc2kh7hT579E66uuppfu/bXhOgWCAQzmX2DXa6mW5YkXlPtL9V3d2Vy9OeKHEtaN8zTnW2fiqb4Wdiqu3xXfSUeReG+2gDeszJJ3eDn0ZQQ3QLBOmby3/vF0uJrwaE46Ix24lActPhaFhw/6fAMsM2/jQfvexCA+79/P93xbobSQxwbO0Zvopd/fPU/EnSuXVRHkiSUWt+6SSnPxmNzRLNpmkT6LhDp6wETKlvaqNzaiiRJc8ZjgqyoZKJhZEVdUnq4t6aW+PAAuWQCTPBU19Bw1V7qdl1dGlNIp63105h8ra1Ae6NCby9mPo9j505yp08z+ud/gRGPI9lsOFpbME2T7IkTaGNjyyq6y9UZt3raafW0L9sxynEpZQGCjYlpmkiSVIpqg1X3PZmC7nQ613B265TJVmGTNd2axk+z1/GG+mFsqgJbb7NM02Zz7S/OWx7ltXupdlbjtXtJFpJ47V50Q+fBc9b31ZaKLdzUcBOGafDppz6NZmh89vbPosqbS6ZurrMRCNaI2TfYxvTWMPOM/UkIfjAWLy0/m5564prQpm6kX0pm2ed105XJkdKtp41uRbQEEgg2I7dvuR1gxk1LOSZvbrpj3aiSimZq7Azu5PDQYXoTvQQdlrgeSg2VtvnE458obTt92d3Nd/OXd/zlCp3R+sXlD8wRzdH+XvqOHSGfskRuKjSKJElUbm2dM76ypa0kxifF22LU7riK2h1XLThm3xvftiznt1TsLS1IDgf5c+fQxsYw4nEcO9pp/vKXsdXWAmAWCsz/rXZ5THcx96jekqFaRk8hI1Oh+thRsYcDvhuQJInvDX+T4Xz/nP3UO5p4S8Pi7YkuNsNBsDHJ5/MoivWIT9d1TNMkGAwiSVIpBX0yJV0wjclWYZN0/9z66amD7CzTtFlML4+yK3ZeCb2Cz+4jWUhS7a6moBeocdfQ4GngE49/gmeGnqHaVc0/3PMP2BU7D5x6gBdGX+CPb/ljWv2tpYytzYIQ3QLBCjDbWA1mOhc/PB7jtzv6SWlTN7//ORrliWiS63we/mxHEwFVIabp/E5HP/8yME5fzrrpsUkSb6sNrPIZCQSC1UCW5AVruCeZfnPjtrlJFBKcGD9BKBuioBfojHUCsNW3Fc3QMDHJatk5+8nreXL6lZliOSmSp4vmoZMvoRUKJTGmFQqltPHJ8ZlYFL1YIJeI4fIHadx7YENHSivusD5v+QsXCP39PwCg1jfQ/+EPUxgYxL51K1Uf+Qj+N71xRY4/3cU8ocVKLcCCtmoKRp5IMcSR6JOokspe33Wl7XyqH6fsLr0O2qqWdLxyGQ6CzYfT6cRms1KWZVmmurq6JLoFF4GrEohabcBU1Yp+z8P08qhjI8f4Yc8PS3Xc19Vdh8/uI+gI8uWTX+Z0+DStvla+cO8XSs7lHdEOAP7suT/jz4/8+QwT0c8e/Szf6/oe//6Gf1/R011JhOgWCFaA2cZqs52Lk5o+pz93WjdIZws02G28EE/z3xoreTGRoTdboCebx68q3Oyv4JOtdez1uhEIBFcu029u0sU06WKakfQI4WwYVVbJalkkJD6y7yMcajg0Y9vnR57nl378S8Dq9uleb0xGsKfj8gdQ7XbyKetBhMPlKqWNTx+/mSKlkizjvesuXOEw45+1Mh7STz2FWleH4vOR7+hg6Ld+C0lV8b3utct+/Oku5qHCKADNrjbeUHc/mlHkK/3/gG5qJLX4jO2u89/CLu/eiz5euQyH1UKL5Bj5y+fnXe+9pxnPoQaSj/WRv5BAjxfAMFCCTjwH66i4tRFJZLotidnGaoFAYEMKbtM0GYkVSOc0PE6V+oB9dc+jagfQBTVXg7dmKv28DNPLozRTQ5XUkj+Jz+7jlsZb+NVHf5Wh9BDX1V7H3939d/gdUwGp0bT1779ozDUZLhgFctrGfkAsRLdAsALMNlZrcznm9PIeuGN/2brvn4TiM6Lk/2vnlhXp7ysQCDYu029uKp2V3LftPn7Q/QO64l0YuoHP7uOj+z46R3ALFibY3DJR1z1V0z07bXyzRkrNaWVN9u3b2fbQfwHQ/ba3U+jqIvq1r62I6J7uYm5XHGT1DP3ZHv5j8P9RMPLopka9Ywv7fTfM2O5w9DF+Hv4JHrWi1MvbrXgWPV65DIfVQlJl7M1TBoJaUaeYLKCkNQBypokjlCV9ZATJrqBWOdEiObTRDPEf9qBFcgTfurI17xsR0zRntAKbFNiTxmrzrV/tOV6KeB6JFTg/nMYwQJatAE5DcIU6P5gGjJ+F8TMgYaWZ+yd8F5pvhonMgUlm13Df2nQrYD0UThQSvDj64gx/kk8+8UmG0lbJU0bL8N9/9t9L+3r7jrfzqi2vIqfnSkL9xvob+adX/gkQfboFAsE0povqVqeDj2yppjdXKAnqyZTznK6TMUweGY/zuho/H9lSgzztwrtYlFwgEAhubbqVV0KvcC5yjp2VO/nAng/wob0fmmNiU44b6m/glQ++ssoz3hhIkkRVSxtVLW3zjlnLSOlKolYGkWw2zGIR565dSHarbZhz1y4KXV0UB1emvnK6i7lX9dGT6aQzfZpoMQSAjNVOzCFPmV6pkopH8VKQciS1OGdTLzOY6+WdjR/CJi/c7qxchsNqofjs1P7qNaXXQ11hij/pRUlrGDYJbWsFsttG8O07cF9Xi6TKGJkio/9wAj2SI3N8TIjuMsRisRk9uWFmK7DF1q8GI9E8p/vTaLqBqshgemioXNzILZ3TMAzwuVUSGY10TgNWSHSHOqDrJ5CLWa/jA9BqzDt8oRaX0wV5s7cZA4NILlLa9mzk7Ix93dp0K3uq9swwEm2qaFre81tjhOgWCJaJ2XXcH2uu4WPNtaX1k2LaZ1M5F0+TLGocT2ZK4nsy0l0uSi4QCK5sZkcUDNPg+Nhx8nqe42PHeWbwGe5ovmNJ9eCCy2MtI6UriWSz4b7hetKHnyV3rgOzaKV45s5ZdZb21pU5z+ku5sdjR+hMn6be0cRrat9KTs/wvZFvcip5HAmZW6vu5pbKuwjaq1AkFdM0ORp7ihPxIyS1OD2ZTnZW7FmRea4EDllGH7Bq2AstFVQGnNjrPdgbpiL2stuGrc6NHskhqSK1vBy5XG7BVmCLrV8NhqN5MnkdRZYoaDrD0fySRLfHqSLLeRIZDVm2Xq8YmbDVh1tWrbZfWh6ykXmHL9Ticro/yZP9T/LA6Qdo87fhUBx84OoPlP2umjT5nBTqSPDb1//2gg+RNxJCdAsEy8RiEepJMd054VIetNu4kM2T0nR6c1YbmMmWYpP7u5j+vgKBYOMxX4uV2cyOKGyp2HJR/bwFy8daRkpXmprf+A0yz79A4XwX5+99NQDa6CgoClUf/diKHrtoFHkh9jQAbe6duBQ3LsVNnaOJ3ux5BnPWZ7zaUVfaRpIkdnh2cyJ+BICUllzROV4KpmGSPRsmd9oSL86rq3BdVYkkS6hdCSTdBEUieNdWgnUVc7YvjmfId8UA8ByqX82pbxicTueCrcAWW7+amBfZB6A+YGVuTE9LXzHcVaA6IDdhuumsmDJSK8PsFpfNvmae7H9yzvfZQuJ8OuWEerko+kZFiG6BYJlYLEI9KZ4fCcU5nsgQLVo1XO0eJ0lNL4n0S+3vKxAINh4LpedNZ/ZNC3BR/bxXEtM0McaTGOk8sseBXOPdkIZFAnAdOMDWr36F8b/9HNlXXkF2OPDccjM1v/EbuA4cWNFja2YRAyvSNV4YsZYZWinN3CbZyOppOlOnucp7APtEGnlXuqO0D6+6Mg+pTdMgHH6CTOYCbncrVVV3IpV5OFaOXEeExI970WPWd3xhIIkkgXNHkPRzwwC4r62lcnvlnG0L/UlCXz2FWTBw7anCd+/myKpYbsqZpl3Mek1L09f3z4yOPUwuN4iq+qmpuZft234Lm23qfiwaPcqF3i+QSLyEYeSw22uorr6XXTv/aNE51gcdRFNFirqJ0yZRv0Bddvn671XIeqzeBaZZpqa7q+zw2S0uDdPggTMTLcPkqZZhiUICu2y/qO+rpQr1jYQQ3QLBMrFYhHpSTE/Wd0+K76SmizRygeAKZak3FrMjCnduvRMZedH67dXAGE+i94QwDRNTtsS2UisydDYq7uuuo+WBr676cV2KmwbHFobzA5xPn2EsP0zRKJRaiO2s2INmaDwbfYIj0Z/jswXQjCIp3YpuB2xVtLl3Asvv+BwOP0Ff35fRjQLKhNivrr57SdtqoSxGXrNSwyUw8zpaKEs6WcBIFUEC76u2zNkueypM5JtnMYsGnkP1BN7ajiSLh1nlmG6adinrX3r5I8RiR5AkBY9nB9nsAIODXyeReIXrDz6ILKuMjj7MqdOfxDR1bLYgHvcOilqccPgJYAmiO2AnlnYSTxfxe2wLRqxX1TxtOpIMtVdb/01SnOskPsnsFpcPnHqgfMswZapl2FK/r2Z/563lg+XlQohugWCZWGqEerb4FmnkAsGVy1JvLGZHFOZLQ18LjHQe0zCRvU6MZA4jnUdZ60kJNiSvrX0bx+NHuJDpJK2nUCSFWnsDe33XsaPiaopGgWv9NzGQvUBCi6GbGgFbJa3uHVzjO4QqW7e1yy1aMpkL6EaBioqdpFLnyGQuLHlbtdqF7FDRJ7PZKmwoVU4SP7L24dxVia12ZhvQ5NODxB/uBsD/+la8dzRf8twF0zANyywsE7ZSqat3kcp0EYtZ5Qk7dvwhzVveTybTw7PP3Usy+QpjYz+kpuZeznb8Eaap07L1o2zb9pvIE581TUst6dCj8SLhZAHDgHCywGjcVvpMTn9IpBvQN54lndNLieh2Nbs6ovsyWaxl2Af2fGDJ+yr3nbfREaJbIFgmZrcEmzRGmw+RRi4QCBa6sTANk96TYWJjGQK1bl6191VIzesv0iV7HJiyhJHMIckSsmf93xwK1icOxclNlXdwU2X52k2bbOdQ8HYOBRe+AV9ux2e3uxVFtpNKnUOR7bjdrUve1rmrEtM0Z9R0Y5po41bd7Owod743QfwHluCWHArZk2GyJ8Ol9VXvvxrFt4J1vZuZUAf0PQ2GZpmFAbimHl5KTP4+tSwSfQZFcaFpMQAKhRDPHL4Vw9AIBA6yo/33UNW5tfizWegzOf0hUb6ok84bKDLoE8bhDtv6eMC6GNO/zxKFBC+OvXjJkerZUfTNgBDdAsEyMdu9HJghqi9WlAsEgs3PQjcWvSfDnPhZH5pmoE64Frfur16y+dpyYJom+lgCYyxhzbfWh1Lrm5GqK9dYfYen13QLBGvJcjs+V1XdCTCjpnupSLKE++pq3FdXl5aN/eNLANiavTi2zXz4bmpTLZrMvE6hPznvesFFkglbgruiHlIjkAnjqb4Zj2cn6fQ5znX+MYND3yCb7S9tks+Pksl0l14PjzyEx9NONttPKPQoyeQpbrrxEVR14eveQp/J6YI8lja5usaJ3a5wotv62/tcG0OuTf8+K/c9daWzMf6KAsEGYDH38sVEuUAguLLJJgs8//AFel4eJxMvICsSsiyxdW8VyUiOgY4Ip58Zor8rRCGjo6lVDAT7Sb/m57zh1jtXRIwb40n0c6OYuSKYYMazSJKEXOOdY54m6rgF64XldnyWJHnJNdxLofZX5jelc24PsOV/C4GyIrirrAh3asT66a5CkhSuOfCvdHX9FZHoM2Sz/QQCN5DJdJHN9iFJKqapl3axre0TtLV9nGjseV588T3k8yOMjf+Yxob7Fzx0fcAOpslw1Lo3NE0T0zSRJAmPQ0HTTcZieVRFxue24XPbAEt0b8T4zGaMVF8uQnQLBMvEYu7li4lygUBw5ZJNFXjwsy+QCOWQVYlAnZtCViMdyxMeSuF0qZx6ehgtr4PNIOON4kkHsY820vN1nez+Akdjzy57ixUjnbcia7IESJiagZG2rl3CPE2wXpEkaaIGVpQ6CKZRvcv6Oa2mG8DpbGDPnr8uDdP1PE8/cyMAHvc2HNPa1Pl8+wHwT/wEyGUHFz20JEkgSaTzOoYB6Xym9Dm1arfNWT83B6uZmbXeEaJbIFgmFnMvX0yUCwSCK5cj3+0mEcpR2ejhzb9xDR6/A9Mw6XlpnNiYVfs53GWleDe/SeG7/Bhfz1baT9wOukQmUViRFiuyx4Ghypg5HUwTyWGzlgnzNIFAsNGQZKsN1iwSyZO4Xa2oagWmqXO+63+hTfR8r617Iw57DVadt0Ei+QpVVf9/e38eH+dZ3/v/7/ueTdtolyzbWizvTpzYsbM5wbEdjJMGespS4BAOIYUTKA38oOQcltIHS3/tYWlOA6UsoS1ZWMNyApSGEGdBcYNDQhLvu61d1r7MjGaf+8tp/toAAFrxSURBVP7+MdZYkrVbo/X1fDxaMvfcM3Pd8m1Zb32u63PdIp/vcOr1E13jP9q67mAkIafDVKE3eTwYSSg362JE6wvGU1Xx+Wai22IuBoRuYJqM1xhtvFAOYHGybVtnXmmXJOUUePSrrx2QrzOkvJIsbbm9Ultuq1K4P6ZXf1uvSDCu1idM3VTwboU7LRlO6ZrdlSpanqMqa/q3WDFLvHLY9pA13QNrtmmeNv/ZlqVATY2i9fVyV1UpZ8cOGebCqEJN97ZhWLjOt/xMLed/oszMKkWjnYrFkk3vKir+Qnm5yaUA5eXvUVPTIzp37gG1tf06te47O3uNSktvn9DnjLaue6TjHb5o6nXtfRG19nrmRQfz4RbifttTRegGZgjdygGMJOSPKRKMS5IajnYrO98jT5ZLXc0B7f33YzJNU6u3luot/2uLnvjmIfk6w4q1SpKpnMIMlVQkf4GXji1WDMOQc0metGTo9y6apy0MgZoadT/8iOxIRIYn+QO9d9euWR7V9Ji1vY6HsS1b4ZPdineG5CzOVMa6QvbbnmNyc69WT++LF4K0La93o8qXv1vLlr0jdc7aNX8rj2eJWloeUzBYJ49niYqLd2pl9V/LNCd2X43Wa2Ck48ebLm5FZtu67A78s2Uh7rc9VYRuAADSbPj2X1Ubi1I/eNvWxRV8BWVZeuffXi9JeuzvX1JPa1CHf9ekqo1FevaR4/J1hnXT21Zr4y3LdXRfs1742Rn99t+OKK/kOpVUemds2p5hGHKU5jKlfJ6L1tfLjkTkWbtWkVOnFK1fOFUofzCmQCihuGUrnrDV7e9VTqZTFcUZWrM0S4ZhqLYtqIbOsPr6Y6ntmXZvKpJ3GrtFh092y7+vWYpb0oVdCDI3FE3b++PyLV36Vi1d+tYxzzEMUyuqPqgVVR+c8ueM1mtg8PHm7rD2HuhSLHGxS30oaulUS1A9gbiuWzO/ijcLcb/tqSJ0AwCQZoO3/4qFEzr4bKO8hRmq3lSsiisKZToNWXFbxeU5clz4wby4PEc9rUH5u8I69VKr2uuTaww33LRULo9D67ct1Qs/OyPZUtOJHpVUUm3G5LirqmR4PIqcOiXD45G7auFUodr6ogrHksHFYUoO05AvGNfRhoAcpqFVZVlq642qrz8mj9NUMJqerbjinSEpbslVlq1Ya3/yMXDB8GUQsbil/khi2DlSJGYpHE2M8i5zF13MLyJ0AwCQZr3tQcXjljwZTnU2BtTbFpTDZaqtzifDMLRsdb6aTvSos7lfiQsVjs7mfklSXmmmoqGLP2x11PtVcUWhOuov7t/r8iyMdbiYWTk7kj8MD17TvVCEIsm/RzkZDm2s8qok16X//GOHLFsKXgg1m6q9ynDlqaEjrFfP+dIyDmdxpuQ0FWvtl5xm8vF0O3hAqq+X+nqlcFivdZeqLpCr3rBL4f6EPG5blVl9uq6sS3memCSpL+LSS+eL1RLIVNByy53hVOGybG16fYVWbi6Z/jEuEpPtJTB8GcTqpdl6y41LRj1//M+31NX1uyF7yhuLtFv4XEPoBgAgzfJLs+R0mupp7ZdsW063KYfTVDQcV297UDf82Uq1nHlVPef79b3P7Jck9fdGZJiGtv7JCmXnufXir87Kitv69TcPqmBJlnrbkhUzd6ZT1fyQjCkwTHPBrOEerijXpf6OhALhhI42+BVP2LJsqcjr0pqlWZKkTHf6F0hkrCuUpCFruqfdkSNSwC/l50sOhw4fzpI/KuVnhBR1uBUKSyfDeWrw5+h/3OKTyyH9qsYrX8Qph2GpcGmG/L0xtZzuVcuZXr3zM9epuJyZM1Mx2V4Co3U0n6qurt+poeHflbCicpjJteIDe8wTyGcXoRsAgDSr2phcw3nuQIeaTnQr1B+THbOU6XUrvzRLZdV5evPHrtEffnVObXU+OV0Ola8v0A1/tlJl1ck1fG/5+Ba9+tt6tdf51NMWVJbXrbJVebrujmpl582/BjtAOm2uzpVtS42dYfkvzBQxDSkvyymXc+aChmEa6V/DvWGDtGat5E0G5St8NVoXOyWvO67f969WfSBX3S39CsUcalp9i0pXeOV76veSpOvXx7Xlo9vUdLJHv3zgNcmWAt0RQvcUTTZEj9bRfMBkK+fBYJ0SVlQ5OWsVCJxSMFiXem6sQI70I3QDAJBmhmloxdXFqtpYpLrDnao92ClJqt5UnArkS1fn680f3zLqe5StzNMdH7p6RsYLzHdnzgfV2BlWodelG9fmKxKztO9Yt861hWQYhq5esYBC5ZatQx5eu6dC2ntMkmQ4HKklK5JkOg1l9bUpzxNVX8Stl055dPofXpK/KyzTNLTuxjJVbqTR21SNF6KHG62j+YDJVs6zslbIYboVCJySw3QP2UN8rECO9CN0AwAwQwzTUPWmElVvujgd3LZs1R3qHLGzOTAVlm3r6S6fakMRVWd6tLsoV+Yi2qM6nrBTWy4tL/TI4zLlcZkq9LrV2hNRe19E0gIJ3bYtNdRLfX1SXp5UXiEdTwZu25ursuuu0Llf1kmScoszVLG+UOaTv9ebV7fpiYZKdfhd6mxMfq0yvS6VVHpl8v1nykYL0VPZN962bZ3vDisYTign06l4whq3cl5UtFOShkwhHzBWIEf6EboBAJhFgzubOy9Me11xdfEsjwrz2dNdPj3Y2KGIZcljJu+pPcVjbzVkW5YCNTWK1tcrvrxS/x4t01PH2tTqC8tlmlpekKm3XLNcH7hlpQzDUCxh6RvPndHPX21Sa19YRdke3XHVUt23Z62yPbP742XCsmVf2Imvtz+eOuYPJv/bOUdD5VhbC46qoV46dFBKJCTDkF55Repol7KyFH/9Hh37+Xn1tgWVlevW5t2VOvPUUa1tbtbvGperw+/S1bvKdeMWp+qPdOq3vw3o+R+fUna+h2ZqUzTatmCjVazHqmS39kbVE4gpGrfV7Y8py+MYt3JuGOaoU8bHCuRIP0I3AACzaKCzedHyHHU1B9TbHpztIWGeqw1FFLEsbcjJ1PFASLWhyLivCdTUqPvhR2RHIvpS6ev0ZFayUd/aJTnyh+M60erXF39zQh6nqbtvrtYnfnZIj7/WLNOQVhRnq7E7qO++UKujLX360T03zmi1dKQqYpHXpS5/TI2dYXUHYoonbEUubCFWWZLsIH6k3q+W7ojilp16rxeO98g0kluKrbrQcG2UDx1aYa6sSobeyzClX8D19SUDd26eVHtOiselvDz1b79N//nv59TR4Ff+kixds7tCp/bVa0tmrZriWar35UiS1q+w5TpxWKuNhJ5z5ikaN9R0vJvQPc1GW+s91hrw/nBcDtNQodclfyiugpxLp59PxliBHOlHyzoAAGbRQGfzruaAnE5T+aVj/KAPTEB1pkce09TxQEge01R15viN9qL19bIjEXnWrtUhV3JN7461JXrqr3fouf+1U54LIbC5N6QjzX16/LVmSdLn/vRKPXvfTn3r3cl1xX+o7dZTx1rTdGUjG6gWNndFdOZ8v1p7o7pxXb7WLM1SToZD4WhClmWrIMela1fnamVZ8u9YJJbcE3kgjEtSKJo8Fo2Ps2/3QIX53Nnk/zbUX/Z1DP4FXDxuTewXcHl5kmVJZ88kA3dBgbqqNunnXz2sjga/lq7O09s+sVXR1g65gj0qz/Qpkrj44397ba+USKjXWahociKAnJ70d3VfbJJrvXXJWu/Rjg8853AYiidsZWc4tLQwY9wp6Zi7qHQDADCLBhqpDZ5SClyO3UW5kjRkTfd43FVVMjweRU6d0tWlJWpxeVVzqkN7HqiRPxxXJG7p+hWFumf7Sv3kj42p1/3JxjJJ0q3rS+VxmorELdWc6tDtG5em5+JGMFK1cGmBRxurvNpYNfra7a2r87R19djT7kc1UGEuLJK6u5KPL9NkfgF3cSq6qSt7A3JZF35JEI/rycca5Q8nK6Kxrj79+l8OKtITkB1y6ERXnipy++VySbGYVFPTr0M5XvmCCUmGTFNac93U94nGyEZb6z1WI7XxmqxhfiF0AwAwg0Zat8kabkwn0zDGXcM9XM6OHZKSFe9/qKhUVmeh/t9rzTrVlmyy5XaYWr/Uq9xMl1r6wqnXFeUkq+imaagw263zfWE194Yv/YA0mmzH6GmRlyc5HMnA7XAkH1+myfwCbvBU9LXehFwDxWu/Xwnr4tTwzh5JPb4Lj9wKxJyyPJl623uX69VDcbWc6VVfX1Qej7R8aYaufesGlVQskCZzc8hoa71HOj58ucSqsqxUhXsqDdkwNxC6AQCYQQM/LAf7orIsW211Pl3/pmo6lmNUlmXruZPtqu3sV3VxtnatK5WkS45dzjpqwzTl3bVLkvTN353R4wdO6tqqAj34nq3q7o/qHQ/u16P76+UY4zNse9Sn0qos3y3Ztlq6wwpFLbV0hSTbVlmBJ32BpLIq+b+D13RfpoGtBSdi8FT0J5o3afWWUm3eXSnV1+muZQelRH/ylwFXb5KqVgxag16UGu8brr3wtRm8Pt3ZK9l5l70+HVM3XnO1yWwhhrmD0A0AwAzqbQ8q2BdVJBxXLBzXqZdatWRFLtVujOq5k+36t321isSt1NpqSZcce/2GkacFD+5M7q6qSlW1hx8zTFP94bj+729PybalVSXZystw6UBjr0q8HvUEY3rhdKf+dNOy1Hv/4rUmveWacklSTzAqSVqenzHiOKLhkF7+1f/Tyf375O9olycnR6uvvUGv++/vVUZOsrFXNBTUCz/5gU69+F8K9vXJW1ysK7bfqhvf+k6ZjpHXGhuGIRmGevvjCkYs9Ujq7U92805bIDGMZJidJaNORR/tlwFjjXdwB/SBr/EsXttiN15ztdGew9xG6AYAYAbll2bJsmzFwnG5MpwyHQYdyzGm2s5+ReKW1pV5dbLVr9rOfkka8dhIBncmNzwXf0Affsy7a5f2Hm9V4kLJ+pkT7Vry7Gn94Vy36ruS92jMspU1qNHW1589o/wst2w7OR4p2YBtJL/48t+p8dhhGaap4vJK9XW06eDe36j17Bnd+ff3yzAMPf6Vv1PTsSMyHU7lLSlT7/kW7f/ZD9XXdl5/8uH7Rr3G/nBcsYQt05QMGRPa03hOmGIX9FGnok/llwFpWJ+OpKlMBx9rucSsLKXAtOBPCgCAGVS1sUhtdT6deqlVpsNQVq6bjuUYU3VxtjxOUydb/fI4TVUXZ+vnfr9+vyFLv1dCKsjS7+M+/e1zByRJTTs2yWkaqgtF9I+1rXohnq2u93xY2YmEqjvb9L72Ht0a8qW6lUdOnVK0Ptl9uzMQVY7HqUAkrs5AVP+6r1axhJXaVmvdkhzZtlSQ5VJPMKa6rqA+8bND8oVjkqTrVxRqzxVll1xDV1ODGo8dliTtuvsDuua2N6nnfLO++7EPqu3caZ3cv09Ol1tNx45Ikv7bfX+jVVuv16u/+Q899/CDOrbvOW2548+0ZOXqEb9G2RlOuRyGYnFbki2Pa/w9jeeEKVaZJzMVfVxpWJ+OpKlMB6e52sI0D74bAQCwcBimoevfVK0lK3LpWI4JGVjDPXj99it1ltQfUJYMLXU5lZfhki4U0AwjWWF7x4GzaghH5Xa6tLKrQy05uXptWaX+f7L1S/m15EK3csPjkbsqOQ25ujhbG8q8augOqi8cUyxhyZaU5XZoWV6m3n5thSRp7RKv6ruC6glG1RuKqTjHrT/ZuFT/67Z1I64tt62LW3AZhjnkfyWp/vABmWbysdPt0cprrk1+zg036bmHH5Qk1R18ddTQXZbvlm3nqLXnYrCZK4EkFkno5V/X6tyBDvX3RmQ6DHmLMrT2hjJds6RXRiIhFRRK589Lx48nXzQN+35P2ChT0hNxS81nOtXV7FMkGJPD5VDhUq+qNpTK6WZbsYmYynTw0ZquYX4jdAMAMMOmtUqFBc80jUvWaw/ksTeV5eufN1zaxKslHFVDOLnG+n9XL9Xd6tW+1ia9r2y1bBnq3Xi1Ntz93kvWee9cU6zQocM6F+rSytVFuu0tO/X82a5LmrhJmlQTt8LyChVXVKmzsV7PPvRtHXr6N+prb0s9H+juSk27zfB6ZVwI4Fn5+alzfJ3tI773wBTeYCShpYUZc66j8/M/OqkTLyb3Li9clq1oKK6u5n7t/39n5XxDka52O5KBOxRM/uLk0MHkC2dqXfUoU9KP/6FBvs7kmLK8HkWCMbXV9SjQG9LV22n+OBHTPR2cRmrzF6EbAABgnvrPjj79qv2gcp0OXe3N0iery3SVN0tLPC5VZ7pVG4rq/vo2/SpnmRrKi+VMJPT2skK9vjhPjgvdygcL7nteVz3xfW28sNY7XOrQ60c4b7SmbaMxTYfe+ukvaN8PH1b94QPqa29V+YaN6m5uUm/beTmcTlmJxKUvHNYRfaQ1snM9iLScTa6RrryyUH/6kc2KRxP6t/v2KRGz5Lezkh3Gjx9PBu6ypVJP96yvqw76IsnALal6Y5mWrixUKBDRa8+cVX9vWJ0tPpWUMw19PNM9HZxGavMXoRsAAGAechhSqdspp2HodDCip7t82tfj16+3rNFV3iz9fPNq3X2kVof8IR0OhCRJRS6nrsrJVO9DDyvw3HOK1NXK6u2To6RY2dddL+fSsiFrvYMvv6y+x3+h4MsvKxEIyFlQoMwtW1T+1QcmPV5vUbHu+Mj/Sj2OR6P69gffI0kqWLpc0VAy5IV9PtmWJcM0FezrTZ2fW1w6YsCe60Fk2eo8+TpCajjarR/93R8UDcWViFlaujpPm99QKeVdGOuhg8nAPQfWVduDftthpJYtXKxs93X0E7onYLqnitNIbf7iTwoAAGCeGNj+69bmVv335UtVfv0tMkxTz3X59K5D5xSxbD3U3Kn711XoE6eadMgf0j3lxfrUyqV6ptOnDxyr19+cblZ0/6va8cc/yl1dLdOToVhTk/p++UuZubnyrFmjyKlTsqJR9Xzv+7JjMVlZ2QqWVSgjHlHimWemNPa2c2dUsHSZ3JlZsqyEar7/XUWCya7r627aLn9Xpw4/+5TisajOvfZHrdp6vU794fep16/YtEX+EQL2XA8iO+9cL9uWTr7Yqu6W5PWaTkPFy3Pkybow1jTs+305srweZXk9CvojOne4Va11PYoEY6nno6HYGK+efk9+54jOvppcXrD62lLd9j83SpL6OoJ66T9q1XK6V0F/VG6PU4XLsrXp9RVauXnkLvrzGY3U5q+59V0JAAAAoxrY/qsoEpHl8SggW95du7SrKFeFLoe6Ywk1h2Pa1xPQ010+SdI7ygqV7XAouzsqR8JWwmHo11t3aP177tHOW66WJLV98YvqfuRRWT6fsq7dKjM/Xz2PPCI7FlPwlt366qa3q99wyuM0dc+1S7R+guO1LUvnXvujelpb1HD4oBqOHFRB2VL19/Yo5E+Ob8sdf6alq9dpycrVWr7+CjWfOKZf/d//o/wlZeo53yJJWn/zDi1ZuVpWT+SSgD3Xg8iBZxp08g+tWroqT3/yl1cp5I/p8f/7qg7XNMtwGNr+jrXTvu+3bdmqP9I1pFnjZNZgG4ahDdsqVX+sXX0dAUWCMeUWZSkUiCjcH5vR9dzHf9+SCtyD2batX33tgHydYTmcpgqXZsvfFVbL6V61nOnVOz9znYrLvTM2zplAk7X5i9ANAAAwx1i2rae7fKoNRVSd6dHuolyZhqFofb3sSEQ/3P1G3fri88q5sNVXTbdf3bHkmuiKDLd88Yvrow/6Q7rKm6WXO3xKXGgY3lJUpTNGjnZeOCdz61bpkUclSVlbtsi5ZIk6vvwVSckw+1cPfVpZsZAaS6rUlvWX0pbqCV3Hudf+qFf+8xdKxKIK9/crKzdPve2tki0tWblam95wh666dY+k5Lrvt3zy83rhJ9/T6RdfUG9bq7zFJbrillt141vfKWnkSt9cDiKxaEIv/apWsqWV15Qo0+tWptetslV5qjvUqaYTPWn53PojXTrwdIPicUtOZ/IPfbLNGz2ZLq3dujz12EpYevnJU5KkjJyZ+cVGX0dQ+x47rbKVuQr0RBS40J1ekvp7I/J1hiVJ1/9ptbbcVqWmkz365QOvSbYU6I4suNCN+YvQDQAAMMc83eXTg40diliWPBc6ee8pzpO7qkqGx6OflFbo63d+SEsNS9l/OK4zwWQYyXKYuqeiRCVup/KdDvXGE/rEyUb9W1OHaq2IZBgybFtL+xPyhWL6t33nVF2QodU/+akkyVVRoaxt2xQYNIW85KXfqbOgTFY0pFVNJ2Td/2lFd26Uu3z5pQMfpqe1RYlYVMUVK9TZWKerXn+brn3jm0c935OVpVvv/qBuvfuDIz4/lwP2SOLRhKwLe5x3NPiTx2IJdZ9PTjNP19Zbve1BxeOWipbnqKs5oN724KTfI9AbUma2Ww6XQ7Ztq+5omxLx5NZvxcvTv57bSlja+91jMgzpDe+7Ur/4p9eGPJ+V51FeSab6OkJ66T9qdfqPbfJ3hWWahtbdWKZKtmLEHELoBgAAmGNqQxFFLEsbcjJ1PBBSbSgZqge29vpgl097jSyddWeqIRxVeYZb1+dl669XLNHqrAxJ0q+2rNHX6tv0Ym9AtaGI8lwOLTMcuj7uUP6yLL1U2y07HNa7fvsdBc8dkKOkWBXf+qZMt1v2oEp53tveppb/8REdOX5WW//2HpnhkPoef1wlH/nwuNdRULZMDpdbnY11crjcKihbloav1tyVmePWsjX5ajndq1Mvtamt1qdoJKGQL7md2/oby9LyufmlWXI6TXU1B+R0msovzZr0e7Q39Kq9vlcZ2W5FI3HFo8l7YunKQnkLMqd7yJd4+T/r1Fbr0+6/uEK5xZd+nmkaevPHr9ET3zqsjga/OhsDkqRMr0slld5xt7EDZhKhGwAAYI6pzvTIY5o6HgjJY5qqzkxWdg3TlHfXLn1I0oeGvWagyVrXhb231+zYoW9cMXJDrn/bd05uX6/uefJfVNx8TqEly7Xx+w/JXVEhSXIuubgfd+ZVVyW3CNuwRKceKFSio1NtdQe1//yP1RfvVjgRVpYjW8syKrQ1/ybluvIlSb86/2OdL2qU7ipIvdfTqtGR82f0Z0vvnLav1Vz3J395lV79bb1qD3Yq0JNcf7ykOldX7SzXuhvSE7qrLlR5B6/pnqyc/Ez1dQYVDiZ/QZCdl6Gy6gItqSoY55WXr73ep1eerNfytfkK+aOqO9R5yTm2Zet3Pzypjga/rr61XDf+2SrVH+nSb//1iJ7/8Sll53sWZDM1zE+EbgAAgDlmd1GuJA1Z0z2egSZr9oU9tiXJO8Ie25K0JtiuK3/6D8r3dal++Vq5v3h/KnBLUubVV8vMyZEVCCh89IikdyrW3KxEd3INcluJpfORRuW7CuUwnPLH+3Sq/6iawnV65/L3y21enP6d68xThnmx0lrgWlzTfjOyXbrprat101tXz9hnGqYx6TXcw5VW5qu0Mn96BnTBRBu8dTX3y7ZsNZ/uVcvp3uRrL+xidu7VDj340Rq94S+uUP3hLknS+huXyuVxaPXWUj33PYei4YSajncTujFnELoBAADmGNMwtKd4cutmB5qsDeyxHb3QZG0kyx/4gmK+ZGBZ5rbkvf9vVXsh++T/+Z+r4O1vV/GH71X7l76s3p/+TMFXXlW8o0NKJOQoKVbBO9+hO8u3yetM/jLg993P6rDvFQUT/WoO1as6e23qs7bk3aR13o2T/ApgIZpwg7eBhG1r0I7hSZZly4okZCUuPtNe71NJpVe9bUFFI8lp8E5PetbLA1NB6AYAAFgABpqsRU6dkuHxyF01xl7P0WjqP121ZxQe9FT8ddslSUV33y1HTo66H3lU0fp6OQoL5b11l0o+/nG5liwZ8nZlnnId1iuSJIcx9MfL3/c8q+e7nlK2M0fLM6p0XcHrlOXIvryLxayybVs9bQGF+6PKyHarYEmODGP8NdQTbfCW6XWraHm2fF3JO9NbmKGQP6qQP5bapzsciMmT5VQkGFfND0/q0HNNyfNtyXQYWnPdkhHfG5gNhG4AAIAFYKDJWvTCmu6BxyNZ/ewzoz43WP6f/7ny//zPxzzHsi0d9x+UJHnMDCXsuOwLlUqn4VS2w6uoEZY/3qcTgUNqDtfr7cvulsucW/tpY+J62gJqOdslO2HLcCTDdmHZ+NtzTbTBW297UE6PQ0tX5qmntV9LVuRe2F4tljonI8elt/7vrXrlN3VqOdOrvvaQPFlOLV+Tr2vvWKGSCrYLw9xB6AYAAFgABpqszaSYFdUzHb9WU7hOpkzlOLw64n9VpuHQTYW7VOAuksNwyrZtvdS7Twf6/iB/vE+1wdNam3PljI4V0yfcH5WdsJWV61HQF1G4Pzr+izS0wVteSaZs29aBpxsuWd89EM4j4bhyizO1cnOJbr1rwyXvV7g0W294H/cR5j5CNwAASJu+vfXyP9Mw4nPL/+F1siIJlddmqetbh2X1RmRFLTnzPcq8uljeHeUyPfyoMpfYtq2G0Dn1xXqUYWbqsP8VdUbblGFmKtvhVamnTF2RDtX1n1aBu1j9iYAqM1fKMAytyd6gA31/kCQF4v5ZvhJcjoxstwyHoaAvIsNhKCN7YrMWBjd4qzvUOer67unovg7MJfxLBgAApsS2bIVPdiveGZKzOFMZ6wpH7EQsSWa2U87CYXvtGpIdjGlJa4bizqBcJVmSL6J4Z0j+ZxsVaw6o+C9owDXbBgftiBVWa7hJkURYvfFuJZRQmadcV3g360TgoLqjnYorptZIs5rC9cpxJqf4VmWt0tn+k6n3HGjAhvmpYEmOJA1Z0z1ZY63vNkwjGbSPJM/TEY3a6RyYDwjdAABgSsInu+Xf1yzFLelCpSpzw8gVqYx1hSp8x7pLn3CaaqoK6pr37JAnJ0N2zFLHvx5StMGv8MkeWcGYzCxXOi9j0RgcnvNcBakK9HgaQud0yPeyElZCoUS/DMOhUCKghJJdomN2VEd8ryhihZWwE8pxeBVTTL3xbgUSPtV0PimH4VAgkaxu57uKVJ21dqyPTPs14fIYhjGhNdxjGW9994Q7nQPzAKEbAABMSbwzJMUtucqyFWvtTz4eRehIl5oOvSAz0yH3cq9y31Al9/IcObxutS0Ly7ywvY/hMuUu9yra4JcMSQ4C1HQZHJ4dZvLrXZW1atzX9cV6lLASKnQX63w4LEuW4nYi9XxXtH3I+Us8y9Sf8CvbzFHUjipuxxWzo8p3FWpF1hptzr1eTnN6fgSd6jVh9o03hXyinc6B+YDQDQAApsRZnCk5TcVa+yWnmXw8ElNyeF2SaSjeEVL4RLfCZ3pU+qHNMko9Q05NBKIKHumUJGVuKmFNt5L7Ej93sl21nf2qLs7WrnWlModNs53IOYPDc3e0U32xngl9fp6rQA7Toe5opzIdWVqaUSGPmTFqZXkmq89TvaaZYFu26o90DQmVTI++aPD67pFMtNM5MB/wLxkAAJiSjHWFkjRkTfdwWZtL5L15WWqKePhUjzq/e0SK2wrsb5H3z6pT58a7Qup86KgsX1TuqlwVvGX1zFzIHPfcyXb9275aReKWPBem2b5+w5JJnzM4PDtMh/JcBSN+3vDQXJGR/DOaaIg2DGPaqs3jBfiJXtNsYHr05aGZGhYSQjcAAJgSwzRGXcM9wFUytDqVsbZAZpZTVjCuRF8kdTza4FffD0/K6o8rY0OhCt+1XqbbkZZxzze1nf2KxC2tK/PqZKtftZ39UzqnMnOlpKHheSRzacr2eGOZ6DXNBqZHX57xKuHAfELoBgAA02pwV/PY+X55d1fKdaFzefh0j6xgXJLkLMiQJOV3udTz0DEpbivnpmXKe9NKpuEOUl2cLY/T1MlWvzxOU9XF2VM6Z6IV6Lk0ZXu8sUxnVX26MT0awABCNwAAmFaDu5pHWwIKvtouR75HhttUvCPZbM1wm8q5eZkSvqhWnsqRZEsOQ9FGvzq+dTD1XvlvXi338slvR7SQ7FpXKklD1mtP5ZyJmuyU7XSu4R4+llxnvuqDZ+dFt3KmRwMYQOgGAADTYqDCHXipVZY/KveKXCUCURmmIStiKdEdliPfI09Vrryvr5SrJEuJdr8MXQhNCVvRRv+Q97TC8Vm4krnFNI1L1mdP9hzLtvV0l0+1oYiqMz3aXZQrc5SwOtkp2+mcjj58LLZtX/wsw6H2yPkxm7rNpoHp0QMN1Q4+27jgGqrZtq2etsCQ/brn0p8BMFcQugEAwLQYqHAn/FEleiOKH+2U6XbKvTJXWVcWK2Nd4SVhw1GQoVe2deuOO+6Qy8V+3OnydJdPDzZ2KGJZ8pjJpl57ivNGPHeyU7bTOR19+FgO9f0x9Vkt4SadCRxTpiN71teej2UhN1TraQuo5WyX7IQt48L2fpe7fzewEJmzPQAAALAwDOzb7cy/sA1YzJIVjCla75d/X7PCJ7tnd4CLWG0ooohlaUNOpiKWpdpQZPwXTVC6O4jbtq364Fkd6vujIlZYpmGqO9opW5ZMw6FCd7ESVmJObRc2WG97ULFYQu4Mp/o6Qzp3oEO2Zc/2sKZFKBBRLByXLSkWjisUmL77ClhIqHQDAIBpMbBvd7wzJMM0ZHo9snwROTKdUtxKhnLMiupMjzymqeOBkDymqepMz/gvmqB0dxAfMn3dcKT2CY9YYZ0PN87J7cIGyy/NUjxiqed8nySpvd6n+iNdC6LanYhZiobjskMxGYahRMya7SEBcxKhGwAATIuBfbpDR7sUbfTLTliSaSjeG5HhMmWF4rIte8GsZ51PdhflStKQNd3TJd0dxIdPX/eYGbo679oRG7jNRVUbi3TuQIdi4bgKyrIVCccXzPZhDpcpt8cph8tUImbJ4WISLTASQjcAAJgWA/t2Z6wrVPhkt2IdQcWaAoo0+WWYhsK1fXKf7B53b29MP9MwRl3DPRLbshSoqVG0vl7uqirl7Nghw5ydQDXa9PW5vF3YcNn5HpkOU/6usLLy3Atm+7DMHI9cmU7ZCVuuTKcyc6ZvBgWwkBC6AQDAtBoI35kbiuTf16REb0SusmzFWvuZYj5HDQ/Zsm11P/Ko7EhEhicZpLy7ds3K2CY6fT2dW5ddjvojXWo53SPDlBIJS0tX5y+Y7cMKliS38xvcvRzApQjdAAAgbQbWecda+yWnmXyMCZvMVl+XI1BTo+6HH0mFbFdFuexIRJ61axU5dUrR+vpp/8yJmmhFO51bl12O3vagEglby9YWqKs5IHemQ/VHuobs3z1fl1wYhkG3cmACCN0AACBtBtZ5xztDchZnph5jYiaz1dfliNbXDwnZkmR4PIqcOiXD40lWv+e4dG5ddjnyS7PkdJrqag7I6TQVDSUW7BZiAEZG6AYAAGkzMNUckzNQ4f5BS5faIjHdmJ+tE/3had3qazB3VdWQkJ2za5cM0xyypnuuS/fWZVM1MJV8oLLd0xZUPG6paHmOupoDC6apGoDREboBAADmmIEKd3s0po5YTC/29muJxzWtW30NNhCq50LjtKlK99ZlU2WYxtBK9qHOIZXvhdJUDcDoCN0AAABzTG0ooohl6Ya8bP2hr18rMz1697Kiad3qazDDNGetUdp0mS/dzIdXvhdKUzUAoyN0AwAAzDHVmR55TFMn+sMqdbv07mVFaVnLjZl3SeUbwIJH6AYAAJhjBirag7uWAwDmJ0I3AADAHGMaBpVtAFggCN0AAADziGXberqzTyeOHNPy9lbtLi1Q7jxsfAYAiwWhGwAAYB55usunbx45rf62XrnjDoWffl7/TZr3jdAAYKHiV6IAAADzSG0oonA4olVd7Ypn56ghK0fR+vrZHhYAYBSEbgAAgHmkOtOjjAyPzhaVytkfUGUwIHdV1WwPCwAwCqaXAwAAzCO7i3KljWt0QrHkmu7dtyhnx47ZHhYAYBSEbgAAgHnENAztKcnXnl03zfZQAAATwPRyAAAAAADShNANAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE3oXg4AAIBFxbYsBWpqFK2vl7uqSjk7dsgwzVGPA8DlIHQDAABgUQnU1Kj74UdkRyIyPB5JknfXrlGPA8DlIHQDAABgUYnW18uORORZu1aRU6fU8bWvqelDfyVJcuTny7tnjyKnTilaX6/wyZPq/MY3FXz5ZSUCATkLCpS5ZYvKv/rALF8FgPmC0A0AAIAFYyJTxN1VVTI8HkVOnVLC51P03LmLT5qmIqdOyfB4ZMdiqnvnf5cdDsvMyZFn9WpZwX4Fnnlmhq8KwHxG6AYAAMCCMZEp4jk7dkiSggcOqOfhR5S5ebNira2Kt7bKXV0t7xt2y1VZqY77/6/scFi5f/qnWvr//zuZGRmSpESgf2YvCsC8RmcIAAAALBiDp47bkYii9fWXnGOYpnK2b1fwxRdluFxadv8/pqrhriWlKrr7brmXLbtYAbdtnf2TO3Ty2utU/967Fa2rm8ErAjDfEboBAACwYAyeOm54PHJXVY14Xsc3vqHwwUMq+9xn5S4vv+T5aG1t6r99v/51qsod/MMf1HDXXYo2NafnAgAsOIRuAAAAzCu2Zcn/3HPqevhh+Z97TrZlpZ7L2bFDhXe/V97b9ihz6xZF6uqGnGNblroeflhd335QWddfr9w3vnHkz4gnUv+d9+dv06rfPKHqXzwuORyygkH1Pf54ei8SwILBmm4AAADMK2Ot2zZMU95du+R/7rnUOf3DtgXr/clPJdtW8OWXdfKaLZLDITsUkiT5nvyt/M9sVsV3Hkx9XubGqyRJ7vJyOQoLlOjoVKw5fZVu/381K/jHNsV7w7JjlhzZLrmrcuW9tVLupdmK90Xkf7ZBkTqfEn1RybLkKMhQ9tYlyrl5mQwHdTVgLuFvJAAAAOaViazbHu2caH297FgseZJty45EZAeDkm0nj1mW7EhEsfPnZebkSJLCR49IkmLNzUp090iS3CtGnrY+HSLn+pToj8lZmCFnUYYS/qhChzvV+a+HZEUTineG1P+HViV6InIWeCTDULwtqL4natX7H+fG/wAAM4pKNwAAAOaV8dZt25alhM+neHe3Ei+/LEdJSeocd1WVXMuWyVlUJMPjSU5F37VLJ6+/QZbPJ1dFhZxFRUr09Mh7+23q+9nP1fvTnyn4yquKd3RIiYQcJcXKf8c70nZ9Re9aL8N1sTbW91Sd/M82ygrGFW8PysxyqeCta5S1pVSG05QVjKntXw4o0R1W8LV2Fbx5ddrGBmDyCN0AAACYVwa2/Bq8F/dggZoahf74igynU3Y8rqxrt6bOGe21hjP5Y7Hl98tYtkwJv1+xxia5KioU7+xUtKFBzuJieW/dpZKPf1zOwsIJj9e2bIVPdiveGZKzOFMZ6wplmMao5xsuU6EjnfLXNMmKxBXvSE59N7NdcpZkyvQ45V6anTrfzHLJtSRLie6wDCcTWYG5htANAACAeWVg3fZoovX1sqNRZV17rSKnTsnh9SpQUzMkaA9sETZgzX/tG3JOpK5OdiSi7G3bFDl1St7bb1PR3XdPabzhk93y72uW4pbkNBVt9MvMdI4ZwBOBmKKN/tRjR2GGit97hUzPpT++xzqCipztlSRlX182pTECSB9CNwAAABaU4dPPE35/qqlaIhiU/5ln5H3964eE70uC/HPPqX8CW49NRLwzJMUtucqyFa7tU/+Bdjlz3NKFqnTGusJLKuE5Ny5V9g1lSvRF1PdErUKHOtX1wxMq/atNQ4J3tNGvzkeOyo5ayryySLm707fWHMDUELoBAACwINiWlaxW19Upc+sWmV6vPCtWpKrWZk6OImfOqD8QUKyxSZJGrZiPN4V9MpzFmZLTVKy1X7JsmQ5TrrJsRc8HFDrapdDRLkUb/TLcpgyXQ5KUuaFIhmHImZ8h765KhQ51Kt4WVPBAh3JuWCpJCh3tUvePT8iOWcq+vkz5b1495rR1ALOD0A0AAIAFYfhWYgNN0lJV69pkZ2/PypWyAoERu54PGG8K+2RkrEuu/453hmSF4grX9inW2i87aina5JcdScgKxuWu9MoKxhU80K6MNQWp9dnhk92p97Kjyf3G/f/VrL7/TF5P3p+skHdHxbSMFcD0I3QDAABgQRi8TVjk1KlUqB6oUvufeUbhQ4eV8PtlZmSM2PV8vLXfU2GYhjI3FF34DFvuC1PJY21BxdqDcuS6Fa33K9YRlMPrUehgp1qOdctRmCE7nFCiL5J8H49DmRuLFKn3qe/X51LHQke6FDrSlfq8ovdcIUeu+7LHDWB6ELoBAACwIAxZy+12K+Hzqevhh1MBOmfHjktC9WDDK+XS6NPPp2pwAA8d71K8OywrnJCjwCN3uVeeNfkKH+1StCmgRHdYdsKWI88jz8o8eXdVyFmQoXh3OPV+diQxpOGaJNlxa1rHDODyELoBAACwIAxeh53w+RT84ytSNDokQI/b9XyESnm6DJ52PriTefam0rFftypf5V/antaxAZg+adnIr66uTu9///tVXV2tzMxMrVq1Sp/73OcUjUaHnHfo0CFt375dGRkZqqio0Fe+8pV0DAcAAACLwMA67KK775YjN1eKRuVZu1Z2JDKhAD286/nldCyf2HiTVW/v9vJk4zSaoAELUloq3SdOnJBlWXrwwQe1evVqHTlyRPfcc4/6+/t1//33S5J8Pp/27Nmj3bt369vf/rYOHz6s973vfcrPz9cHPvCBdAwLAAAAi8RoAXqsddvT2bEcAAakJXTffvvtuv3221OPV65cqZMnT+pb3/pWKnT/4Ac/UDQa1Xe/+1253W5deeWVOnDggP7pn/6J0A0AAIDLMlqAHmvd9nR2LJ8utmVfsoc3FXFgfpmxNd19fX0qLCxMPd6/f79uueUWud0XOyvedttt+vKXv6yenh4VFBSM+D6RSESRSCT12OfzSZJisZhisViaRo/5buDe4B7BQsD9jIWE+xnplPG61ynjda+TJMUTCSmRULC+XvFEQp716xU5c0bB+nplTNP9l477OXyyW/7952XHLdnRhNxH25W5vlCeNQWEb6QV35/HN9GvjWHbtp3msejMmTPaunWr7r//ft1zzz2SpD179qi6uloPPvhg6rxjx47pyiuv1LFjx7Rhw4YR3+vzn/+8vvCFL1xy/Ic//KGysrLScwEAAAAAAAwSDAZ15513qq+vT7m5uaOeN6lK96c+9Sl9+ctfHvOc48ePa/369anHzc3Nuv322/X2t789Fbgvx6c//Wl9/OMfTz32+XyqqKjQnj17xrxQLG6xWEx79+7VG97wBrlcrtkeDnBZuJ+xkHA/Y6bZlqX+F15QtKFB7spKZd9887TsxS2l534eqHRbvWFZwbhcFV7ZkYQyryxSzrZl0/IZwEj4/jy+gVnX45lU6L7vvvt09913j3nOypUrU//d0tKiXbt26aabbtJ3vvOdIeeVlZWpra1tyLGBx2VlZaO+v8fjkefC+pvBXC4XNwPGxX2ChYT7GQsJ9zNmkvvWW9P6/tN5PzuvKJXT4VToaJeijX4ZIUuGy6mMkhz+zmBG8P15dBP9ukwqdJeUlKikpGRC5zY3N2vXrl3aunWrHnroIZnDfoO4bds2feYzn1EsFksNdu/evVq3bt2o67kBAACAxWRgW7GMdYWXNFQDMD+kZZ/u5uZm7dy5U5WVlbr//vvV0dGh1tZWtba2ps6588475Xa79f73v19Hjx7VY489pq997WtDpo4DAAAAYE9vYD5LS/fyvXv36syZMzpz5ozKy8uHPDfQty0vL09PPfWU7r33Xm3dulXFxcX67Gc/y3ZhAAAAAIAFIy2h++677x537bckXX311dq3b186hgAAAAAAwKybsX26AQAAgMmyLfuStcxMrQYwnxC6AQAAMGeFT3bLv69ZiluSM9mOKHND0SyPCgAmLi2N1AAAAIDpEO8MSXFLrrJsKW4lHwPAPELoBgAAwJzlLM6UnKZirf2S00w+BoB5hOnlAAAAmLMG9qNmf2oA8xWhGwAAAHPWwP7UADBfMb0cAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmhG4AAAAAANKE0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaeKc7QEAAICxNYQiuv7F46M+f9+KJfrf1Ut1yB/UP9W16jVfUD2xhHKdDl3lzdRHq5boxvycGRwxAAAYQOgGAGAOsSxLp0+fVnd3twoLC7VmzRp5TFNbcrOGnOeLJ3QmGJEkLXG71BeL6+0HzqovnlC2w9S67AydCUb0XLdfv+8N6JVtV6rYzT/7AADMNP71BQBgDjl9+rT279+vRCIhh8MhSVq3bp2e2Lp2yHl3PfeizihDnlhUJ//9W/pETp76rrlFkvSPa5brrUuL9KPzXfrrE42KWLa++t2H5GxtliS98Y1v1HXXXTezFwYAwCJF6AYAYA7p7u5WIpFQaWmp2tvb1d3dfek5sbhq5JEkXd3eoGJvjvLcLmUm4go5nPrfp5v17aZOnQ5GlGka2hHsSQVuAAAws2ikBgDAHFJYWCiHw6H29nY5HA4VFhZecs7DzZ2KyJDDSmhrR7Nyc3NVmp2pb5VlqyrDrf6EpUOBkEKWpVzZSpw9pSuvvHIWrgYAAFDpBgBgDlmzZo0kDVnTPVjEsvRQc2fy3LZGGb5eNfl6FXc49bXSTp3P9Opzq5bpruVF+tezzfpSc7eevuI6fXBtmY4ePTrj1wMAwGJH6AYAYA4xTVPr1q2TZdt6usun3zV3qjrTo91FuTINQz9t7VFHNC5DtjY3nU2t+z5avEznM72SpFvNmDINQ+79v5Mqr5ZtGPpDf2QWrwoAgMWL0A0AwBz0dJdPDzZ2KGJZSoTDOmGHtKc4T9/qtSVJt3gz9Q/3/qWamprU3d2tnvDF1/6/A0e0s7FBr/T6pcrksUzTlH8WrgMAgMWO0A0AwBxUG4ooYllaGo/ooM+nA4EeNTU362zpSknSx1aXKzs7W+vWrZMkFfeH9eMXjylhmvqXrBL9rD+u1o03SpLc8ZiaHv+J3Bfe+8knn9TBgwf1P//n/5yNSwMAYFGhkRoAAHNQdaZHHtPUqVBUDsvSyuwMPe8tkSRd481S4sgB9fb2ps43W5v13w7s04rO8/LaltpMlzKiEa1qb9KbX3te7mAgdW4ikVAsFpvpSwIAYFGi0g0AwBx0a6FXr/mCeinUL09PUCXdzfqgaaqqqkqZsZD+a/9+Pf3008rLy5Nt2/L5fFoi6U9Pvap77rlHpaWlg97tTerp6dHXvvY1SezTDQDATCJ0AwAwBz3b7ddLff2KeDIVLF2mSEmB1kT71dDQoEQioYyMDOXk5Mjn8ykUCsnhcCgjI0M7d+4cFrgBAMBsInQDADAHDazp3pCTqeOSPKVlymw6q3g8Lo/HI5/Pp8rKSm3atEnHjx9XaWmp2tvbFY/HR3y/goICff7zn5/RawAAAIRuAADmpIE13ccDIXlMU9WZHhUWFioajaq9vV2GYai5uVler1cOh0Pt7e1yOBwqLCyc7aEDAIBBCN0AAMxBu4tyJSUr3gP7dKvQqxMnTigSiai4uFiRSEQZGRnatm2buru7VVhYqDVr1szyyAEAwGCEbgAA5iDTMLSnOG/oQcPQ+vXr1dPTo2g0KqfTqaKiotS2YQAAYO4hdAMAME0sy9KB7oC6IzEVelzaXJgj05ze3TkHKtlUtgEAmB8I3QCwiFm2pZqmGv2u4XeSpJ0VO7WjYodMY3qD4lScfLlJXS0+SVLR8lytu7ZckhQORtV4okN9nUHFInF5Ml1aUpWvZauLZBjGbA5ZB7oDOt7XL8uW2sJRSdKW4txp/QzTNKlsAwAwjxC6AWAesGxL+5r2qd5Xr8rsyml7331N+/T1V7+ulv4WSdLhzsMyDVM7KnZM22dMRVt9bypwDxaLxHWoplbxaEKmw1RmjltBf0T1x9oVDcdVfVXZLIz2ou5ITJYtZTsd6o8n1B2Jpe2zLMvS6dOnh1S8p7uqDgAALh+hGwBm0eAwXZVbpe3l20esMu9r2qdHjz2qSCKibDNbd+iOKb3PcPW+evXH+uVxeCRJ/bF+1fvqp+fiJsi2bfW0BRTujyoj263MHLdqD7fKW5CpSCimaPjiFlidLT7FowlJ0tW3rFBWboba6np09uB5na/t1rLVRfJkumZ0/IMVelxqC0fVH0/INJKPL8dYwfr06dPav3+/EomEHA6HJFEBBwBgDiJ0A8AsGhymB4LvSFXmel+9IomI1hSsUW137ZTfZ7iq3Cplu7LVF+2TJBVlFKkqt+pyLmnSetoCajnbJTthS6YUCydkGNKarct19IVhvwCwB/33wFRy4+JzfR39Kq3Mn4FRj2xzYY4kDVnTfTnGCtbd3d1KJBKp/bm7u7svb/AAACAtCN0AMIsGh+nTPadHrDJbtiVf1KfucLdeaX1FZRmXTqGeyPuMZHv5dlmyhqzp3l6+/bKuabLC/VHZCVtZuR71tAUUDce1ZstyZWS7Lzm3YEmO6o+ZshKWDj9fK0+WSyF/JPX84Kr4bDBNc8w13Ed7AjrtC6o/nlDctpXhMFWa4damQm+qKt4dielAl18d4ahCRo6sTTcq198t3+njQ4J1YWEh+3MDADAPELoBYBZV5VbJ4/DodM9peRyeEavM+5r26dX2V+U0nIrbcW0q3SQ1Tf59RmIapnZV7NKuil3TcTlTkpHtluEw5O8OKRqOK7coSyUVeaOee+VNlao/3q7+3rCi4bhKKvPVXt8rSZoD/d/G1BqKKpywlONyKmHb8kXjqguEdT4Y0durl6g/ltB/NnYqbtvymIYyZSmYk6tATq7MUHhIsKaLOQAA8wOhGwBm0UBVefBa7OHqffWKJqLaWrZVp3tOy+v2jvk+Fd4KWbL06NFHJ7W+e7YULElOwe5o6lPQH5G/J6QXf31ckmQlkvPJu1t8evHXx3XtbWvlLczSxptXpF7v7w6mQndmjmdGxz5ZO8oK5DQvdlh/tcung90BRSxbfdG4agMhxW1bDkN624olchnS02fq1Wx45Khep+qVF2c50MUcAID5gdANALNoIp3Ch1exK72V6lPfqO9T01gzpfXds8UwDBWWeRWPJtTV7JNt2UOWbkuSbSu55luSrysob2GmDMNQPJpQ3dE2SZLT7VBecfYMj35ynKah+kBIh3sCilrJSrckZThM5bmdsodduGmaKi4qUnN3QAnDUHcsrjKnYxZGDgAAporQDQBz3PBq+I1LbtSTB58c9fypru9Op+EdyguW5MgwjEuOb/tvG4bstf3KU6cVCcWG7NN99uB5RcMxeTJdCvdHk9VwQ1q1aakczpmv6Nu2rab+iPpiceW5nCrP9oy5X3gobqkjfHErsRynQ7uXFcplmqrKydSx3n4lbOnnde3KcprqiV5cpx6MJ9J6LQAAYPoRugFgjhteDY/FkoHNsi3VNNZcsk3YVNd3p9PgDuWGIxlIC8u8ox4fS35ptrpa/AoFojJNQ/klWVq+tnjWqtxN/REd6Q0oYdlyXJg6XpGTMer56/OztS4vS/3xhP7Y6VNtIKzftfboTRXFWpLp1uuXFepgt189kbgiCUurvZk64w9JkkyNHuYBAMDcROgGgHlqf8t+fe/k9y6ZRn7z8pt1uPOwTnWf0trCtbp5+c2zPNKhHcqDvojC/dERj4cCEXW3KlX53vKG1ZdUjas3lql646Ud3GdLXyyuhGWrwONSTySmvlhcFeO8xjAMZTsdKs1wqzYQVm80rrO+kNbnZ6siO0MV2RdD+zl/MBW6c938sw0AwHzDv94AMMdZtqV9TfuGTC+XpAZ/w4jTyF9ofkGvtb+mSCKi19pf0wvNL8z6mu6BDuVBX0SGw0htBzb8eDyaUMOxdsXjlpxOU7JtFS4dfQuuuSDP5ZTDNNQTiclhGspzDf2ndWD6eVckqmjC1pZir5ymqab+iI729qfO67ww5bw1GFFZVvKXKIFYQq91+SVJ+W6nCgjdAADMO/zrDQBz3L6mfUMaow00FKv0Vo44jfxy13QPD/nT0f18oEP54DXdIx3vbvEpHEqG13Asoe5W/5wN3QNhujca05IMt9ymoTy3S+XZQzuoD0w/jyYsdUfjOt7Xr1y3U8F4QlEr+WdpSMq8sB59b0u3nKahDIcpfyyuhC05DUM3l+aPuVYcAADMTYRuAJjjLgnR/nrlK1/1vnpdU3qNvG6vVuSuSDVcu9w13cNDvnT53c8HOpSPd7z7vD+1atkY9P/nouFruTfm54y4lnvw9PP+C43Q/LGELNuWaUguw5DX5VRpRrL6X5GdodZQRL5oXC7TUHmWR5uLvCr0uGb0+gAAwPQgdAPAHDc8RAeiAeUrX881PieH06G7rrhrSCieyN7fY5nN7ucFZTny94aUiCXkznCqoCxnxj57sia6lntg+rkvGleBx5UK5yN1PZeknUsLZvZCAABAWhG6AWCOGx6i63rqJEmr8lfpZN/JS0LxRPb+Hstsdj8vLPPKMIxLpqHPReOt5R4wEKaHh2vDMFSRkzFu0zUAADC/EboBYI4bHqLthK0+9els71l5nNMbii3bkmVbKs9J7om9s3LnpCvll2O0aehz0WhhejjCNQAAixuhGwDmmW3LtunJg0/q1spbVZU/+enjY9nXtE/fP/791HpuU+ZlN1FbqAjTAABgIgjdADDPDITgd61/l1yu6W2uNZvruReikdZt04EcAIDFhdANAEiZzfXcC8lA2K7vD6kzHJPTMOQwk2F7eIdzgjkAAAsboRsAkHK5nc+RNLCdmD8aV9SyVZrpVjRhXdLh3LZtvdbl1zl/SKYhZTodki4N5gAAYP4idAMAUi638zmSBrYTy3O71BGOqjcak9flvKTDeVN/ROf8QYUStlymIcUTo249BgAA5idCNwAA02xgO7FoIqFsl0MlHpeqcjIv6XDeF4vLkCGXKcUsW05j9K3HAADA/MS/7AAATLORthMbaZ12nsupTKcpxS05HdJK76XBHAAAzG+EbgAAptlEtxObaDgHAADzF6EbAIBZwl7fAAAsfOZsDwAAAAAAgIWK0A0AAAAAQJoQugEAAAAASBNCNwAAAAAAaULoBgAAAAAgTQjdAAAAAACkCaEbAAAAAIA0IXQDAAAAAJAmztkeAABgYbFsS/ua9qneV6+q3CptL98u0+B3vAAAYHEidAMAptW+pn169NijiiQi8jg8kqQdFTtmeVQAAACzg9IDAGBa1fvqFUlEtKZgjSKJiOp99bM9JAAAgFlD6AYATKuq3Cp5HB6d7jktj8Ojqtyq2R4SAADArGF6OQBgWm0v3y5JQ9Z0AwAALFaEbgDAtDINkzXcAAAAFzC9HAAAAACANCF0AwAAAACQJkwvBwCkHXt3AwCAxYrQDQCL2GTD8FTDM3t3AwCAxYrQDQCL2GTD8FTD8+C9u0/3nGbvbgAAsGgwtw8AFrHBYTiSiIwbhid7/gD27gYAAIsVoRsA5hHLtvRC8wuSpBeaX5BlW5f1fpMNw1MJz5ZtyZKl8pxyrcpbpf+x4X+wdzcAAFg0mF4OAPPIvqZ9+tGJH+kO3aEfnfiRDIdxWWujB8Lv4DXa03n+wJi/f+z7qSnppmHSRA0AACwahG4AmEcGpndLmtT07tGYhjmp0D7Z8yXWcwMAgMWN0A0A88jA9G7Flfa10dO1zRfruQEAwGJG6AaAeWR7+XbZCVt9B/v0rvXvmvLa6PEC9X2/u09P1T8lScr35GttwVpJU9vmaypT0gEAABYKQjcAzCOmYerm5TfriYNP6OblN095bfRYW389fvrxVOCWpBx3zpCp7JOtgE9lSjoAAMBCQegGgEVotHXWjb5GfemlL2lTySbV++rVG+lVIBrQsuxlqWnhU92rGwAAYDGifSwALDCWbammsUaPHn1UNY01I24rNtI667gV16f2fUqmYepL27+kLGeWJKksq0x3XXHXkGniU9mrGwAAYDGi0g0AC8xEKtEjrbP+xoFv6FDnIX1x+xdV7i1Pnbsib8WQ19MYDQAAYOII3QAwR0x2rfRARXv4+RPZomv4OuujnUf174f/XW9a+Sa9aeWbxhznRBujTVf3cwAAgPmM0A0Ac8Rk10rvb9mv75383iXnT6USfbr3tBJ2Qnvr9+qZhmckSeF4WJL0dP3Tuv4H1+uZtz8jr9s74cZorP0GAAAgdAPAnDGRCvVgDf6GEc+fyhZdtm1LkiKJyCXPxe244vG4/qvpv9QR6phwFf7ZhmfV5G/SirwVCkQDI14P1XAAALDQEboBYI6YbIW60ls54vlT2aKrMKNQ15ddn6pK33XFXfo/f/g/aulv0e0rbtebVr5pUlXrfU37dLjzsPqifTrYcXBI9/Ph51ENBwAACxmhGwDmiMlWqLct2ybDYUyqoj2a8arsg58/1X1KzzY8O2Z1ut5XryxXljaXbFZtX62uKr5qxPFNtroPAAAw3xC6AWCOmGyFeioV7cEGT+32RX1ym+4hVfPf/vlvU+fWNNakquqheEiHOw/rbN/ZUavTA1X7QCygcm+5bq28dcRp43RCBwAACx2hGwDmsJHWPE+XwVO73Q63tizZolx37oifM7gKf7b3rM72nR2zOj3Rqv1U1p8DAADMJ4RuAJjDRlrzfFPZTdPy3sOndue6c3XXlXeNeO7gqnpNY42ajjWNWZ2eaBX+cqv1AAAAcx2hGwDmsOHBuM5XJzuR7DT+QvMLuqXqlil3+57q1G6q0wAAABNH6AaAOWx4MPZH/fpR4490h+7Qj078SIbDmHKleKrhmeo0AADAxBG6AWAOGx6M6/rqUntpRxKRcbt9j7UP9mjhebTXsKc2AADA5BG6AWCGTSa8moaZDN5NyeDtj/nldriluCY0JXwq+2CP9pqpvBdBHQAALHaEbgCYYZMNr0O6jJtubS3ZKjVJ71r/rnGnhE9lH+zRXjOV95pKUAcAAFhIKDcAwAwbHF4nMkV88PlRKyqv2ytJunn5zeNWjafSLG2010zlvSZ7rQMs21JNY40ePfqoahprZNnWhF4HAAAw11DpBoAZNtnwOvz8Sm+l+tQ35JzRpnFPpVnaaK8Z6fh408en2iGdCjkAAFgoCN0AMMMmG4SHn3/jkhv15MEnh5wzWkidSqfx0V4z0vGaxpoxw/FUO6RPZSo7AADAXEToBoAZNtkgPPz8WCx2yTnjhdSpNjQb73Xjfe5UtxebaoUcAABgriF0A8ACMF5Inep07fFel65wPNUKOQAAwFxD6AaAWTYd22qNF1KnOl17vNelKxxPtUIOAAAw1xC6AWCWTUfTsPFC6lQr0uO9jnAMAAAwNkI3AMyymWgaNtWKNNO8AQAALg+hGwBm2Uw0DRtckZ7MdPZ0VLKnYzo9AADAfEHoBoBZdrnV5MmG2NneA3u2Px8AAGAmEboBYJZdbjV5siF2tOnsM1WBZg9uAACwmKR9Pl8kEtHmzZtlGIYOHDgw5LlDhw5p+/btysjIUEVFhb7yla+kezgAsOAMDrGRRGTcEDvadPaB8P5U/VN69Nij2te0b1LjsGxLNY01evToo6pprJFlW5P6fAAAgIUo7ZXuT3ziE1q2bJkOHjw45LjP59OePXu0e/duffvb39bhw4f1vve9T/n5+frABz6Q7mEBwLyTqkT31itPealQO9kQO9p09qlUoAdXx31Rn15te1VRKzpmxZ3mbAAAYDFJa+j+zW9+o6eeeko///nP9Zvf/GbIcz/4wQ8UjUb13e9+V263W1deeaUOHDigf/qnfyJ0A8AIBirRiXhCb9FbtL9lv3au2DnpEDswnX0gMH//2PdVlVulityKSVegB09t7w53y2k4tbVs65ihnW3GAADAYpK20N3W1qZ77rlHv/jFL5SVlXXJ8/v379ctt9wit9udOnbbbbfpy1/+snp6elRQUDDi+0YiEUUikdRjn88nSYrFYorFYtN8FVgoBu4N7hHMZ/W99UrEE1qTu0bqlhr6GlL39E1lN+mmspskSYl4QgklJCUr0ftb9qvB36BKb6W2LduWWqf9QvML+tGJH6XWgr9z3Tv1nnXvSZ1745Ibx/07MzCmdfnrdCByQHE7rtruWmU7slWZXcnfOYyL789YSLifsZBwP49vol8bw7Zte7o/3LZt3XHHHbr55pv1t3/7t6qrq1N1dbVee+01bd68WZK0Z88eVVdX68EHH0y97tixY7ryyit17NgxbdiwYcT3/vznP68vfOELlxz/4Q9/OGK4BwAAAABgugWDQd15553q6+tTbm7uqOdNqtL9qU99Sl/+8pfHPOf48eN66qmn5Pf79elPf3oybz8hn/70p/Xxj3889djn86miokJ79uwZ80KxuMViMe3du1dveMMb5HK5Zns4wJSkqtZ9Dcqry9Pu3buHzBYayY9O/EjPNjyrVfmrdLb3rG6tvFXvWv8uSZdWut+1/l26efnNUxvTCJX06XwNFi6+P2Mh4X7GQsL9PL6BWdfjmVTovu+++3T33XePec7KlSv17LPPav/+/fJ4PEOeu/baa/Xud79bjzzyiMrKytTW1jbk+YHHZWVlo76/x+O55H0lyeVycTNgXNwnmMsmsmXXzhU7FYvF9ETdE3K73ePez1X5VXK0OHSy76Q8To+q8qtSr7ml6hYZDuOytwjbuWJnauyPnX5s3PeqaazR905+LxX2DYfBGm/w/RkLCvczFhLu59FN9OsyqdBdUlKikpKScc/753/+Z/393/996nFLS4tuu+02PfbYY7rhhhskSdu2bdNnPvMZxWKx1GD37t2rdevWjbqeGwAWssnutz0RYzVZG9zQbHDgr/BWSIbU6GuccBifzNjZpxsAACwmaWmkVllZOeRxTk6OJGnVqlUqLy+XJN155536whe+oPe///365Cc/qSNHjuhrX/uaHnjggXQMCQDmnOGV7bq+umkPoxPtFD44NAdjQUlSlitrwuF/MkGafboBAMBikvZ9ukeTl5enp556Svfee6+2bt2q4uJiffazn2W7MACLxvDq8DWl18xaGB0cmn/f/HtJ0qbSTRMO/5MJ0uzTDQAAFpMZCd0rVqzQSE3Sr776au3bt28mhgAAc87w6rDX7dVdV9w1K2F0cGjOdmVL0qTC/2SCNPt0AwCAxWTWKt0AsNgNrw6vyF0x7WF0Is3ZpKGheaQ13eMhSAMAAIyM0A0As2QmpllPtMHZRELzRAM8AAAALiJ0A8Asmc7q8GiBeDo7haejuzoAAMBCR+gGgAVgtEA8nZ3CLzfAUykHAACLEaEbAOYpy7ZU01ijel+9zvaeHTEQT2QK+0TD8OUGeCrlAABgMSJ0A8A8tb9lv7538ntD9tYeHognMoV9omF4SLO13ApZtqVHjz464ar1dE51BwAAmC8I3QAwTzX4G1Ih9lT3Ka3OX61V+asm3ZRtomF4cICvaazRo8cnV7WezqnuAAAA8wWhGwDmqUpvZSrEZjgzdGvlrVOarj2VMDyVqvVMdGsHAACYawjdADBPbVu2TYbDuOwQO5UwPJWgzl7eAABgMSJ0A8A8NV0hdirvQ9UaAABgYgjdAIAxjdbdnKo1AADA+AjdAIAxsdUXAADA1I29vwsAYNEb3DQtkoiw1RcAAMAkUOkGgEVotCnjI2GrLwAAgKkjdAPAIjSZKeM0TQMAAJg6QjcALEKT2WebpmkAAABTx5puAFiEmDIOAAAwM6h0A8AixJRxAACAmUHoBoBFiCnjAAAAM4Pp5QAAAAAApAmhGwAAAACANCF0AwAAAACQJoRuAAAAAADShNANAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE0I3QAAAAAApAmhGwAAAACANCF0AwAAAACQJoRuAAAAAADShNANAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE0I3QAAAAAApAmhGwAAAACANCF0AwAAAACQJoRuAAAAAADShNANAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE0I3QAAAAAApAmhGwAAAACANCF0AwAAAACQJoRuAAAAAADSxDnbA7hctm1Lknw+3yyPBHNZLBZTMBiUz+eTy+Wa7eEAl4X7GQsJ9zMWEu5nLCTcz+MbyKADmXQ08z50+/1+SVJFRcUsjwQAAAAAsNj4/X7l5eWN+rxhjxfL5zjLstTS0iKv1yvDMGZ7OJijfD6fKioq1NjYqNzc3NkeDnBZuJ+xkHA/YyHhfsZCwv08Ptu25ff7tWzZMpnm6Cu3532l2zRNlZeXz/YwME/k5ubyTQMLBvczFhLuZywk3M9YSLifxzZWhXsAjdQAAAAAAEgTQjcAAAAAAGlC6Mai4PF49LnPfU4ej2e2hwJcNu5nLCTcz1hIuJ+xkHA/T59530gNAAAAAIC5iko3AAAAAABpQugGAAAAACBNCN0AAAAAAKQJoRsAAAAAgDQhdGNRiEQi2rx5swzD0IEDB4Y8d+jQIW3fvl0ZGRmqqKjQV77yldkZJDCGuro6vf/971d1dbUyMzO1atUqfe5zn1M0Gh1yHvcz5pNvfOMbWrFihTIyMnTDDTfopZdemu0hAeP64he/qOuuu05er1elpaV685vfrJMnTw45JxwO695771VRUZFycnL0tre9TW1tbbM0YmBivvSlL8kwDH3sYx9LHeNenh6EbiwKn/jEJ7Rs2bJLjvt8Pu3Zs0dVVVV65ZVX9I//+I/6/Oc/r+985zuzMEpgdCdOnJBlWXrwwQd19OhRPfDAA/r2t7+tv/mbv0mdw/2M+eSxxx7Txz/+cX3uc5/Tq6++qk2bNum2225Te3v7bA8NGFNNTY3uvfdevfjii9q7d69isZj27Nmj/v7+1Dl//dd/rf/4j//QT3/6U9XU1KilpUVvfetbZ3HUwNhefvllPfjgg7r66quHHOdeniY2sMA98cQT9vr16+2jR4/akuzXXnst9dw3v/lNu6CgwI5EIqljn/zkJ+1169bNwkiByfnKV75iV1dXpx5zP2M+uf766+1777039TiRSNjLli2zv/jFL87iqIDJa29vtyXZNTU1tm3bdm9vr+1yueyf/vSnqXOOHz9uS7L3798/W8MERuX3++01a9bYe/futXfs2GF/9KMftW2be3k6UenGgtbW1qZ77rlH3/ve95SVlXXJ8/v379ctt9wit9udOnbbbbfp5MmT6unpmcmhApPW19enwsLC1GPuZ8wX0WhUr7zyinbv3p06Zpqmdu/erf3798/iyIDJ6+vrk6TU9+NXXnlFsVhsyP29fv16VVZWcn9jTrr33nv1xje+ccg9K3EvTydCNxYs27Z199136y//8i917bXXjnhOa2urlixZMuTYwOPW1ta0jxGYqjNnzujrX/+6PvjBD6aOcT9jvujs7FQikRjxfuVexXxiWZY+9rGP6eabb9bGjRslJb/fut1u5efnDzmX+xtz0Y9//GO9+uqr+uIXv3jJc9zL04fQjXnnU5/6lAzDGPP/Tpw4oa9//evy+/369Kc/PdtDBkY10ft5sObmZt1+++16+9vfrnvuuWeWRg4AuPfee3XkyBH9+Mc/nu2hAJPW2Nioj370o/rBD36gjIyM2R7Oguac7QEAk3Xffffp7rvvHvOclStX6tlnn9X+/fvl8XiGPHfttdfq3e9+tx555BGVlZVd0oFx4HFZWdm0jhsYyUTv5wEtLS3atWuXbrrppksapHE/Y74oLi6Ww+EY8X7lXsV88eEPf1i//vWv9fzzz6u8vDx1vKysTNFoVL29vUMqhNzfmGteeeUVtbe3a8uWLaljiURCzz//vP7lX/5Fv/3tb7mXpwmhG/NOSUmJSkpKxj3vn//5n/X3f//3qcctLS267bbb9Nhjj+mGG26QJG3btk2f+cxnFIvF5HK5JEl79+7VunXrVFBQkJ4LAAaZ6P0sJSvcu3bt0tatW/XQQw/JNIdOVuJ+xnzhdru1detWPfPMM3rzm98sKTlN95lnntGHP/zh2R0cMA7btvWRj3xEjz/+uH73u9+purp6yPNbt26Vy+XSM888o7e97W2SpJMnT6qhoUHbtm2bjSEDI3r961+vw4cPDzn2F3/xF1q/fr0++clPqqKignt5mhi2bduzPQhgJtTV1am6ulqvvfaaNm/eLCnZ/GTdunXas2ePPvnJT+rIkSN63/vepwceeEAf+MAHZnfAwCDNzc3auXOnqqqq9Mgjj8jhcKSeG/htM/cz5pPHHntM733ve/Xggw/q+uuv11e/+lX95Cc/0YkTJy5Z6w3MJX/1V3+lH/7wh/rlL3+pdevWpY7n5eUpMzNTkvShD31ITzzxhB5++GHl5ubqIx/5iCTp97///ayMGZionTt3avPmzfrqV78qiXt5ulDpxqKWl5enp556Svfee6+2bt2q4uJiffaznyWgYM7Zu3evzpw5ozNnzgyZxiglqy4S9zPml3e+853q6OjQZz/7WbW2tmrz5s168sknCdyY8771rW9JSoaTwR566KHUcqEHHnhApmnqbW97myKRiG677TZ985vfnOGRApePe3l6UOkGAAAAACBN6F4OAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE0I3QAAAAAApAmhGwAAAACANCF0AwAAAACQJoRuAAAAAADShNANAAAAAECaELoBAAAAAEgTQjcAAAAAAGlC6AYAAAAAIE3+Pw6zkgFzHY8XAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_tsne(model, val_loader, classnames)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
