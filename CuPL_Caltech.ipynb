{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeqbcP9keET0/ukM9lzm22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CuPL_Caltech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT2a2N2Yof_",
        "outputId": "a6ebdaa9-8bb4-45eb-faca-44327c080594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks that a private key is already created. If you have already push it to github, no action required.\n",
            " Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGjqRi4rdAabjvEtcfoE2oUILModQFbTBLvzjz/YT04D root@735fd2533285\n",
            "\n",
            "Please use SSH method to clone repo.\n",
            "Cloning into 'applied-ml'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 88 (delta 46), reused 18 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (88/88), 1.92 MiB | 4.94 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/applied-ml\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=d140035c6873426035d396a1b46f08fa7e1938c98c36e671926476058270851a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Change the path if necessary\n",
        "datasets_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "  if not os.path.exists('/content/applied-ml/'):\n",
        "    !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "    import colab_github\n",
        "    colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "    !git clone git@github.com:oz-e/applied-ml.git\n",
        "    if not os.path.exists('/content/applied-ml/'):\n",
        "      raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "  %cd /content/applied-ml/\n",
        "\n",
        "  # Install any other requirements (to be converted to requirements.txt)\n",
        "  !pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"CuPL\"):\n",
        "  !git clone https://github.com/sarahpratt/CuPL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNDfTcucYyJ1",
        "outputId": "855f6905-f6c6-4fd7-f78f-1a0f37ac9fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CuPL'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 56 (delta 6), reused 52 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (56/56), 4.68 MiB | 5.35 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from pkg_resources import packaging\n",
        "from CuPL.imagenet_prompts.standard_image_prompts import imagenet_templates\n",
        "import pdb\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jtmnqOzCbmfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/16\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOZZLgVbq7k",
        "outputId": "73f7b811-e3a6-4049-8e40-5dcacf746993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:05<00:00, 62.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aml.datasets\n",
        "\n",
        "all_images = aml.datasets.Caltech101(datasets_path, split='test', transform=preprocess)\n",
        "loader = torch.utils.data.DataLoader(all_images, batch_size=512, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWp11SF3bQwg",
        "outputId": "da2e2d6a-479e-4fc4-be51-8ab7a97d5386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp&confirm=t&uuid=924ebefe-3d77-4acd-a9c8-46a14ea47098\n",
            "To: /content/applied-ml/datasets/caltech101/101_ObjectCategories.tar.gz\n",
            "100%|██████████| 132M/132M [00:00<00:00, 135MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m&confirm=t&uuid=c7821163-292e-469b-94c3-3c66de07d593\n",
            "To: /content/applied-ml/datasets/caltech101/Annotations.tar\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 60.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/applied-ml/datasets/caltech101/split.json\n",
            "100%|██████████| 809k/809k [00:00<00:00, 8.94MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PROMPTS = './CuPL/all_prompts/full_prompts/cal_prompts_full.json'\n",
        "\n",
        "with open(PATH_TO_PROMPTS) as f:\n",
        "  gpt3_prompts = json.load(f)"
      ],
      "metadata": {
        "id": "-EC3XKq2Zis4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_class_mapping = {\n",
        "    'centered face': 'face',\n",
        "    'side of a car': 'car_side',\n",
        "    'body of a cougar cat': 'cougar_body',\n",
        "    'face of a cougar cat': 'cougar_face',\n",
        "    'head of a crocodile': 'crocodile_head',\n",
        "    'head of a flamingo': 'flamingo_head',\n",
        "    'snoopy (cartoon beagle)': 'snoopy',\n",
        "    'yin and yang symbol': 'yin_yang',\n",
        "}"
      ],
      "metadata": {
        "id": "r6KLeBmtarfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For bridging classsname in dataset with name in CuLP prompts\n",
        "new_gpt3_prompts = {}\n",
        "\n",
        "for key, value in gpt3_prompts.items():\n",
        "  new_key = special_class_mapping.get(key, key)\n",
        "  new_gpt3_prompts[new_key.replace(' ', '_').lower()] = value"
      ],
      "metadata": {
        "id": "1S89k8iXjsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier(classnames, templates):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\ttexts = [template.format(classname) for template in templates] #format with class\n",
        "\t\t\ttexts = clip.tokenize(texts).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "9FsIz2qjbwIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier_gpt(classnames, templates, use_both):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\tif use_both:\n",
        "\t\t\t\ttexts = [template.format(classname) for template in templates]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttexts = []\n",
        "\n",
        "\t\t\tfor t in new_gpt3_prompts[classname]:\n",
        "\t\t\t\ttexts.append(t)\n",
        "\t\t\ttexts = clip.tokenize(texts, truncate=True).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "hUQlruDEbwy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating standard text embeddings...\")\n",
        "zeroshot_weights_base = zeroshot_classifier(all_images.classnames, imagenet_templates)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating CuPL text embeddings...\")\n",
        "zeroshot_weights_cupl = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, False)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating combined text embeddings...\")\n",
        "zeroshot_weights_gpt_both = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, True)\n",
        "print(\"Done.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uct62dTFbyz4",
        "outputId": "25d70f61-ac95-4c2c-f57f-3e6325084f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating standard text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 18.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating CuPL text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 47.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating combined text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 14.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0.\n",
        "correct_base = 0.\n",
        "correct_cupl = 0.\n",
        "correct_both = 0.\n",
        "\n",
        "print(\"Classifying ImageNet...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "\tfor i, (images, target) in enumerate(tqdm(loader)):\n",
        "\t\timages = images.cuda()\n",
        "\t\ttarget = target.cuda()\n",
        "\n",
        "\t\t# predict\n",
        "\t\timage_features = model.encode_image(images)\n",
        "\t\timage_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\t\tlogits_base = image_features @ zeroshot_weights_base\n",
        "\t\tlogits_cupl = image_features @ zeroshot_weights_cupl\n",
        "\t\tlogits_both = image_features @ zeroshot_weights_gpt_both\n",
        "\n",
        "\t\tpred_base = torch.argmax(logits_base, dim =1)\n",
        "\t\tpred_cupl = torch.argmax(logits_cupl, dim =1)\n",
        "\t\tpred_both = torch.argmax(logits_both, dim =1)\n",
        "\n",
        "\t\tfor j in range(len(target)):\n",
        "\t\t\ttotal += 1.\n",
        "\t\t\tif pred_base[j] == target[j]:\n",
        "\t\t\t\tcorrect_base += 1.\n",
        "\t\t\tif pred_cupl[j] == target[j]:\n",
        "\t\t\t\tcorrect_cupl += 1.\n",
        "\t\t\tif pred_both[j] == target[j]:\n",
        "\t\t\t\tcorrect_both += 1."
      ],
      "metadata": {
        "id": "q6odbVXGdAts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21554462-cecd-40e0-d14f-64456b1e5044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying ImageNet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:17<00:00,  3.45s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "top1 = (correct_base / total) * 100\n",
        "print(f\"Top-1 accuracy standard: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_cupl / total) * 100\n",
        "print(f\"Top-1 accuracy CuPL: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_both / total) * 100\n",
        "print(f\"Top-1 accuracy both: {top1:.2f}\")"
      ],
      "metadata": {
        "id": "YjXwPA6ZdFZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a7792f-073d-4be7-b11f-a61607a274bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 accuracy standard: 93.06\n",
            "Top-1 accuracy CuPL: 94.20\n",
            "Top-1 accuracy both: 93.67\n"
          ]
        }
      ]
    }
  ]
}