{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLk0yJupMwrf9NMbA/QrJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CuPL_Caltech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT2a2N2Yof_",
        "outputId": "5ea18b6f-b10b-430e-ea06-03174bf62771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks that a private key is already created. If you have already push it to github, no action required.\n",
            " Otherwise, Please go to https://github.com/settings/ssh/new to upload the following key: \n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICzD/eO7jX02ocY1VItXcK8kDuwwNBFQVNS3zvD0P3hI root@1cf5d1d28fee\n",
            "\n",
            "Please use SSH method to clone repo.\n",
            "Cloning into 'applied-ml'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 85 (delta 44), reused 18 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (85/85), 1.92 MiB | 4.80 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "/content/applied-ml\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=25971910859f330c635e4cb83231530c57bcd506352e315b2af392d040396b35\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Change the path if necessary\n",
        "datasets_path = 'datasets'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Deploy AML code in colab\n",
        "if 'google.colab' in sys.modules:\n",
        "  !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "  import colab_github\n",
        "  colab_github.github_auth(persistent_key=False)\n",
        "\n",
        "  # Remember to follow the instructions to add the SSH key to your account in order to clone private repo\n",
        "  if os.path.exists('/content/applied-ml/'):\n",
        "    %cd /content/applied-ml/\n",
        "  else:\n",
        "    !git clone git@github.com:oz-e/applied-ml.git\n",
        "    if not os.path.exists('/content/applied-ml/'):\n",
        "      raise Exception('Please follow the instructions to add the SSH key to your account in order to clone private repo')\n",
        "\n",
        "  # Install any other requirements (to be converted to requirements.txt)\n",
        "  !pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"CuPL\"):\n",
        "  !git clone https://github.com/sarahpratt/CuPL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNDfTcucYyJ1",
        "outputId": "3d285a46-7f87-48a0-984e-8ba35b07d132"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CuPL'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 56 (delta 6), reused 52 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (56/56), 4.68 MiB | 9.02 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from pkg_resources import packaging\n",
        "from CuPL.imagenet_prompts.standard_image_prompts import imagenet_templates\n",
        "import pdb\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jtmnqOzCbmfr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/16\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOZZLgVbq7k",
        "outputId": "e99fcdb1-5f36-4ce6-95e1-b428d51473c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [01:46<00:00, 3.31MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PROMPTS = './CuPL/all_prompts/full_prompts/cal_prompts_full.json'"
      ],
      "metadata": {
        "id": "-EC3XKq2Zis4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aml.datasets\n",
        "\n",
        "all_images = aml.datasets.Caltech101(datasets_path, split='test', transform=preprocess)\n",
        "loader = torch.utils.data.DataLoader(all_images, batch_size=512, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWp11SF3bQwg",
        "outputId": "875d6860-bfd6-414e-a4fd-c3cfebcfcdea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp&confirm=t&uuid=51012cb1-e8bf-4947-aa81-d3196a64325e\n",
            "To: /content/applied-ml/datasets/caltech101/101_ObjectCategories.tar.gz\n",
            "100%|██████████| 132M/132M [00:01<00:00, 87.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m\n",
            "To: /content/applied-ml/datasets/caltech101/Annotations.tar\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 29.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/applied-ml/datasets/caltech101/split.json\n",
            "100%|██████████| 809k/809k [00:00<00:00, 130MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_class_mapping = {\n",
        "    'face'          : 'centered face',\n",
        "    'car_side'      : 'side of a car',\n",
        "    'cougar_body'   : 'body of a cougar cat',\n",
        "    'cougar_face'   : 'face of a cougar cat',\n",
        "    'crocodile_head': 'head of a crocodile',\n",
        "    'flamingo_head' : 'head of a flamingo',\n",
        "    'snoopy'        : 'snoopy (cartoon beagle)',\n",
        "    'yin_yang'      : 'yin and yang symbol',\n",
        "}"
      ],
      "metadata": {
        "id": "r6KLeBmtarfp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier(classnames, templates):\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\ttexts = [template.format(classname) for template in templates] #format with class\n",
        "\t\t\ttexts = clip.tokenize(texts).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "9FsIz2qjbwIa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier_gpt(classnames, templates, use_both):\n",
        "\twith open(PATH_TO_PROMPTS) as f:\n",
        "\t\tgpt3_prompts = json.load(f)\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tzeroshot_weights = []\n",
        "\t\tfor classname in tqdm(classnames):\n",
        "\t\t\tif use_both:\n",
        "\t\t\t\ttexts = [template.format(classname) for template in templates]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttexts = []\n",
        "\n",
        "\t\t\t# Bridge classsname in dataset with name in CuLP prompts\n",
        "\t\t\tnew_classname = classname\n",
        "\t\t\tif classname in special_class_mapping:\n",
        "\t\t\t\tnew_classname = special_class_mapping[classname]\n",
        "\t\t\tnew_classname = new_classname.replace('_', ' ')\n",
        "\n",
        "\t\t\tfor t in gpt3_prompts[new_classname]:\n",
        "\t\t\t\ttexts.append(t)\n",
        "\t\t\ttexts = clip.tokenize(texts, truncate=True).cuda() #tokenize\n",
        "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\t\t\tclass_embedding = class_embeddings.mean(dim=0)\n",
        "\t\t\tclass_embedding /= class_embedding.norm()\n",
        "\t\t\tzeroshot_weights.append(class_embedding)\n",
        "\n",
        "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "\treturn zeroshot_weights"
      ],
      "metadata": {
        "id": "hUQlruDEbwy1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating standard text embeddings...\")\n",
        "zeroshot_weights_base = zeroshot_classifier(all_images.classnames, imagenet_templates)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating CuPL text embeddings...\")\n",
        "zeroshot_weights_cupl = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, False)\n",
        "print(\"Done.\\n\")\n",
        "\n",
        "print(\"Creating combined text embeddings...\")\n",
        "zeroshot_weights_gpt_both = zeroshot_classifier_gpt(all_images.classnames, imagenet_templates, True)\n",
        "print(\"Done.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uct62dTFbyz4",
        "outputId": "1dcb6dec-8ad8-4fb9-c177-6fc4e295cbfe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating standard text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 17.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating CuPL text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 47.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n",
            "Creating combined text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 14.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0.\n",
        "correct_base = 0.\n",
        "correct_cupl = 0.\n",
        "correct_both = 0.\n",
        "\n",
        "print(\"Classifying ImageNet...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "\tfor i, (images, target) in enumerate(tqdm(loader)):\n",
        "\t\timages = images.cuda()\n",
        "\t\ttarget = target.cuda()\n",
        "\n",
        "\t\t# predict\n",
        "\t\timage_features = model.encode_image(images)\n",
        "\t\timage_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\t\tlogits_base = image_features @ zeroshot_weights_base\n",
        "\t\tlogits_cupl = image_features @ zeroshot_weights_cupl\n",
        "\t\tlogits_both = image_features @ zeroshot_weights_gpt_both\n",
        "\n",
        "\t\tpred_base = torch.argmax(logits_base, dim =1)\n",
        "\t\tpred_cupl = torch.argmax(logits_cupl, dim =1)\n",
        "\t\tpred_both = torch.argmax(logits_both, dim =1)\n",
        "\n",
        "\t\tfor j in range(len(target)):\n",
        "\t\t\ttotal += 1.\n",
        "\t\t\tif pred_base[j] == target[j]:\n",
        "\t\t\t\tcorrect_base += 1.\n",
        "\t\t\tif pred_cupl[j] == target[j]:\n",
        "\t\t\t\tcorrect_cupl += 1.\n",
        "\t\t\tif pred_both[j] == target[j]:\n",
        "\t\t\t\tcorrect_both += 1."
      ],
      "metadata": {
        "id": "q6odbVXGdAts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20ce777-36b9-436c-9edb-8bb0f8fa2bff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying ImageNet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 5/5 [00:16<00:00,  3.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "top1 = (correct_base / total) * 100\n",
        "print(f\"Top-1 accuracy standard: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_cupl / total) * 100\n",
        "print(f\"Top-1 accuracy CuPL: {top1:.2f}\")\n",
        "\n",
        "top1 = (correct_both / total) * 100\n",
        "print(f\"Top-1 accuracy both: {top1:.2f}\")"
      ],
      "metadata": {
        "id": "YjXwPA6ZdFZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f22d1a-76e8-4d08-e854-49d9292e5a66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 accuracy standard: 93.06\n",
            "Top-1 accuracy CuPL: 94.20\n",
            "Top-1 accuracy both: 93.67\n"
          ]
        }
      ]
    }
  ]
}