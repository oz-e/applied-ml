{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CoCoOp_caltech_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/KaiyangZhou/Dassl.pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbh5uHW1uAer",
        "outputId": "d211cfd3-82f8-4852-f40d-073b9b2f8b87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
            "  Cloning https://github.com/KaiyangZhou/Dassl.pytorch.git to /tmp/pip-req-build-2k76f8ob\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KaiyangZhou/Dassl.pytorch.git /tmp/pip-req-build-2k76f8ob\n",
            "  Resolved https://github.com/KaiyangZhou/Dassl.pytorch.git to commit c61a1b570ac6333bd50fb5ae06aea59002fb20bb\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8==3.7.9 (from dassl==0.6.3)\n",
            "  Using cached flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting yapf==0.29.0 (from dassl==0.6.3)\n",
            "  Using cached yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting isort==4.3.21 (from dassl==0.6.3)\n",
            "  Using cached isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting yacs (from dassl==0.6.3)\n",
            "  Using cached yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (5.2.0)\n",
            "Collecting tb-nightly (from dassl==0.6.3)\n",
            "  Using cached tb_nightly-2.20.0a20250323-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (4.67.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (2024.11.6)\n",
            "Collecting wilds==1.2.2 (from dassl==0.6.3)\n",
            "  Using cached wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (0.9.0)\n",
            "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Using cached entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Using cached pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Using cached pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Using cached mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.0.2)\n",
            "Collecting ogb>=1.2.6 (from wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting outdated>=0.2.0 (from wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (11.1.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2025.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (0.21.0+cu124)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dassl==0.6.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dassl==0.6.3) (3.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->dassl==0.6.3) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (5.29.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (3.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs->dassl==0.6.3) (6.0.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb>=1.2.6->wilds==1.2.2->dassl==0.6.3) (2.3.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->wilds==1.2.2->dassl==0.6.3) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->wilds==1.2.2->dassl==0.6.3) (2025.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tb-nightly->dassl==0.6.3) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->dassl==0.6.3) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (1.7.1)\n",
            "Using cached flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
            "Using cached isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "Using cached wilds-1.2.2-py3-none-any.whl (92 kB)\n",
            "Using cached yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
            "Using cached tb_nightly-2.20.0a20250323-py3-none-any.whl (5.5 MB)\n",
            "Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Using cached ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "Using cached outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Using cached pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
            "Using cached pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Building wheels for collected packages: dassl\n",
            "  Building wheel for dassl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dassl: filename=dassl-0.6.3-py3-none-any.whl size=138529 sha256=36805f8c662f9376811cce8e3dff5a8a27424ed89ddec7fbb05f3f4538838843\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xbumou2h/wheels/ff/23/48/2796fd9e0be04cddeae2896067958443ac0eda066bce445caf\n",
            "Successfully built dassl\n",
            "Installing collected packages: yapf, mccabe, yacs, pyflakes, pycodestyle, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, littleutils, isort, entrypoints, tb-nightly, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, flake8, nvidia-cusolver-cu12, ogb, wilds, dassl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: entrypoints\n",
            "    Found existing installation: entrypoints 0.4\n",
            "    Uninstalling entrypoints-0.4:\n",
            "      Successfully uninstalled entrypoints-0.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dassl-0.6.3 entrypoints-0.3 flake8-3.7.9 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.20.0a20250323 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KaiyangZhou/CoOp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELNIb_sdY-rG",
        "outputId": "02eb6e0b-d571-4a6c-d240-122a511518f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 199 (delta 199), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 31.18 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R +x CoOp/"
      ],
      "metadata": {
        "id": "G1ncHr1YsDTI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets/"
      ],
      "metadata": {
        "id": "uPfI7ioZjNLi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P datasets/ https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\n",
        "!unzip -q datasets/caltech-101.zip -d datasets/\n",
        "!tar -xzf datasets/caltech-101/101_ObjectCategories.tar.gz -C datasets/caltech-101/\n",
        "!rm -rf datasets/__MACOSX/\n",
        "!rm datasets/caltech-101/101_ObjectCategories.tar.gz\n",
        "!rm datasets/caltech-101/Annotations.tar\n",
        "!rm datasets/caltech-101/show_annotation.m\n",
        "!rm datasets/caltech-101.zip\n",
        "!gdown 1hyarUivQE36mY6jSomru6Fjd-JzwcCzN -O datasets/caltech-101/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLiuCh-llnZS",
        "outputId": "2a46a888-119a-4790-99fc-029fadb414c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 18:02:37--  https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\n",
            "Resolving data.caltech.edu (data.caltech.edu)... 35.155.11.48\n",
            "Connecting to data.caltech.edu (data.caltech.edu)|35.155.11.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://s3.us-west-2.amazonaws.com/caltechdata/47/20/fc77-d78a-4c50-81c9-d47c2004df45/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dcaltech-101.zip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250323T180238Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=0eed995a6c66a69b4e41153b321f8d4c2c32841b4bd6dd9b3982f7c6c060d46b [following]\n",
            "--2025-03-23 18:02:38--  https://s3.us-west-2.amazonaws.com/caltechdata/47/20/fc77-d78a-4c50-81c9-d47c2004df45/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dcaltech-101.zip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250323T180238Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=0eed995a6c66a69b4e41153b321f8d4c2c32841b4bd6dd9b3982f7c6c060d46b\n",
            "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.92.152.232, 52.92.147.96, 52.218.224.104, ...\n",
            "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.92.152.232|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137414764 (131M) [application/octet-stream]\n",
            "Saving to: ‘datasets/caltech-101.zip’\n",
            "\n",
            "caltech-101.zip     100%[===================>] 131.05M  18.8MB/s    in 8.4s    \n",
            "\n",
            "2025-03-23 18:02:47 (15.5 MB/s) - ‘datasets/caltech-101.zip’ saved [137414764/137414764]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "100% 809k/809k [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CoOp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiwDruVXsG_g",
        "outputId": "53d426bb-4752-4259-80cb-6387b8a05cb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn8dVtYKseOj",
        "outputId": "74d0b225-8b7b-4a44-d43c-aa30b6a5da0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy (from -r requirements.txt (line 1))\n",
            "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->-r requirements.txt (line 1)) (0.2.13)\n",
            "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/DATA=\\/path\\/to\\/datasets/DATA=\\/content\\/datasets/g' scripts/cocoop/base2new_train.sh\n",
        "!sed -i 's/DATA=\\/path\\/to\\/datasets/DATA=\\/content\\/datasets/g' scripts/cocoop/base2new_test.sh"
      ],
      "metadata": {
        "id": "hxknysFn8wHC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd scripts/cocoop/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQtcIdmhMBMV",
        "outputId": "ea422803-473e-4a99-b68b-a4745ee3d583"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp/scripts/cocoop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./base2new_train.sh caltech101 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj5aYM0TqzmN",
        "outputId": "707ab8e8-f05c-4561-9e32-a3d570eaf339"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-23 18:11:40.258749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742753500.503514    3216 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742753500.564975    3216 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 18:11:41.080960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
            "output_dir: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoCoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: base\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: a photo of a\n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoCoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.36\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoCoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/datasets/caltech-101/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE BASE CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  50\n",
            "# train_x  800\n",
            "# val      200\n",
            "# test     1,549\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "100%|███████████████████████████████████████| 351M/351M [00:07<00:00, 44.4MiB/s]\n",
            "Building custom CLIP\n",
            "Initial context: \"a photo of a\"\n",
            "Number of context words (tokens): 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear1.weight'}\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1/tensorboard)\n",
            "epoch [1/10] batch [20/800] time 0.090 (0.218) data 0.001 (0.025) loss 3.5449 (1.4195) lr 1.0000e-05 eta 0:29:00\n",
            "epoch [1/10] batch [40/800] time 0.090 (0.154) data 0.000 (0.013) loss 0.0028 (0.8833) lr 1.0000e-05 eta 0:20:29\n",
            "epoch [1/10] batch [60/800] time 0.089 (0.133) data 0.000 (0.009) loss 0.8857 (0.7865) lr 1.0000e-05 eta 0:17:34\n",
            "epoch [1/10] batch [80/800] time 0.090 (0.122) data 0.000 (0.007) loss 0.1163 (0.7566) lr 1.0000e-05 eta 0:16:07\n",
            "epoch [1/10] batch [100/800] time 0.091 (0.116) data 0.000 (0.005) loss 0.8096 (0.7236) lr 1.0000e-05 eta 0:15:15\n",
            "epoch [1/10] batch [120/800] time 0.089 (0.112) data 0.000 (0.005) loss 0.0166 (0.7627) lr 1.0000e-05 eta 0:14:40\n",
            "epoch [1/10] batch [140/800] time 0.089 (0.109) data 0.001 (0.004) loss 0.1615 (0.7642) lr 1.0000e-05 eta 0:14:18\n",
            "epoch [1/10] batch [160/800] time 0.094 (0.107) data 0.001 (0.004) loss 0.0289 (0.8024) lr 1.0000e-05 eta 0:14:02\n",
            "epoch [1/10] batch [180/800] time 0.091 (0.106) data 0.000 (0.003) loss 0.0209 (0.7646) lr 1.0000e-05 eta 0:13:45\n",
            "epoch [1/10] batch [200/800] time 0.090 (0.104) data 0.000 (0.003) loss 0.1479 (0.7409) lr 1.0000e-05 eta 0:13:31\n",
            "epoch [1/10] batch [220/800] time 0.091 (0.103) data 0.000 (0.003) loss 0.0139 (0.7367) lr 1.0000e-05 eta 0:13:20\n",
            "epoch [1/10] batch [240/800] time 0.092 (0.102) data 0.000 (0.002) loss 0.0055 (0.7247) lr 1.0000e-05 eta 0:13:10\n",
            "epoch [1/10] batch [260/800] time 0.089 (0.101) data 0.000 (0.002) loss 0.0409 (0.6985) lr 1.0000e-05 eta 0:13:02\n",
            "epoch [1/10] batch [280/800] time 0.103 (0.101) data 0.003 (0.002) loss 0.0116 (0.7034) lr 1.0000e-05 eta 0:12:58\n",
            "epoch [1/10] batch [300/800] time 0.091 (0.100) data 0.000 (0.002) loss 1.1387 (0.7236) lr 1.0000e-05 eta 0:12:52\n",
            "epoch [1/10] batch [320/800] time 0.090 (0.100) data 0.000 (0.002) loss 0.7065 (0.7515) lr 1.0000e-05 eta 0:12:45\n",
            "epoch [1/10] batch [340/800] time 0.091 (0.099) data 0.000 (0.002) loss 0.0062 (0.7265) lr 1.0000e-05 eta 0:12:40\n",
            "epoch [1/10] batch [360/800] time 0.092 (0.099) data 0.001 (0.002) loss 0.0134 (0.7074) lr 1.0000e-05 eta 0:12:34\n",
            "epoch [1/10] batch [380/800] time 0.092 (0.098) data 0.000 (0.002) loss 0.3855 (0.6917) lr 1.0000e-05 eta 0:12:29\n",
            "epoch [1/10] batch [400/800] time 0.088 (0.098) data 0.000 (0.002) loss 0.2172 (0.6784) lr 1.0000e-05 eta 0:12:26\n",
            "epoch [1/10] batch [420/800] time 0.093 (0.098) data 0.001 (0.002) loss 0.0001 (0.6877) lr 1.0000e-05 eta 0:12:23\n",
            "epoch [1/10] batch [440/800] time 0.091 (0.098) data 0.000 (0.002) loss 0.0309 (0.6947) lr 1.0000e-05 eta 0:12:19\n",
            "epoch [1/10] batch [460/800] time 0.092 (0.098) data 0.000 (0.002) loss 0.0008 (0.6918) lr 1.0000e-05 eta 0:12:15\n",
            "epoch [1/10] batch [480/800] time 0.092 (0.097) data 0.000 (0.002) loss 0.1021 (0.6986) lr 1.0000e-05 eta 0:12:11\n",
            "epoch [1/10] batch [500/800] time 0.092 (0.097) data 0.000 (0.001) loss 0.0191 (0.6871) lr 1.0000e-05 eta 0:12:07\n",
            "epoch [1/10] batch [520/800] time 0.092 (0.097) data 0.000 (0.001) loss 0.0064 (0.6909) lr 1.0000e-05 eta 0:12:04\n",
            "epoch [1/10] batch [540/800] time 0.101 (0.097) data 0.001 (0.001) loss 2.9531 (0.7085) lr 1.0000e-05 eta 0:12:02\n",
            "epoch [1/10] batch [560/800] time 0.091 (0.097) data 0.000 (0.001) loss 0.1227 (0.6946) lr 1.0000e-05 eta 0:11:59\n",
            "epoch [1/10] batch [580/800] time 0.092 (0.097) data 0.000 (0.001) loss 0.0009 (0.7037) lr 1.0000e-05 eta 0:11:56\n",
            "epoch [1/10] batch [600/800] time 0.091 (0.096) data 0.001 (0.001) loss 0.0005 (0.6992) lr 1.0000e-05 eta 0:11:53\n",
            "epoch [1/10] batch [620/800] time 0.091 (0.096) data 0.000 (0.001) loss 0.0567 (0.7102) lr 1.0000e-05 eta 0:11:50\n",
            "epoch [1/10] batch [640/800] time 0.100 (0.096) data 0.000 (0.001) loss 0.0037 (0.7076) lr 1.0000e-05 eta 0:11:47\n",
            "epoch [1/10] batch [660/800] time 0.092 (0.096) data 0.000 (0.001) loss 0.0264 (0.7018) lr 1.0000e-05 eta 0:11:45\n",
            "epoch [1/10] batch [680/800] time 0.093 (0.096) data 0.000 (0.001) loss 0.4807 (0.6967) lr 1.0000e-05 eta 0:11:43\n",
            "epoch [1/10] batch [700/800] time 0.092 (0.096) data 0.000 (0.001) loss 0.1073 (0.6910) lr 1.0000e-05 eta 0:11:40\n",
            "epoch [1/10] batch [720/800] time 0.094 (0.096) data 0.001 (0.001) loss 0.0052 (0.6878) lr 1.0000e-05 eta 0:11:38\n",
            "epoch [1/10] batch [740/800] time 0.093 (0.096) data 0.000 (0.001) loss 0.0313 (0.6829) lr 1.0000e-05 eta 0:11:35\n",
            "epoch [1/10] batch [760/800] time 0.092 (0.096) data 0.000 (0.001) loss 0.0001 (0.6871) lr 1.0000e-05 eta 0:11:33\n",
            "epoch [1/10] batch [780/800] time 0.092 (0.096) data 0.000 (0.001) loss 0.0029 (0.6878) lr 1.0000e-05 eta 0:11:30\n",
            "epoch [1/10] batch [800/800] time 0.090 (0.096) data 0.001 (0.001) loss 0.0003 (0.6942) lr 2.0000e-03 eta 0:11:28\n",
            "epoch [2/10] batch [20/800] time 0.093 (0.124) data 0.000 (0.026) loss 0.0193 (0.2106) lr 2.0000e-03 eta 0:14:51\n",
            "epoch [2/10] batch [40/800] time 0.093 (0.109) data 0.000 (0.013) loss 0.0005 (0.2226) lr 2.0000e-03 eta 0:12:58\n",
            "epoch [2/10] batch [60/800] time 0.093 (0.103) data 0.001 (0.009) loss 0.0434 (0.3826) lr 2.0000e-03 eta 0:12:18\n",
            "epoch [2/10] batch [80/800] time 0.093 (0.101) data 0.000 (0.007) loss 0.3398 (0.4397) lr 2.0000e-03 eta 0:11:58\n",
            "epoch [2/10] batch [100/800] time 0.102 (0.100) data 0.007 (0.006) loss 1.1816 (0.5114) lr 2.0000e-03 eta 0:11:48\n",
            "epoch [2/10] batch [120/800] time 0.104 (0.100) data 0.007 (0.005) loss 0.5757 (0.5507) lr 2.0000e-03 eta 0:11:46\n",
            "epoch [2/10] batch [140/800] time 0.095 (0.099) data 0.000 (0.005) loss 0.0117 (0.5341) lr 2.0000e-03 eta 0:11:39\n",
            "epoch [2/10] batch [160/800] time 0.098 (0.098) data 0.000 (0.004) loss 4.5625 (0.5739) lr 2.0000e-03 eta 0:11:32\n",
            "epoch [2/10] batch [180/800] time 0.093 (0.098) data 0.000 (0.004) loss 0.0012 (0.5938) lr 2.0000e-03 eta 0:11:28\n",
            "epoch [2/10] batch [200/800] time 0.093 (0.098) data 0.000 (0.003) loss 0.0033 (0.6395) lr 2.0000e-03 eta 0:11:23\n",
            "epoch [2/10] batch [220/800] time 0.093 (0.097) data 0.000 (0.003) loss 0.0006 (0.5988) lr 2.0000e-03 eta 0:11:19\n",
            "epoch [2/10] batch [240/800] time 0.098 (0.097) data 0.007 (0.003) loss 0.2522 (0.5864) lr 2.0000e-03 eta 0:11:17\n",
            "epoch [2/10] batch [260/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.1575 (0.5927) lr 2.0000e-03 eta 0:11:14\n",
            "epoch [2/10] batch [280/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.0901 (0.5660) lr 2.0000e-03 eta 0:11:11\n",
            "epoch [2/10] batch [300/800] time 0.093 (0.097) data 0.000 (0.003) loss 0.0845 (0.5484) lr 2.0000e-03 eta 0:11:07\n",
            "epoch [2/10] batch [320/800] time 0.095 (0.097) data 0.001 (0.002) loss 0.0029 (0.5315) lr 2.0000e-03 eta 0:11:04\n",
            "epoch [2/10] batch [340/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.1050 (0.5349) lr 2.0000e-03 eta 0:11:01\n",
            "epoch [2/10] batch [360/800] time 0.104 (0.096) data 0.002 (0.002) loss 0.4719 (0.5822) lr 2.0000e-03 eta 0:10:58\n",
            "epoch [2/10] batch [380/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0014 (0.5800) lr 2.0000e-03 eta 0:10:57\n",
            "epoch [2/10] batch [400/800] time 0.093 (0.096) data 0.000 (0.002) loss 5.4297 (0.5800) lr 2.0000e-03 eta 0:10:54\n",
            "epoch [2/10] batch [420/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0347 (0.6072) lr 2.0000e-03 eta 0:10:51\n",
            "epoch [2/10] batch [440/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0006 (0.6021) lr 2.0000e-03 eta 0:10:49\n",
            "epoch [2/10] batch [460/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0120 (0.5945) lr 2.0000e-03 eta 0:10:46\n",
            "epoch [2/10] batch [480/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.1422 (0.5809) lr 2.0000e-03 eta 0:10:43\n",
            "epoch [2/10] batch [500/800] time 0.100 (0.096) data 0.001 (0.002) loss 0.3086 (0.5876) lr 2.0000e-03 eta 0:10:42\n",
            "epoch [2/10] batch [520/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0033 (0.5938) lr 2.0000e-03 eta 0:10:40\n",
            "epoch [2/10] batch [540/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0062 (0.6052) lr 2.0000e-03 eta 0:10:37\n",
            "epoch [2/10] batch [560/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0087 (0.5897) lr 2.0000e-03 eta 0:10:35\n",
            "epoch [2/10] batch [580/800] time 0.093 (0.096) data 0.001 (0.002) loss 5.0469 (0.6010) lr 2.0000e-03 eta 0:10:33\n",
            "epoch [2/10] batch [600/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0057 (0.6044) lr 2.0000e-03 eta 0:10:30\n",
            "epoch [2/10] batch [620/800] time 0.098 (0.096) data 0.006 (0.002) loss 0.3391 (0.6076) lr 2.0000e-03 eta 0:10:28\n",
            "epoch [2/10] batch [640/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0014 (0.5979) lr 2.0000e-03 eta 0:10:27\n",
            "epoch [2/10] batch [660/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0821 (0.5847) lr 2.0000e-03 eta 0:10:24\n",
            "epoch [2/10] batch [680/800] time 0.092 (0.095) data 0.000 (0.002) loss 2.6523 (0.5831) lr 2.0000e-03 eta 0:10:22\n",
            "epoch [2/10] batch [700/800] time 0.092 (0.095) data 0.000 (0.002) loss 1.0859 (0.5810) lr 2.0000e-03 eta 0:10:19\n",
            "epoch [2/10] batch [720/800] time 0.094 (0.095) data 0.001 (0.002) loss 0.0015 (0.5848) lr 2.0000e-03 eta 0:10:17\n",
            "epoch [2/10] batch [740/800] time 0.098 (0.095) data 0.000 (0.002) loss 3.9238 (0.5830) lr 2.0000e-03 eta 0:10:14\n",
            "epoch [2/10] batch [760/800] time 0.092 (0.095) data 0.000 (0.002) loss 0.0001 (0.5705) lr 2.0000e-03 eta 0:10:13\n",
            "epoch [2/10] batch [780/800] time 0.092 (0.095) data 0.000 (0.002) loss 0.0002 (0.5613) lr 2.0000e-03 eta 0:10:11\n",
            "epoch [2/10] batch [800/800] time 0.090 (0.095) data 0.000 (0.002) loss 1.7988 (0.5612) lr 1.9511e-03 eta 0:10:08\n",
            "epoch [3/10] batch [20/800] time 0.091 (0.116) data 0.000 (0.017) loss 0.2004 (0.3042) lr 1.9511e-03 eta 0:12:18\n",
            "epoch [3/10] batch [40/800] time 0.094 (0.105) data 0.000 (0.009) loss 0.0070 (0.4042) lr 1.9511e-03 eta 0:11:06\n",
            "epoch [3/10] batch [60/800] time 0.093 (0.101) data 0.000 (0.006) loss 0.4861 (0.3658) lr 1.9511e-03 eta 0:10:39\n",
            "epoch [3/10] batch [80/800] time 0.101 (0.100) data 0.000 (0.005) loss 0.5151 (0.3679) lr 1.9511e-03 eta 0:10:34\n",
            "epoch [3/10] batch [100/800] time 0.093 (0.099) data 0.000 (0.004) loss 4.3281 (0.3806) lr 1.9511e-03 eta 0:10:24\n",
            "epoch [3/10] batch [120/800] time 0.095 (0.098) data 0.001 (0.004) loss 0.0027 (0.4048) lr 1.9511e-03 eta 0:10:17\n",
            "epoch [3/10] batch [140/800] time 0.092 (0.098) data 0.000 (0.003) loss 0.0001 (0.3850) lr 1.9511e-03 eta 0:10:11\n",
            "epoch [3/10] batch [160/800] time 0.095 (0.097) data 0.001 (0.003) loss 0.0834 (0.4058) lr 1.9511e-03 eta 0:10:06\n",
            "epoch [3/10] batch [180/800] time 0.114 (0.097) data 0.002 (0.003) loss 0.0118 (0.4209) lr 1.9511e-03 eta 0:10:03\n",
            "epoch [3/10] batch [200/800] time 0.098 (0.097) data 0.007 (0.003) loss 0.0201 (0.4237) lr 1.9511e-03 eta 0:10:03\n",
            "epoch [3/10] batch [220/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.0029 (0.4794) lr 1.9511e-03 eta 0:10:00\n",
            "epoch [3/10] batch [240/800] time 0.094 (0.097) data 0.000 (0.002) loss 0.0260 (0.4642) lr 1.9511e-03 eta 0:09:57\n",
            "epoch [3/10] batch [260/800] time 0.097 (0.097) data 0.002 (0.002) loss 0.0071 (0.4605) lr 1.9511e-03 eta 0:09:54\n",
            "epoch [3/10] batch [280/800] time 0.095 (0.097) data 0.000 (0.002) loss 0.0037 (0.4500) lr 1.9511e-03 eta 0:09:50\n",
            "epoch [3/10] batch [300/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0881 (0.4611) lr 1.9511e-03 eta 0:09:47\n",
            "epoch [3/10] batch [320/800] time 0.099 (0.096) data 0.001 (0.002) loss 0.1892 (0.4927) lr 1.9511e-03 eta 0:09:45\n",
            "epoch [3/10] batch [340/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.0009 (0.5484) lr 1.9511e-03 eta 0:09:45\n",
            "epoch [3/10] batch [360/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0263 (0.5508) lr 1.9511e-03 eta 0:09:42\n",
            "epoch [3/10] batch [380/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.9341 (0.5432) lr 1.9511e-03 eta 0:09:39\n",
            "epoch [3/10] batch [400/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.2268 (0.5427) lr 1.9511e-03 eta 0:09:37\n",
            "epoch [3/10] batch [420/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0609 (0.5719) lr 1.9511e-03 eta 0:09:34\n",
            "epoch [3/10] batch [440/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0065 (0.5640) lr 1.9511e-03 eta 0:09:32\n",
            "epoch [3/10] batch [460/800] time 0.101 (0.096) data 0.003 (0.002) loss 0.0495 (0.5415) lr 1.9511e-03 eta 0:09:31\n",
            "epoch [3/10] batch [480/800] time 0.092 (0.096) data 0.000 (0.002) loss 4.1914 (0.5409) lr 1.9511e-03 eta 0:09:28\n",
            "epoch [3/10] batch [500/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0002 (0.5350) lr 1.9511e-03 eta 0:09:26\n",
            "epoch [3/10] batch [520/800] time 0.092 (0.096) data 0.000 (0.002) loss 1.8770 (0.5191) lr 1.9511e-03 eta 0:09:23\n",
            "epoch [3/10] batch [540/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0004 (0.5202) lr 1.9511e-03 eta 0:09:21\n",
            "epoch [3/10] batch [560/800] time 0.094 (0.096) data 0.001 (0.002) loss 1.6699 (0.5149) lr 1.9511e-03 eta 0:09:19\n",
            "epoch [3/10] batch [580/800] time 0.100 (0.096) data 0.010 (0.002) loss 1.3379 (0.5169) lr 1.9511e-03 eta 0:09:17\n",
            "epoch [3/10] batch [600/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0002 (0.5193) lr 1.9511e-03 eta 0:09:16\n",
            "epoch [3/10] batch [620/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0369 (0.5104) lr 1.9511e-03 eta 0:09:13\n",
            "epoch [3/10] batch [640/800] time 0.095 (0.096) data 0.000 (0.002) loss 4.8711 (0.5071) lr 1.9511e-03 eta 0:09:11\n",
            "epoch [3/10] batch [660/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.3674 (0.5059) lr 1.9511e-03 eta 0:09:09\n",
            "epoch [3/10] batch [680/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0024 (0.5042) lr 1.9511e-03 eta 0:09:07\n",
            "epoch [3/10] batch [700/800] time 0.099 (0.096) data 0.001 (0.002) loss 6.2930 (0.5062) lr 1.9511e-03 eta 0:09:05\n",
            "epoch [3/10] batch [720/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0032 (0.5020) lr 1.9511e-03 eta 0:09:03\n",
            "epoch [3/10] batch [740/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0026 (0.4931) lr 1.9511e-03 eta 0:09:01\n",
            "epoch [3/10] batch [760/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.4424 (0.4886) lr 1.9511e-03 eta 0:08:59\n",
            "epoch [3/10] batch [780/800] time 0.094 (0.096) data 0.001 (0.001) loss 0.0088 (0.4954) lr 1.9511e-03 eta 0:08:57\n",
            "epoch [3/10] batch [800/800] time 0.089 (0.096) data 0.000 (0.001) loss 1.6582 (0.4940) lr 1.8090e-03 eta 0:08:54\n",
            "epoch [4/10] batch [20/800] time 0.102 (0.117) data 0.010 (0.017) loss 0.0004 (0.2984) lr 1.8090e-03 eta 0:10:50\n",
            "epoch [4/10] batch [40/800] time 0.094 (0.108) data 0.001 (0.011) loss 0.5894 (0.3836) lr 1.8090e-03 eta 0:10:01\n",
            "epoch [4/10] batch [60/800] time 0.093 (0.103) data 0.001 (0.007) loss 0.0108 (0.4776) lr 1.8090e-03 eta 0:09:33\n",
            "epoch [4/10] batch [80/800] time 0.094 (0.101) data 0.001 (0.005) loss 0.0005 (0.4484) lr 1.8090e-03 eta 0:09:18\n",
            "epoch [4/10] batch [100/800] time 0.094 (0.100) data 0.001 (0.004) loss 0.0005 (0.4296) lr 1.8090e-03 eta 0:09:08\n",
            "epoch [4/10] batch [120/800] time 0.094 (0.099) data 0.000 (0.004) loss 0.0049 (0.5134) lr 1.8090e-03 eta 0:09:02\n",
            "epoch [4/10] batch [140/800] time 0.094 (0.098) data 0.001 (0.003) loss 0.0256 (0.5330) lr 1.8090e-03 eta 0:08:56\n",
            "epoch [4/10] batch [160/800] time 0.099 (0.099) data 0.000 (0.003) loss 0.0017 (0.5228) lr 1.8090e-03 eta 0:08:56\n",
            "epoch [4/10] batch [180/800] time 0.092 (0.098) data 0.000 (0.003) loss 0.0018 (0.5075) lr 1.8090e-03 eta 0:08:52\n",
            "epoch [4/10] batch [200/800] time 0.093 (0.098) data 0.000 (0.003) loss 4.0859 (0.4971) lr 1.8090e-03 eta 0:08:48\n",
            "epoch [4/10] batch [220/800] time 0.095 (0.097) data 0.001 (0.003) loss 0.7412 (0.4853) lr 1.8090e-03 eta 0:08:44\n",
            "epoch [4/10] batch [240/800] time 0.093 (0.097) data 0.000 (0.003) loss 2.9473 (0.4984) lr 1.8090e-03 eta 0:08:40\n",
            "epoch [4/10] batch [260/800] time 0.094 (0.097) data 0.000 (0.002) loss 0.0193 (0.5092) lr 1.8090e-03 eta 0:08:37\n",
            "epoch [4/10] batch [280/800] time 0.098 (0.097) data 0.002 (0.002) loss 0.0045 (0.5009) lr 1.8090e-03 eta 0:08:35\n",
            "epoch [4/10] batch [300/800] time 0.093 (0.097) data 0.000 (0.003) loss 0.0001 (0.5270) lr 1.8090e-03 eta 0:08:34\n",
            "epoch [4/10] batch [320/800] time 0.094 (0.097) data 0.000 (0.002) loss 4.4219 (0.5266) lr 1.8090e-03 eta 0:08:31\n",
            "epoch [4/10] batch [340/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.3884 (0.5154) lr 1.8090e-03 eta 0:08:28\n",
            "epoch [4/10] batch [360/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0001 (0.5066) lr 1.8090e-03 eta 0:08:25\n",
            "epoch [4/10] batch [380/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0577 (0.5279) lr 1.8090e-03 eta 0:08:22\n",
            "epoch [4/10] batch [400/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0104 (0.5293) lr 1.8090e-03 eta 0:08:19\n",
            "epoch [4/10] batch [420/800] time 0.097 (0.096) data 0.001 (0.002) loss 0.0534 (0.5182) lr 1.8090e-03 eta 0:08:18\n",
            "epoch [4/10] batch [440/800] time 0.092 (0.096) data 0.000 (0.002) loss 3.5059 (0.5232) lr 1.8090e-03 eta 0:08:16\n",
            "epoch [4/10] batch [460/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.3142 (0.5118) lr 1.8090e-03 eta 0:08:13\n",
            "epoch [4/10] batch [480/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.4985 (0.5178) lr 1.8090e-03 eta 0:08:11\n",
            "epoch [4/10] batch [500/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0007 (0.5020) lr 1.8090e-03 eta 0:08:09\n",
            "epoch [4/10] batch [520/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0011 (0.4954) lr 1.8090e-03 eta 0:08:06\n",
            "epoch [4/10] batch [540/800] time 0.100 (0.096) data 0.009 (0.002) loss 0.0990 (0.5066) lr 1.8090e-03 eta 0:08:04\n",
            "epoch [4/10] batch [560/800] time 0.094 (0.096) data 0.000 (0.002) loss 2.5332 (0.5270) lr 1.8090e-03 eta 0:08:03\n",
            "epoch [4/10] batch [580/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0015 (0.5193) lr 1.8090e-03 eta 0:08:00\n",
            "epoch [4/10] batch [600/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0012 (0.5290) lr 1.8090e-03 eta 0:07:58\n",
            "epoch [4/10] batch [620/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.8623 (0.5457) lr 1.8090e-03 eta 0:07:56\n",
            "epoch [4/10] batch [640/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0411 (0.5455) lr 1.8090e-03 eta 0:07:53\n",
            "epoch [4/10] batch [660/800] time 0.104 (0.096) data 0.004 (0.002) loss 0.0000 (0.5417) lr 1.8090e-03 eta 0:07:51\n",
            "epoch [4/10] batch [680/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0002 (0.5428) lr 1.8090e-03 eta 0:07:50\n",
            "epoch [4/10] batch [700/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0021 (0.5427) lr 1.8090e-03 eta 0:07:48\n",
            "epoch [4/10] batch [720/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0162 (0.5455) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [740/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0035 (0.5371) lr 1.8090e-03 eta 0:07:44\n",
            "epoch [4/10] batch [760/800] time 0.094 (0.095) data 0.000 (0.002) loss 0.5645 (0.5345) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [780/800] time 0.094 (0.095) data 0.001 (0.002) loss 0.0026 (0.5369) lr 1.8090e-03 eta 0:07:40\n",
            "epoch [4/10] batch [800/800] time 0.092 (0.095) data 0.000 (0.002) loss 0.0004 (0.5293) lr 1.5878e-03 eta 0:07:37\n",
            "epoch [5/10] batch [20/800] time 0.096 (0.125) data 0.000 (0.026) loss 0.0014 (0.3795) lr 1.5878e-03 eta 0:09:57\n",
            "epoch [5/10] batch [40/800] time 0.097 (0.110) data 0.002 (0.013) loss 2.1055 (0.3662) lr 1.5878e-03 eta 0:08:41\n",
            "epoch [5/10] batch [60/800] time 0.093 (0.104) data 0.001 (0.009) loss 0.5991 (0.3344) lr 1.5878e-03 eta 0:08:15\n",
            "epoch [5/10] batch [80/800] time 0.093 (0.102) data 0.000 (0.007) loss 0.0121 (0.3319) lr 1.5878e-03 eta 0:08:00\n",
            "epoch [5/10] batch [100/800] time 0.095 (0.100) data 0.000 (0.006) loss 0.0006 (0.3234) lr 1.5878e-03 eta 0:07:51\n",
            "epoch [5/10] batch [120/800] time 0.099 (0.100) data 0.010 (0.005) loss 0.0028 (0.3616) lr 1.5878e-03 eta 0:07:48\n",
            "epoch [5/10] batch [140/800] time 0.092 (0.100) data 0.000 (0.005) loss 0.0105 (0.3919) lr 1.5878e-03 eta 0:07:44\n",
            "epoch [5/10] batch [160/800] time 0.094 (0.099) data 0.000 (0.004) loss 0.0004 (0.3656) lr 1.5878e-03 eta 0:07:38\n",
            "epoch [5/10] batch [180/800] time 0.093 (0.098) data 0.000 (0.004) loss 1.4365 (0.3729) lr 1.5878e-03 eta 0:07:33\n",
            "epoch [5/10] batch [200/800] time 0.092 (0.098) data 0.000 (0.003) loss 0.0007 (0.3800) lr 1.5878e-03 eta 0:07:29\n",
            "epoch [5/10] batch [220/800] time 0.093 (0.097) data 0.000 (0.003) loss 2.1191 (0.4328) lr 1.5878e-03 eta 0:07:25\n",
            "epoch [5/10] batch [240/800] time 0.096 (0.097) data 0.001 (0.003) loss 0.0028 (0.4375) lr 1.5878e-03 eta 0:07:23\n",
            "epoch [5/10] batch [260/800] time 0.092 (0.097) data 0.000 (0.003) loss 0.0128 (0.4682) lr 1.5878e-03 eta 0:07:21\n",
            "epoch [5/10] batch [280/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.1616 (0.4808) lr 1.5878e-03 eta 0:07:18\n",
            "epoch [5/10] batch [300/800] time 0.093 (0.097) data 0.000 (0.003) loss 0.1048 (0.4628) lr 1.5878e-03 eta 0:07:15\n",
            "epoch [5/10] batch [320/800] time 0.092 (0.097) data 0.000 (0.003) loss 0.0013 (0.4836) lr 1.5878e-03 eta 0:07:12\n",
            "epoch [5/10] batch [340/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0042 (0.4684) lr 1.5878e-03 eta 0:07:09\n",
            "epoch [5/10] batch [360/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0011 (0.4643) lr 1.5878e-03 eta 0:07:06\n",
            "epoch [5/10] batch [380/800] time 0.100 (0.096) data 0.000 (0.003) loss 0.0743 (0.4573) lr 1.5878e-03 eta 0:07:05\n",
            "epoch [5/10] batch [400/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0014 (0.4662) lr 1.5878e-03 eta 0:07:03\n",
            "epoch [5/10] batch [420/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0002 (0.4919) lr 1.5878e-03 eta 0:07:00\n",
            "epoch [5/10] batch [440/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.1146 (0.4746) lr 1.5878e-03 eta 0:06:58\n",
            "epoch [5/10] batch [460/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0007 (0.4654) lr 1.5878e-03 eta 0:06:55\n",
            "epoch [5/10] batch [480/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0026 (0.4702) lr 1.5878e-03 eta 0:06:53\n",
            "epoch [5/10] batch [500/800] time 0.101 (0.096) data 0.008 (0.002) loss 0.0002 (0.4579) lr 1.5878e-03 eta 0:06:51\n",
            "epoch [5/10] batch [520/800] time 0.095 (0.096) data 0.000 (0.002) loss 1.5234 (0.4715) lr 1.5878e-03 eta 0:06:49\n",
            "epoch [5/10] batch [540/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0372 (0.4675) lr 1.5878e-03 eta 0:06:47\n",
            "epoch [5/10] batch [560/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0033 (0.4720) lr 1.5878e-03 eta 0:06:45\n",
            "epoch [5/10] batch [580/800] time 0.092 (0.095) data 0.000 (0.002) loss 0.0001 (0.4661) lr 1.5878e-03 eta 0:06:42\n",
            "epoch [5/10] batch [600/800] time 0.093 (0.095) data 0.001 (0.002) loss 0.6079 (0.4642) lr 1.5878e-03 eta 0:06:40\n",
            "epoch [5/10] batch [620/800] time 0.096 (0.095) data 0.000 (0.002) loss 0.0001 (0.4669) lr 1.5878e-03 eta 0:06:38\n",
            "epoch [5/10] batch [640/800] time 0.106 (0.096) data 0.007 (0.002) loss 3.5938 (0.4683) lr 1.5878e-03 eta 0:06:37\n",
            "epoch [5/10] batch [660/800] time 0.092 (0.095) data 0.000 (0.002) loss 1.0000 (0.4891) lr 1.5878e-03 eta 0:06:35\n",
            "epoch [5/10] batch [680/800] time 0.093 (0.095) data 0.000 (0.002) loss 2.5527 (0.4868) lr 1.5878e-03 eta 0:06:32\n",
            "epoch [5/10] batch [700/800] time 0.093 (0.095) data 0.000 (0.002) loss 0.0000 (0.4925) lr 1.5878e-03 eta 0:06:30\n",
            "epoch [5/10] batch [720/800] time 0.095 (0.095) data 0.000 (0.002) loss 7.2891 (0.4954) lr 1.5878e-03 eta 0:06:28\n",
            "epoch [5/10] batch [740/800] time 0.093 (0.095) data 0.000 (0.002) loss 0.0352 (0.4996) lr 1.5878e-03 eta 0:06:26\n",
            "epoch [5/10] batch [760/800] time 0.101 (0.095) data 0.007 (0.002) loss 0.0022 (0.5032) lr 1.5878e-03 eta 0:06:24\n",
            "epoch [5/10] batch [780/800] time 0.093 (0.095) data 0.000 (0.002) loss 0.8394 (0.4961) lr 1.5878e-03 eta 0:06:22\n",
            "epoch [5/10] batch [800/800] time 0.088 (0.095) data 0.000 (0.002) loss 2.9453 (0.5051) lr 1.3090e-03 eta 0:06:20\n",
            "epoch [6/10] batch [20/800] time 0.093 (0.115) data 0.000 (0.017) loss 0.2788 (0.2326) lr 1.3090e-03 eta 0:07:38\n",
            "epoch [6/10] batch [40/800] time 0.093 (0.105) data 0.000 (0.009) loss 0.0695 (0.3875) lr 1.3090e-03 eta 0:06:53\n",
            "epoch [6/10] batch [60/800] time 0.094 (0.101) data 0.000 (0.006) loss 0.0105 (0.3512) lr 1.3090e-03 eta 0:06:37\n",
            "epoch [6/10] batch [80/800] time 0.099 (0.100) data 0.004 (0.005) loss 0.0000 (0.3353) lr 1.3090e-03 eta 0:06:31\n",
            "epoch [6/10] batch [100/800] time 0.092 (0.099) data 0.000 (0.004) loss 0.0002 (0.4412) lr 1.3090e-03 eta 0:06:26\n",
            "epoch [6/10] batch [120/800] time 0.093 (0.098) data 0.000 (0.003) loss 0.6860 (0.4196) lr 1.3090e-03 eta 0:06:20\n",
            "epoch [6/10] batch [140/800] time 0.092 (0.097) data 0.000 (0.003) loss 0.7915 (0.4428) lr 1.3090e-03 eta 0:06:15\n",
            "epoch [6/10] batch [160/800] time 0.098 (0.097) data 0.004 (0.003) loss 0.0889 (0.4259) lr 1.3090e-03 eta 0:06:12\n",
            "epoch [6/10] batch [180/800] time 0.095 (0.097) data 0.000 (0.003) loss 0.0006 (0.4582) lr 1.3090e-03 eta 0:06:09\n",
            "epoch [6/10] batch [200/800] time 0.102 (0.097) data 0.010 (0.002) loss 0.0014 (0.4442) lr 1.3090e-03 eta 0:06:07\n",
            "epoch [6/10] batch [220/800] time 0.094 (0.097) data 0.000 (0.002) loss 0.0012 (0.4358) lr 1.3090e-03 eta 0:06:06\n",
            "epoch [6/10] batch [240/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.0001 (0.4107) lr 1.3090e-03 eta 0:06:03\n",
            "epoch [6/10] batch [260/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0115 (0.3989) lr 1.3090e-03 eta 0:06:00\n",
            "epoch [6/10] batch [280/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.5312 (0.4090) lr 1.3090e-03 eta 0:05:57\n",
            "epoch [6/10] batch [300/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0002 (0.4137) lr 1.3090e-03 eta 0:05:54\n",
            "epoch [6/10] batch [320/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0184 (0.4007) lr 1.3090e-03 eta 0:05:52\n",
            "epoch [6/10] batch [340/800] time 0.105 (0.096) data 0.007 (0.002) loss 1.1426 (0.4219) lr 1.3090e-03 eta 0:05:50\n",
            "epoch [6/10] batch [360/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0002 (0.4401) lr 1.3090e-03 eta 0:05:48\n",
            "epoch [6/10] batch [380/800] time 0.093 (0.096) data 0.000 (0.002) loss 4.2344 (0.4536) lr 1.3090e-03 eta 0:05:46\n",
            "epoch [6/10] batch [400/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.1343 (0.4447) lr 1.3090e-03 eta 0:05:44\n",
            "epoch [6/10] batch [420/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0009 (0.4451) lr 1.3090e-03 eta 0:05:42\n",
            "epoch [6/10] batch [440/800] time 0.093 (0.095) data 0.001 (0.002) loss 1.3203 (0.4355) lr 1.3090e-03 eta 0:05:39\n",
            "epoch [6/10] batch [460/800] time 0.092 (0.096) data 0.000 (0.002) loss 1.8857 (0.4334) lr 1.3090e-03 eta 0:05:38\n",
            "epoch [6/10] batch [480/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0038 (0.4337) lr 1.3090e-03 eta 0:05:36\n",
            "epoch [6/10] batch [500/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0158 (0.4252) lr 1.3090e-03 eta 0:05:34\n",
            "epoch [6/10] batch [520/800] time 0.093 (0.095) data 0.000 (0.002) loss 0.0350 (0.4218) lr 1.3090e-03 eta 0:05:32\n",
            "epoch [6/10] batch [540/800] time 0.092 (0.095) data 0.000 (0.001) loss 0.5894 (0.4132) lr 1.3090e-03 eta 0:05:30\n",
            "epoch [6/10] batch [560/800] time 0.093 (0.095) data 0.000 (0.001) loss 3.4355 (0.4227) lr 1.3090e-03 eta 0:05:27\n",
            "epoch [6/10] batch [580/800] time 0.095 (0.095) data 0.000 (0.001) loss 0.0054 (0.4161) lr 1.3090e-03 eta 0:05:25\n",
            "epoch [6/10] batch [600/800] time 0.100 (0.095) data 0.001 (0.002) loss 4.2969 (0.4279) lr 1.3090e-03 eta 0:05:24\n",
            "epoch [6/10] batch [620/800] time 0.093 (0.095) data 0.000 (0.001) loss 0.2866 (0.4322) lr 1.3090e-03 eta 0:05:22\n",
            "epoch [6/10] batch [640/800] time 0.096 (0.095) data 0.001 (0.001) loss 0.2272 (0.4363) lr 1.3090e-03 eta 0:05:20\n",
            "epoch [6/10] batch [660/800] time 0.094 (0.095) data 0.000 (0.001) loss 0.9209 (0.4431) lr 1.3090e-03 eta 0:05:18\n",
            "epoch [6/10] batch [680/800] time 0.093 (0.095) data 0.000 (0.001) loss 0.2673 (0.4418) lr 1.3090e-03 eta 0:05:16\n",
            "epoch [6/10] batch [700/800] time 0.093 (0.095) data 0.000 (0.001) loss 0.0003 (0.4449) lr 1.3090e-03 eta 0:05:14\n",
            "epoch [6/10] batch [720/800] time 0.099 (0.095) data 0.007 (0.001) loss 2.0527 (0.4504) lr 1.3090e-03 eta 0:05:12\n",
            "epoch [6/10] batch [740/800] time 0.093 (0.095) data 0.000 (0.001) loss 0.0230 (0.4461) lr 1.3090e-03 eta 0:05:10\n",
            "epoch [6/10] batch [760/800] time 0.095 (0.095) data 0.000 (0.001) loss 0.0001 (0.4484) lr 1.3090e-03 eta 0:05:08\n",
            "epoch [6/10] batch [780/800] time 0.095 (0.095) data 0.001 (0.001) loss 0.0005 (0.4457) lr 1.3090e-03 eta 0:05:06\n",
            "epoch [6/10] batch [800/800] time 0.092 (0.095) data 0.000 (0.001) loss 0.0007 (0.4418) lr 1.0000e-03 eta 0:05:04\n",
            "epoch [7/10] batch [20/800] time 0.094 (0.115) data 0.000 (0.017) loss 0.1389 (0.2471) lr 1.0000e-03 eta 0:06:05\n",
            "epoch [7/10] batch [40/800] time 0.097 (0.106) data 0.000 (0.009) loss 0.0009 (0.1896) lr 1.0000e-03 eta 0:05:33\n",
            "epoch [7/10] batch [60/800] time 0.093 (0.103) data 0.000 (0.007) loss 0.0052 (0.2296) lr 1.0000e-03 eta 0:05:22\n",
            "epoch [7/10] batch [80/800] time 0.095 (0.101) data 0.000 (0.005) loss 3.6484 (0.3701) lr 1.0000e-03 eta 0:05:14\n",
            "epoch [7/10] batch [100/800] time 0.094 (0.100) data 0.000 (0.004) loss 0.0001 (0.3608) lr 1.0000e-03 eta 0:05:08\n",
            "epoch [7/10] batch [120/800] time 0.095 (0.099) data 0.001 (0.004) loss 4.3633 (0.4993) lr 1.0000e-03 eta 0:05:03\n",
            "epoch [7/10] batch [140/800] time 0.093 (0.098) data 0.000 (0.003) loss 0.0040 (0.5054) lr 1.0000e-03 eta 0:04:59\n",
            "epoch [7/10] batch [160/800] time 0.110 (0.098) data 0.007 (0.003) loss 0.0016 (0.5188) lr 1.0000e-03 eta 0:04:57\n",
            "epoch [7/10] batch [180/800] time 0.095 (0.098) data 0.000 (0.003) loss 0.0002 (0.4883) lr 1.0000e-03 eta 0:04:55\n",
            "epoch [7/10] batch [200/800] time 0.095 (0.098) data 0.001 (0.003) loss 0.0198 (0.4477) lr 1.0000e-03 eta 0:04:52\n",
            "epoch [7/10] batch [220/800] time 0.094 (0.097) data 0.001 (0.003) loss 0.0096 (0.4487) lr 1.0000e-03 eta 0:04:49\n",
            "epoch [7/10] batch [240/800] time 0.095 (0.097) data 0.000 (0.003) loss 2.7852 (0.4892) lr 1.0000e-03 eta 0:04:47\n",
            "epoch [7/10] batch [260/800] time 0.094 (0.097) data 0.001 (0.002) loss 0.0025 (0.4774) lr 1.0000e-03 eta 0:04:44\n",
            "epoch [7/10] batch [280/800] time 0.093 (0.097) data 0.000 (0.002) loss 2.7969 (0.5012) lr 1.0000e-03 eta 0:04:42\n",
            "epoch [7/10] batch [300/800] time 0.100 (0.097) data 0.003 (0.002) loss 0.0402 (0.4964) lr 1.0000e-03 eta 0:04:40\n",
            "epoch [7/10] batch [320/800] time 0.095 (0.097) data 0.000 (0.002) loss 0.2114 (0.4964) lr 1.0000e-03 eta 0:04:38\n",
            "epoch [7/10] batch [340/800] time 0.094 (0.097) data 0.001 (0.002) loss 0.2783 (0.4998) lr 1.0000e-03 eta 0:04:36\n",
            "epoch [7/10] batch [360/800] time 0.094 (0.097) data 0.000 (0.002) loss 0.0026 (0.4864) lr 1.0000e-03 eta 0:04:34\n",
            "epoch [7/10] batch [380/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0046 (0.4798) lr 1.0000e-03 eta 0:04:31\n",
            "epoch [7/10] batch [400/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0003 (0.4642) lr 1.0000e-03 eta 0:04:29\n",
            "epoch [7/10] batch [420/800] time 0.092 (0.096) data 0.001 (0.002) loss 0.0010 (0.4615) lr 1.0000e-03 eta 0:04:27\n",
            "epoch [7/10] batch [440/800] time 0.100 (0.096) data 0.012 (0.002) loss 0.2240 (0.4846) lr 1.0000e-03 eta 0:04:26\n",
            "epoch [7/10] batch [460/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0001 (0.4818) lr 1.0000e-03 eta 0:04:23\n",
            "epoch [7/10] batch [480/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.9229 (0.4684) lr 1.0000e-03 eta 0:04:21\n",
            "epoch [7/10] batch [500/800] time 0.094 (0.096) data 0.001 (0.002) loss 1.5723 (0.4638) lr 1.0000e-03 eta 0:04:19\n",
            "epoch [7/10] batch [520/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0000 (0.4647) lr 1.0000e-03 eta 0:04:17\n",
            "epoch [7/10] batch [540/800] time 0.098 (0.096) data 0.001 (0.002) loss 0.0051 (0.4699) lr 1.0000e-03 eta 0:04:15\n",
            "epoch [7/10] batch [560/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.1251 (0.4762) lr 1.0000e-03 eta 0:04:13\n",
            "epoch [7/10] batch [580/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0013 (0.4704) lr 1.0000e-03 eta 0:04:11\n",
            "epoch [7/10] batch [600/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0012 (0.4624) lr 1.0000e-03 eta 0:04:09\n",
            "epoch [7/10] batch [620/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0014 (0.4548) lr 1.0000e-03 eta 0:04:07\n",
            "epoch [7/10] batch [640/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0009 (0.4495) lr 1.0000e-03 eta 0:04:05\n",
            "epoch [7/10] batch [660/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0000 (0.4465) lr 1.0000e-03 eta 0:04:03\n",
            "epoch [7/10] batch [680/800] time 0.097 (0.096) data 0.001 (0.002) loss 0.0001 (0.4496) lr 1.0000e-03 eta 0:04:01\n",
            "epoch [7/10] batch [700/800] time 0.094 (0.096) data 0.001 (0.002) loss 2.5781 (0.4621) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [720/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0119 (0.4609) lr 1.0000e-03 eta 0:03:57\n",
            "epoch [7/10] batch [740/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0209 (0.4589) lr 1.0000e-03 eta 0:03:55\n",
            "epoch [7/10] batch [760/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0317 (0.4636) lr 1.0000e-03 eta 0:03:53\n",
            "epoch [7/10] batch [780/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0285 (0.4563) lr 1.0000e-03 eta 0:03:51\n",
            "epoch [7/10] batch [800/800] time 0.091 (0.095) data 0.000 (0.002) loss 0.0012 (0.4620) lr 6.9098e-04 eta 0:03:49\n",
            "epoch [8/10] batch [20/800] time 0.094 (0.131) data 0.000 (0.026) loss 2.3945 (0.4894) lr 6.9098e-04 eta 0:05:12\n",
            "epoch [8/10] batch [40/800] time 0.093 (0.113) data 0.000 (0.013) loss 0.0004 (0.3321) lr 6.9098e-04 eta 0:04:26\n",
            "epoch [8/10] batch [60/800] time 0.093 (0.106) data 0.000 (0.009) loss 0.0002 (0.3141) lr 6.9098e-04 eta 0:04:09\n",
            "epoch [8/10] batch [80/800] time 0.094 (0.103) data 0.000 (0.007) loss 0.0004 (0.2956) lr 6.9098e-04 eta 0:03:59\n",
            "epoch [8/10] batch [100/800] time 0.094 (0.101) data 0.000 (0.005) loss 0.1421 (0.3446) lr 6.9098e-04 eta 0:03:53\n",
            "epoch [8/10] batch [120/800] time 0.103 (0.101) data 0.008 (0.005) loss 0.0000 (0.3563) lr 6.9098e-04 eta 0:03:49\n",
            "epoch [8/10] batch [140/800] time 0.094 (0.100) data 0.001 (0.005) loss 0.2487 (0.4166) lr 6.9098e-04 eta 0:03:46\n",
            "epoch [8/10] batch [160/800] time 0.095 (0.100) data 0.001 (0.004) loss 0.0014 (0.3884) lr 6.9098e-04 eta 0:03:43\n",
            "epoch [8/10] batch [180/800] time 0.094 (0.099) data 0.000 (0.004) loss 0.0202 (0.3660) lr 6.9098e-04 eta 0:03:40\n",
            "epoch [8/10] batch [200/800] time 0.093 (0.099) data 0.001 (0.003) loss 0.0445 (0.3816) lr 6.9098e-04 eta 0:03:36\n",
            "epoch [8/10] batch [220/800] time 0.093 (0.098) data 0.001 (0.003) loss 0.0001 (0.3579) lr 6.9098e-04 eta 0:03:33\n",
            "epoch [8/10] batch [240/800] time 0.104 (0.098) data 0.006 (0.003) loss 0.0012 (0.3745) lr 6.9098e-04 eta 0:03:31\n",
            "epoch [8/10] batch [260/800] time 0.094 (0.098) data 0.000 (0.003) loss 0.0000 (0.3669) lr 6.9098e-04 eta 0:03:29\n",
            "epoch [8/10] batch [280/800] time 0.095 (0.098) data 0.000 (0.003) loss 0.0027 (0.3603) lr 6.9098e-04 eta 0:03:27\n",
            "epoch [8/10] batch [300/800] time 0.092 (0.098) data 0.000 (0.003) loss 0.4983 (0.3659) lr 6.9098e-04 eta 0:03:24\n",
            "epoch [8/10] batch [320/800] time 0.092 (0.097) data 0.000 (0.003) loss 0.0039 (0.3777) lr 6.9098e-04 eta 0:03:22\n",
            "epoch [8/10] batch [340/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.0019 (0.3675) lr 6.9098e-04 eta 0:03:19\n",
            "epoch [8/10] batch [360/800] time 0.095 (0.097) data 0.000 (0.002) loss 0.0002 (0.3995) lr 6.9098e-04 eta 0:03:17\n",
            "epoch [8/10] batch [380/800] time 0.099 (0.097) data 0.006 (0.002) loss 0.0003 (0.3996) lr 6.9098e-04 eta 0:03:15\n",
            "epoch [8/10] batch [400/800] time 0.100 (0.097) data 0.009 (0.002) loss 0.0205 (0.3879) lr 6.9098e-04 eta 0:03:13\n",
            "epoch [8/10] batch [420/800] time 0.094 (0.097) data 0.000 (0.002) loss 0.0012 (0.3882) lr 6.9098e-04 eta 0:03:11\n",
            "epoch [8/10] batch [440/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0198 (0.3743) lr 6.9098e-04 eta 0:03:09\n",
            "epoch [8/10] batch [460/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.8896 (0.3687) lr 6.9098e-04 eta 0:03:06\n",
            "epoch [8/10] batch [480/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0033 (0.3858) lr 6.9098e-04 eta 0:03:04\n",
            "epoch [8/10] batch [500/800] time 0.100 (0.096) data 0.007 (0.002) loss 0.0076 (0.3944) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [520/800] time 0.093 (0.096) data 0.000 (0.002) loss 2.0254 (0.3965) lr 6.9098e-04 eta 0:03:01\n",
            "epoch [8/10] batch [540/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0002 (0.4081) lr 6.9098e-04 eta 0:02:59\n",
            "epoch [8/10] batch [560/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0224 (0.4006) lr 6.9098e-04 eta 0:02:57\n",
            "epoch [8/10] batch [580/800] time 0.095 (0.096) data 0.000 (0.002) loss 1.3428 (0.3985) lr 6.9098e-04 eta 0:02:54\n",
            "epoch [8/10] batch [600/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0028 (0.3992) lr 6.9098e-04 eta 0:02:52\n",
            "epoch [8/10] batch [620/800] time 0.099 (0.096) data 0.000 (0.002) loss 2.9609 (0.3967) lr 6.9098e-04 eta 0:02:50\n",
            "epoch [8/10] batch [640/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.3711 (0.4036) lr 6.9098e-04 eta 0:02:49\n",
            "epoch [8/10] batch [660/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0944 (0.4141) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [680/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0132 (0.4112) lr 6.9098e-04 eta 0:02:45\n",
            "epoch [8/10] batch [700/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0006 (0.4191) lr 6.9098e-04 eta 0:02:43\n",
            "epoch [8/10] batch [720/800] time 0.093 (0.096) data 0.001 (0.002) loss 3.0195 (0.4191) lr 6.9098e-04 eta 0:02:41\n",
            "epoch [8/10] batch [740/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0028 (0.4236) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [760/800] time 0.100 (0.096) data 0.007 (0.002) loss 0.0541 (0.4274) lr 6.9098e-04 eta 0:02:37\n",
            "epoch [8/10] batch [780/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0007 (0.4273) lr 6.9098e-04 eta 0:02:35\n",
            "epoch [8/10] batch [800/800] time 0.089 (0.096) data 0.000 (0.002) loss 1.1992 (0.4272) lr 4.1221e-04 eta 0:02:33\n",
            "epoch [9/10] batch [20/800] time 0.093 (0.116) data 0.000 (0.019) loss 0.0012 (0.6350) lr 4.1221e-04 eta 0:03:02\n",
            "epoch [9/10] batch [40/800] time 0.093 (0.105) data 0.001 (0.010) loss 0.0114 (0.5213) lr 4.1221e-04 eta 0:02:43\n",
            "epoch [9/10] batch [60/800] time 0.095 (0.101) data 0.001 (0.007) loss 0.0266 (0.4680) lr 4.1221e-04 eta 0:02:36\n",
            "epoch [9/10] batch [80/800] time 0.098 (0.100) data 0.000 (0.006) loss 0.0001 (0.3926) lr 4.1221e-04 eta 0:02:32\n",
            "epoch [9/10] batch [100/800] time 0.095 (0.100) data 0.000 (0.005) loss 0.6094 (0.4156) lr 4.1221e-04 eta 0:02:29\n",
            "epoch [9/10] batch [120/800] time 0.095 (0.099) data 0.000 (0.004) loss 0.0016 (0.4034) lr 4.1221e-04 eta 0:02:25\n",
            "epoch [9/10] batch [140/800] time 0.092 (0.098) data 0.000 (0.004) loss 0.0046 (0.3464) lr 4.1221e-04 eta 0:02:22\n",
            "epoch [9/10] batch [160/800] time 0.095 (0.097) data 0.001 (0.003) loss 0.0003 (0.3529) lr 4.1221e-04 eta 0:02:20\n",
            "epoch [9/10] batch [180/800] time 0.093 (0.097) data 0.000 (0.003) loss 0.0001 (0.3473) lr 4.1221e-04 eta 0:02:17\n",
            "epoch [9/10] batch [200/800] time 0.101 (0.097) data 0.000 (0.003) loss 0.0518 (0.3642) lr 4.1221e-04 eta 0:02:15\n",
            "epoch [9/10] batch [220/800] time 0.094 (0.097) data 0.000 (0.003) loss 1.1445 (0.3569) lr 4.1221e-04 eta 0:02:13\n",
            "epoch [9/10] batch [240/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.0007 (0.4087) lr 4.1221e-04 eta 0:02:11\n",
            "epoch [9/10] batch [260/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0089 (0.4197) lr 4.1221e-04 eta 0:02:09\n",
            "epoch [9/10] batch [280/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0082 (0.4025) lr 4.1221e-04 eta 0:02:07\n",
            "epoch [9/10] batch [300/800] time 0.096 (0.096) data 0.000 (0.002) loss 0.0002 (0.3976) lr 4.1221e-04 eta 0:02:05\n",
            "epoch [9/10] batch [320/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.3176 (0.4380) lr 4.1221e-04 eta 0:02:02\n",
            "epoch [9/10] batch [340/800] time 0.100 (0.096) data 0.008 (0.002) loss 0.0056 (0.4352) lr 4.1221e-04 eta 0:02:01\n",
            "epoch [9/10] batch [360/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0020 (0.4346) lr 4.1221e-04 eta 0:01:59\n",
            "epoch [9/10] batch [380/800] time 0.094 (0.096) data 0.001 (0.002) loss 1.8086 (0.4528) lr 4.1221e-04 eta 0:01:57\n",
            "epoch [9/10] batch [400/800] time 0.094 (0.096) data 0.000 (0.002) loss 2.1719 (0.4587) lr 4.1221e-04 eta 0:01:55\n",
            "epoch [9/10] batch [420/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.1096 (0.4543) lr 4.1221e-04 eta 0:01:53\n",
            "epoch [9/10] batch [440/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0003 (0.4427) lr 4.1221e-04 eta 0:01:51\n",
            "epoch [9/10] batch [460/800] time 0.098 (0.096) data 0.000 (0.002) loss 0.0269 (0.4353) lr 4.1221e-04 eta 0:01:49\n",
            "epoch [9/10] batch [480/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0001 (0.4392) lr 4.1221e-04 eta 0:01:47\n",
            "epoch [9/10] batch [500/800] time 0.095 (0.096) data 0.001 (0.002) loss 1.2129 (0.4447) lr 4.1221e-04 eta 0:01:45\n",
            "epoch [9/10] batch [520/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.4404 (0.4390) lr 4.1221e-04 eta 0:01:43\n",
            "epoch [9/10] batch [540/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0013 (0.4295) lr 4.1221e-04 eta 0:01:41\n",
            "epoch [9/10] batch [560/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0004 (0.4224) lr 4.1221e-04 eta 0:01:39\n",
            "epoch [9/10] batch [580/800] time 0.102 (0.096) data 0.006 (0.002) loss 0.0333 (0.4302) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [600/800] time 0.094 (0.096) data 0.001 (0.002) loss 0.0007 (0.4325) lr 4.1221e-04 eta 0:01:35\n",
            "epoch [9/10] batch [620/800] time 0.097 (0.096) data 0.000 (0.002) loss 0.2629 (0.4238) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [9/10] batch [640/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.2394 (0.4211) lr 4.1221e-04 eta 0:01:31\n",
            "epoch [9/10] batch [660/800] time 0.093 (0.096) data 0.001 (0.002) loss 0.0483 (0.4164) lr 4.1221e-04 eta 0:01:29\n",
            "epoch [9/10] batch [680/800] time 0.095 (0.096) data 0.000 (0.002) loss 0.0185 (0.4114) lr 4.1221e-04 eta 0:01:27\n",
            "epoch [9/10] batch [700/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0024 (0.4205) lr 4.1221e-04 eta 0:01:25\n",
            "epoch [9/10] batch [720/800] time 0.098 (0.096) data 0.001 (0.002) loss 0.0006 (0.4190) lr 4.1221e-04 eta 0:01:24\n",
            "epoch [9/10] batch [740/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.5806 (0.4222) lr 4.1221e-04 eta 0:01:22\n",
            "epoch [9/10] batch [760/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0680 (0.4382) lr 4.1221e-04 eta 0:01:20\n",
            "epoch [9/10] batch [780/800] time 0.092 (0.095) data 0.000 (0.002) loss 0.0007 (0.4422) lr 4.1221e-04 eta 0:01:18\n",
            "epoch [9/10] batch [800/800] time 0.087 (0.095) data 0.000 (0.001) loss 1.2080 (0.4426) lr 1.9098e-04 eta 0:01:16\n",
            "epoch [10/10] batch [20/800] time 0.095 (0.114) data 0.000 (0.017) loss 0.0004 (0.4801) lr 1.9098e-04 eta 0:01:28\n",
            "epoch [10/10] batch [40/800] time 0.101 (0.106) data 0.007 (0.010) loss 0.0002 (0.7265) lr 1.9098e-04 eta 0:01:20\n",
            "epoch [10/10] batch [60/800] time 0.095 (0.103) data 0.001 (0.007) loss 5.5664 (0.7052) lr 1.9098e-04 eta 0:01:16\n",
            "epoch [10/10] batch [80/800] time 0.094 (0.101) data 0.000 (0.006) loss 0.0015 (0.5778) lr 1.9098e-04 eta 0:01:12\n",
            "epoch [10/10] batch [100/800] time 0.094 (0.100) data 0.001 (0.005) loss 0.0864 (0.5675) lr 1.9098e-04 eta 0:01:09\n",
            "epoch [10/10] batch [120/800] time 0.094 (0.099) data 0.001 (0.004) loss 0.0027 (0.5125) lr 1.9098e-04 eta 0:01:07\n",
            "epoch [10/10] batch [140/800] time 0.094 (0.098) data 0.001 (0.004) loss 0.0023 (0.4812) lr 1.9098e-04 eta 0:01:04\n",
            "epoch [10/10] batch [160/800] time 0.096 (0.098) data 0.001 (0.003) loss 0.4241 (0.4786) lr 1.9098e-04 eta 0:01:02\n",
            "epoch [10/10] batch [180/800] time 0.094 (0.098) data 0.001 (0.003) loss 0.1201 (0.4631) lr 1.9098e-04 eta 0:01:00\n",
            "epoch [10/10] batch [200/800] time 0.095 (0.098) data 0.000 (0.003) loss 0.0097 (0.4272) lr 1.9098e-04 eta 0:00:58\n",
            "epoch [10/10] batch [220/800] time 0.094 (0.098) data 0.000 (0.003) loss 1.2715 (0.4356) lr 1.9098e-04 eta 0:00:56\n",
            "epoch [10/10] batch [240/800] time 0.094 (0.097) data 0.000 (0.003) loss 0.6523 (0.4621) lr 1.9098e-04 eta 0:00:54\n",
            "epoch [10/10] batch [260/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.1082 (0.4528) lr 1.9098e-04 eta 0:00:52\n",
            "epoch [10/10] batch [280/800] time 0.094 (0.097) data 0.001 (0.002) loss 0.0652 (0.4527) lr 1.9098e-04 eta 0:00:50\n",
            "epoch [10/10] batch [300/800] time 0.105 (0.097) data 0.000 (0.002) loss 3.8789 (0.5083) lr 1.9098e-04 eta 0:00:48\n",
            "epoch [10/10] batch [320/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.0023 (0.4963) lr 1.9098e-04 eta 0:00:46\n",
            "epoch [10/10] batch [340/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.0004 (0.4875) lr 1.9098e-04 eta 0:00:44\n",
            "epoch [10/10] batch [360/800] time 0.093 (0.097) data 0.000 (0.002) loss 0.4417 (0.4894) lr 1.9098e-04 eta 0:00:42\n",
            "epoch [10/10] batch [380/800] time 0.096 (0.096) data 0.000 (0.002) loss 0.0011 (0.4848) lr 1.9098e-04 eta 0:00:40\n",
            "epoch [10/10] batch [400/800] time 0.093 (0.096) data 0.000 (0.002) loss 2.4727 (0.4827) lr 1.9098e-04 eta 0:00:38\n",
            "epoch [10/10] batch [420/800] time 0.099 (0.096) data 0.010 (0.002) loss 0.0000 (0.4815) lr 1.9098e-04 eta 0:00:36\n",
            "epoch [10/10] batch [440/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0000 (0.4648) lr 1.9098e-04 eta 0:00:34\n",
            "epoch [10/10] batch [460/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0027 (0.4487) lr 1.9098e-04 eta 0:00:32\n",
            "epoch [10/10] batch [480/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0004 (0.4439) lr 1.9098e-04 eta 0:00:30\n",
            "epoch [10/10] batch [500/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0422 (0.4404) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [520/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0029 (0.4467) lr 1.9098e-04 eta 0:00:26\n",
            "epoch [10/10] batch [540/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.0004 (0.4463) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [10/10] batch [560/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0172 (0.4413) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [580/800] time 0.093 (0.096) data 0.000 (0.002) loss 1.7217 (0.4298) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [600/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0070 (0.4196) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [620/800] time 0.092 (0.096) data 0.000 (0.002) loss 0.3518 (0.4205) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [640/800] time 0.095 (0.096) data 0.001 (0.002) loss 0.0378 (0.4217) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [660/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0827 (0.4249) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [680/800] time 0.102 (0.096) data 0.002 (0.002) loss 0.0024 (0.4340) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [700/800] time 0.094 (0.096) data 0.000 (0.002) loss 0.0062 (0.4460) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [720/800] time 0.093 (0.096) data 0.000 (0.002) loss 0.0062 (0.4383) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [740/800] time 0.093 (0.095) data 0.000 (0.001) loss 0.0004 (0.4305) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [760/800] time 0.094 (0.095) data 0.001 (0.001) loss 0.0001 (0.4300) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [780/800] time 0.094 (0.095) data 0.000 (0.001) loss 1.6279 (0.4338) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [800/800] time 0.091 (0.095) data 0.000 (0.001) loss 0.3582 (0.4344) lr 4.8943e-05 eta 0:00:00\n",
            "Checkpoint saved to output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 16/16 [01:05<00:00,  4.07s/it]\n",
            "=> result\n",
            "* total: 1,549\n",
            "* correct: 1,515\n",
            "* accuracy: 97.8%\n",
            "* error: 2.2%\n",
            "* macro_f1: 95.6%\n",
            "Elapsed: 0:13:52\n",
            "2025-03-23 18:26:05.541572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742754365.561992    7299 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742754365.568272    7299 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 18:26:05.589159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: True\n",
            "head: \n",
            "load_epoch: 10\n",
            "model_dir: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
            "output_dir: output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoCoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: new\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: a photo of a\n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoCoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.36\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoCoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Loading preprocessed few-shot data from /content/datasets/caltech-101/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE NEW CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  50\n",
            "# train_x  800\n",
            "# val      200\n",
            "# test     916\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"a photo of a\"\n",
            "Number of context words (tokens): 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear2.weight', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear1.weight'}\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "Unable to load checkpoint from \"output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1/prompt_learner/model.pth.tar-10\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CoOp/train.py\", line 207, in <module>\n",
            "    main(args)\n",
            "  File \"/content/CoOp/train.py\", line 145, in main\n",
            "    trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
            "  File \"/content/CoOp/trainers/cocoop.py\", line 302, in load_model\n",
            "    checkpoint = load_checkpoint(model_path)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dassl/utils/torchtools.py\", line 102, in load_checkpoint\n",
            "    checkpoint = torch.load(fpath, map_location=map_location)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL torch.optim.lr_scheduler.CosineAnnealingLR was not an allowed global by default. Please use `torch.serialization.add_safe_globals([CosineAnnealingLR])` or the `torch.serialization.safe_globals([CosineAnnealingLR])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./base2new_test.sh caltech101 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdL0sfX6QuIo",
        "outputId": "d0fa7ba3-b76b-459b-87c0-fb1ead5bd2c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-23 18:31:54.761350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742754714.782265    8833 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742754714.788713    8833 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 18:31:54.809864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: True\n",
            "head: \n",
            "load_epoch: 10\n",
            "model_dir: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
            "output_dir: output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoCoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: new\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: a photo of a\n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoCoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.36\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoCoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Loading preprocessed few-shot data from /content/datasets/caltech-101/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE NEW CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  50\n",
            "# train_x  800\n",
            "# val      200\n",
            "# test     916\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"a photo of a\"\n",
            "Number of context words (tokens): 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.ctx'}\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "Unable to load checkpoint from \"output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1/prompt_learner/model.pth.tar-10\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CoOp/train.py\", line 207, in <module>\n",
            "    main(args)\n",
            "  File \"/content/CoOp/train.py\", line 145, in main\n",
            "    trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
            "  File \"/content/CoOp/trainers/cocoop.py\", line 302, in load_model\n",
            "    checkpoint = load_checkpoint(model_path)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dassl/utils/torchtools.py\", line 102, in load_checkpoint\n",
            "    checkpoint = torch.load(fpath, map_location=map_location)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL torch.optim.lr_scheduler.CosineAnnealingLR was not an allowed global by default. Please use `torch.serialization.add_safe_globals([CosineAnnealingLR])` or the `torch.serialization.safe_globals([CosineAnnealingLR])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2kCbfPnQ6QR",
        "outputId": "9964b7d7-ce88-4864-b0de-69fc574e500d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse_test_res.py output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uod-s9XHK5A5",
        "outputId": "044d951a-cd9a-4521-e92e-55a737c0fe1b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1\n",
            "file: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1/seed1/log.txt. accuracy: 97.80%. \n",
            "===\n",
            "Summary of directory: output/base2new/train_base/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1\n",
            "* accuracy: 97.80% +- 0.00%\n",
            "===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse_test_res.py output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exhz1zIQROF8",
        "outputId": "469f79dc-1d2a-4efc-bba3-c4cef2b6632d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CoOp/parse_test_res.py\", line 174, in <module>\n",
            "    main(args, end_signal)\n",
            "  File \"/content/CoOp/parse_test_res.py\", line 150, in main\n",
            "    parse_function(\n",
            "  File \"/content/CoOp/parse_test_res.py\", line 97, in parse_function\n",
            "    assert len(outputs) > 0, f\"Nothing found in {directory}\"\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "AssertionError: Nothing found in output/base2new/test_new/caltech101/shots_16/CoCoOp/vit_b16_c4_ep10_batch1_ctxv1\n"
          ]
        }
      ]
    }
  ]
}