{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "znVHVI09IOQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "import gdown\n",
        "import json\n",
        "import PIL\n",
        "\n",
        "def download_ucf101(root, download):\n",
        "    torchvision.datasets.utils.download_and_extract_archive('https://drive.google.com/file/d/10Jqome3vtUA2keJkNanAiFpgbyC9Hc2O', os.path.join(root, 'ucf101'), filename='UCF-101-midframes.zip')\n",
        "\n",
        "# Split datasets (train, val, test) according to https://github.com/KaiyangZhou/CoOp/blob/main/DATASETS.md\n",
        "# folder, img folder, json file in Google drive\n",
        "datasets_list = {\n",
        "    'caltech101': (torchvision.datasets.Caltech101,     'caltech101',       '101_ObjectCategories', '1hyarUivQE36mY6jSomru6Fjd-JzwcCzN'),\n",
        "    'oxfordpets': (torchvision.datasets.OxfordIIITPet,  'oxford-iiit-pet',  'images',               '1501r8Ber4nNKvmlFVQZ8SeUHTcdTTEqs'),\n",
        "    'flowers102': (torchvision.datasets.Flowers102,     'flowers-102',      'jpg',                  '1Pp0sRXzZFZq15zVOzKjKBu4A9i01nozT'),\n",
        "    'food101'   : (torchvision.datasets.Food101,        'food-101',         'images',               '1QK0tGi096I0Ba6kggatX1ee6dJFIcEJl'),\n",
        "    'dtd'       : (torchvision.datasets.DTD,            'dtd', os.path.join('dtd', 'images'),       '1u3_QfB467jqHgNXC00UIzbLZRQCg2S7x'),\n",
        "    'eurosat'   : (torchvision.datasets.EuroSAT,        'eurosat',          '2750',                 '1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o'),\n",
        "    'ucf101'    : (download_ucf101,                     'ucf101',           'UCF-101-midframes',    '1I0S0q91hJfsV9Gf4xDIjgDq4AqBNJb1y'),\n",
        "}\n",
        "\n",
        "\n",
        "class AMLDataset(torchvision.datasets.VisionDataset):\n",
        "    def __init__(self, dataset_name, root, split: str='train', transforms=None, transform=None, target_transform=None):\n",
        "        dataset_info = datasets_list[dataset_name]\n",
        "\n",
        "        # Download the dataset with the help of torchvision.datasets object\n",
        "        dataset_info[0](root, download=True)\n",
        "\n",
        "        # Since torchvision.datasets put data inside a subfolder, we change path into this new root folder, and store everything inside it\n",
        "        root = os.path.join(root, dataset_info[1])\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "\n",
        "        # Images are further inside the new root folder\n",
        "        self.img_folder = os.path.join(root, dataset_info[2])\n",
        "\n",
        "        # Download json inside the new root folder\n",
        "        split_file_path = os.path.join(root, 'split.json')\n",
        "        if not os.path.exists(split_file_path):\n",
        "            gdown.download(id=dataset_info[3], output=split_file_path)\n",
        "\n",
        "        # Read json file, resulting in a dict[str('train', 'val', 'test'), list[str(impath), int(label), str(classname)]]\n",
        "        with open(split_file_path, 'r') as f:\n",
        "            data_source = json.load(f)\n",
        "\n",
        "        self._items = data_source[split]\n",
        "        self._num_classes = self.get_num_classes(data_source['train'])\n",
        "        self._lab2cname, self._classnames = self.get_lab2cname(data_source['train'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._items)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        impath, label, classname = self._items[index]\n",
        "\n",
        "        img = PIL.Image.open(os.path.join(self.img_folder, impath))\n",
        "\n",
        "        # if self.transforms is not None:\n",
        "        #     img, label = self.transforms(img, label)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    @property\n",
        "    def lab2cname(self):\n",
        "        return self._lab2cname\n",
        "\n",
        "    @property\n",
        "    def classnames(self):\n",
        "        return self._classnames\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return self._num_classes\n",
        "\n",
        "    @staticmethod\n",
        "    def get_num_classes(data_source):\n",
        "        \"\"\"Count number of classes.\n",
        "\n",
        "        Args:\n",
        "            data_source (list): a list of Datum objects.\n",
        "        \"\"\"\n",
        "        label_set = set()\n",
        "        for impath, label, classname in data_source:\n",
        "            label_set.add(label)\n",
        "        return max(label_set) + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_lab2cname(data_source):\n",
        "        \"\"\"Get a label-to-classname mapping (dict).\n",
        "\n",
        "        Args:\n",
        "            data_source (list): a list of Datum objects.\n",
        "        \"\"\"\n",
        "        container = set()\n",
        "        for impath, label, classname in data_source:\n",
        "            container.add((label, classname))\n",
        "        mapping = {label: classname for label, classname in container}\n",
        "        labels = list(mapping.keys())\n",
        "        labels.sort()\n",
        "        classnames = [mapping[label] for label in labels]\n",
        "        return mapping, classnames\n",
        "\n",
        "\n",
        "class Caltech101(AMLDataset):\n",
        "    def __init__(self, root, *args, **kwargs):\n",
        "        from urllib.error import HTTPError\n",
        "        try:\n",
        "            super().__init__('caltech101', root, *args, **kwargs)\n",
        "        except HTTPError:\n",
        "            # Use the copy hosted by Terry\n",
        "            from torchvision.datasets.utils import download_and_extract_archive\n",
        "            download_and_extract_archive(\n",
        "                'https://drive.google.com/file/d/1IFqrvpdbrpmI6DPntopcPY6svPu04uYD',\n",
        "                os.path.join(root, 'caltech101'),\n",
        "                filename='101_ObjectCategories.tar.gz',\n",
        "                md5='b224c7392d521a49829488ab0f1120d9',\n",
        "            )\n",
        "            download_and_extract_archive(\n",
        "                'https://drive.google.com/file/d/1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d',\n",
        "                os.path.join(root, 'caltech101'),\n",
        "                filename='Annotations.tar',\n",
        "                md5='6f83eeb1f24d99cab4eb377263132c91',\n",
        "            )\n",
        "            super().__init__('caltech101', root, *args, **kwargs)\n",
        "\n",
        "\n",
        "class OxfordIIITPet(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('oxfordpets', *args, **kwargs)\n",
        "\n",
        "\n",
        "class Flowers102(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('flowers102', *args, **kwargs)\n",
        "\n",
        "\n",
        "class Food101(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('food101', *args, **kwargs)\n",
        "\n",
        "\n",
        "class DTD(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('dtd', *args, **kwargs)\n",
        "\n",
        "\n",
        "class EuroSAT(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('eurosat', *args, **kwargs)\n",
        "\n",
        "\n",
        "class UCF101(AMLDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__('ucf101', *args, **kwargs)\n",
        "\n",
        "\n",
        "class FGVCAircraft(torchvision.datasets.FGVCAircraft):\n",
        "    def __init__(self, root, *args, **kwargs):\n",
        "        super().__init__(root, *args, download=True, **kwargs)\n",
        "        self._lab2cname = {i: self.classes[i] for i in range(len(self.classes))}\n",
        "\n",
        "    @property\n",
        "    def lab2cname(self):\n",
        "        return self._lab2cname\n",
        "\n",
        "    @property\n",
        "    def classnames(self):\n",
        "        return self.classes\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.classes)"
      ],
      "metadata": {
        "id": "QKRvTaoyJCPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "i64CRsdDJELj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/datasets\""
      ],
      "metadata": {
        "id": "HDVAVyBAJGQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bsvj9mARGUVz",
        "outputId": "5edc6303-1f30-4f62-cb42-6048daa2bb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rvw1ymmn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rvw1ymmn\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=f9f4dd316c64931e059ee167ba0d54e91b9d095226fea090f65f26cdbadda764\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cs36r_um/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import random_split\n",
        "import clip\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# Load Caltech101 dataset\n",
        "transform = preprocess\n",
        "train_set = Caltech101(root=dataset_path,split='test', transform=preprocess)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uugj-Uy9Hyxa",
        "outputId": "4b40881b-2c00-4d22-faff-56dfe965b2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 81.3MiB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=1IFqrvpdbrpmI6DPntopcPY6svPu04uYD&confirm=t&uuid=eae6d48f-e277-474d-b8eb-26bfd261b7ae\n",
            "To: /content/datasets/caltech101/101_ObjectCategories.tar.gz\n",
            "100%|██████████| 132M/132M [00:03<00:00, 42.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=1sW96Lj6yLIujKpopd8tBrIO_NCaKBy5d&confirm=t&uuid=a83a6862-2a95-47c4-b861-31f1d24037c9\n",
            "To: /content/datasets/caltech101/Annotations.tar\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 27.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/datasets/caltech101/split.json\n",
            "100%|██████████| 809k/809k [00:00<00:00, 8.64MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=8)\n"
      ],
      "metadata": {
        "id": "cRYMV40Au2iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = int(0.8 * len(train_set))\n",
        "num_val = len(train_set) - num_train\n",
        "train_dataset, val_dataset = random_split(train_set, [num_train, num_val])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGVnFROrdu9a",
        "outputId": "1bac658f-5420-42f7-d5cc-b536ad6dfffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classnames = train_set.classnames"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eB7QBFS3KH6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ],
      "metadata": {
        "id": "InGecH8rSHYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, n_ctx=16, ctx_init=\"\", class_token_position=\"end\", csc=False, input_size=224):\n",
        "        super().__init__()\n",
        "        self.n_cls = len(classnames)\n",
        "        self.n_ctx = n_ctx\n",
        "        self.ctx_init = ctx_init\n",
        "        self.class_token_position = class_token_position\n",
        "        self.csc = csc\n",
        "        self.input_size = input_size\n",
        "\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        device = clip_model.token_embedding.weight.device\n",
        "\n",
        "        assert self.input_size == clip_imsize, f\"cfg_imsize ({self.input_size}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        if self.ctx_init:\n",
        "            ctx_init = self.ctx_init.replace(\"_\", \" \")\n",
        "            self.n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + self.n_ctx, :].to(device)\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if self.csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(self.n_cls, self.n_ctx, ctx_dim, dtype=dtype, device=device)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(self.n_ctx, ctx_dim, dtype=dtype, device=device)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * self.n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words (tokens): {self.n_ctx}\")\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        self.name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + self.n_ctx :, :])\n",
        "\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "\n",
        "    def forward(self):\n",
        "        ctx = self.ctx\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat([prefix, ctx, suffix], dim=1)\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat([prefix_i, ctx_i_half1, class_i, ctx_i_half2, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat([prefix_i, class_i, ctx_i, suffix_i], dim=1)\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid class_token_position: {self.class_token_position}\")\n",
        "\n",
        "        return prompts, self.tokenized_prompts"
      ],
      "metadata": {
        "id": "fbebzKV3SemG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = PromptLearner(\n",
        "            classnames=classnames,\n",
        "            clip_model=clip_model,\n",
        "            n_ctx=16,\n",
        "            ctx_init=\"\",\n",
        "            csc=False,\n",
        "            class_token_position=\"end\",\n",
        "            input_size=224\n",
        "        )\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        prompts, tokenized_prompts = self.prompt_learner()\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ORtK7xf5SgNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "model = CustomCLIP(classnames=classnames, clip_model=clip_model).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cLfC2tMczTa",
        "outputId": "66696bac-738d-4bfa-e926-124105092685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_EPOCH = 10\n",
        "LR = 0.002\n",
        "\n",
        "optimizer = optim.SGD(model.prompt_learner.parameters(), lr=LR, momentum=0.9)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=MAX_EPOCH)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "uqh2iJwDa8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_prompt_learner(model, path=\"output/coop_prompt.pth\"):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save({\"state_dict\": model.prompt_learner.state_dict()}, path)\n",
        "    print(f\"Prompt learner saved to {path}\")"
      ],
      "metadata": {
        "id": "PeJr11TTg07x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRINT_FREQ = 5\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i + 1) % PRINT_FREQ == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{MAX_EPOCH}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    scheduler.step()\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {100.0 * correct / total:.2f}%\")\n",
        "save_prompt_learner(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqze6czSa-U6",
        "outputId": "13159abb-4eaa-4e58-c135-ebec8158269a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [5/62], Loss: 0.0172\n",
            "Epoch [1/10], Step [10/62], Loss: 0.0515\n",
            "Epoch [1/10], Step [15/62], Loss: 0.1022\n",
            "Epoch [1/10], Step [20/62], Loss: 0.0741\n",
            "Epoch [1/10], Step [25/62], Loss: 0.0239\n",
            "Epoch [1/10], Step [30/62], Loss: 0.0173\n",
            "Epoch [1/10], Step [35/62], Loss: 0.1002\n",
            "Epoch [1/10], Step [40/62], Loss: 0.1300\n",
            "Epoch [1/10], Step [45/62], Loss: 0.1168\n",
            "Epoch [1/10], Step [50/62], Loss: 0.2025\n",
            "Epoch [1/10], Step [55/62], Loss: 0.0912\n",
            "Epoch [1/10], Step [60/62], Loss: 0.1484\n",
            "Epoch 1: Train Loss = 6.0818, Accuracy = 96.45%\n",
            "Epoch [2/10], Step [5/62], Loss: 0.0341\n",
            "Epoch [2/10], Step [10/62], Loss: 0.1082\n",
            "Epoch [2/10], Step [15/62], Loss: 0.0670\n",
            "Epoch [2/10], Step [20/62], Loss: 0.2025\n",
            "Epoch [2/10], Step [25/62], Loss: 0.0565\n",
            "Epoch [2/10], Step [30/62], Loss: 0.1329\n",
            "Epoch [2/10], Step [35/62], Loss: 0.1022\n",
            "Epoch [2/10], Step [40/62], Loss: 0.0895\n",
            "Epoch [2/10], Step [45/62], Loss: 0.0719\n",
            "Epoch [2/10], Step [50/62], Loss: 0.1646\n",
            "Epoch [2/10], Step [55/62], Loss: 0.0844\n",
            "Epoch [2/10], Step [60/62], Loss: 0.1270\n",
            "Epoch 2: Train Loss = 5.4145, Accuracy = 97.36%\n",
            "Epoch [3/10], Step [5/62], Loss: 0.0738\n",
            "Epoch [3/10], Step [10/62], Loss: 0.1154\n",
            "Epoch [3/10], Step [15/62], Loss: 0.0967\n",
            "Epoch [3/10], Step [20/62], Loss: 0.0934\n",
            "Epoch [3/10], Step [25/62], Loss: 0.0645\n",
            "Epoch [3/10], Step [30/62], Loss: 0.0296\n",
            "Epoch [3/10], Step [35/62], Loss: 0.0814\n",
            "Epoch [3/10], Step [40/62], Loss: 0.0124\n",
            "Epoch [3/10], Step [45/62], Loss: 0.0458\n",
            "Epoch [3/10], Step [50/62], Loss: 0.0127\n",
            "Epoch [3/10], Step [55/62], Loss: 0.0294\n",
            "Epoch [3/10], Step [60/62], Loss: 0.1147\n",
            "Epoch 3: Train Loss = 5.1343, Accuracy = 97.06%\n",
            "Epoch [4/10], Step [5/62], Loss: 0.0213\n",
            "Epoch [4/10], Step [10/62], Loss: 0.1227\n",
            "Epoch [4/10], Step [15/62], Loss: 0.0505\n",
            "Epoch [4/10], Step [20/62], Loss: 0.1562\n",
            "Epoch [4/10], Step [25/62], Loss: 0.1818\n",
            "Epoch [4/10], Step [30/62], Loss: 0.0461\n",
            "Epoch [4/10], Step [35/62], Loss: 0.1465\n",
            "Epoch [4/10], Step [40/62], Loss: 0.0179\n",
            "Epoch [4/10], Step [45/62], Loss: 0.0344\n",
            "Epoch [4/10], Step [50/62], Loss: 0.0457\n",
            "Epoch [4/10], Step [55/62], Loss: 0.0167\n",
            "Epoch [4/10], Step [60/62], Loss: 0.0577\n",
            "Epoch 4: Train Loss = 4.5908, Accuracy = 97.72%\n",
            "Epoch [5/10], Step [5/62], Loss: 0.0681\n",
            "Epoch [5/10], Step [10/62], Loss: 0.1309\n",
            "Epoch [5/10], Step [15/62], Loss: 0.0291\n",
            "Epoch [5/10], Step [20/62], Loss: 0.0720\n",
            "Epoch [5/10], Step [25/62], Loss: 0.0285\n",
            "Epoch [5/10], Step [30/62], Loss: 0.0793\n",
            "Epoch [5/10], Step [35/62], Loss: 0.1346\n",
            "Epoch [5/10], Step [40/62], Loss: 0.0289\n",
            "Epoch [5/10], Step [45/62], Loss: 0.0440\n",
            "Epoch [5/10], Step [50/62], Loss: 0.0201\n",
            "Epoch [5/10], Step [55/62], Loss: 0.0651\n",
            "Epoch [5/10], Step [60/62], Loss: 0.0418\n",
            "Epoch 5: Train Loss = 4.1382, Accuracy = 97.97%\n",
            "Epoch [6/10], Step [5/62], Loss: 0.0663\n",
            "Epoch [6/10], Step [10/62], Loss: 0.0544\n",
            "Epoch [6/10], Step [15/62], Loss: 0.0540\n",
            "Epoch [6/10], Step [20/62], Loss: 0.0373\n",
            "Epoch [6/10], Step [25/62], Loss: 0.0200\n",
            "Epoch [6/10], Step [30/62], Loss: 0.0884\n",
            "Epoch [6/10], Step [35/62], Loss: 0.0120\n",
            "Epoch [6/10], Step [40/62], Loss: 0.0731\n",
            "Epoch [6/10], Step [45/62], Loss: 0.0312\n",
            "Epoch [6/10], Step [50/62], Loss: 0.0308\n",
            "Epoch [6/10], Step [55/62], Loss: 0.1304\n",
            "Epoch [6/10], Step [60/62], Loss: 0.0566\n",
            "Epoch 6: Train Loss = 3.6297, Accuracy = 98.28%\n",
            "Epoch [7/10], Step [5/62], Loss: 0.0207\n",
            "Epoch [7/10], Step [10/62], Loss: 0.0253\n",
            "Epoch [7/10], Step [15/62], Loss: 0.0554\n",
            "Epoch [7/10], Step [20/62], Loss: 0.1063\n",
            "Epoch [7/10], Step [25/62], Loss: 0.0974\n",
            "Epoch [7/10], Step [30/62], Loss: 0.0659\n",
            "Epoch [7/10], Step [35/62], Loss: 0.0823\n",
            "Epoch [7/10], Step [40/62], Loss: 0.0429\n",
            "Epoch [7/10], Step [45/62], Loss: 0.0855\n",
            "Epoch [7/10], Step [50/62], Loss: 0.0119\n",
            "Epoch [7/10], Step [55/62], Loss: 0.0301\n",
            "Epoch [7/10], Step [60/62], Loss: 0.1633\n",
            "Epoch 7: Train Loss = 3.2850, Accuracy = 98.78%\n",
            "Epoch [8/10], Step [5/62], Loss: 0.0460\n",
            "Epoch [8/10], Step [10/62], Loss: 0.0242\n",
            "Epoch [8/10], Step [15/62], Loss: 0.0229\n",
            "Epoch [8/10], Step [20/62], Loss: 0.0565\n",
            "Epoch [8/10], Step [25/62], Loss: 0.0446\n",
            "Epoch [8/10], Step [30/62], Loss: 0.0313\n",
            "Epoch [8/10], Step [35/62], Loss: 0.1064\n",
            "Epoch [8/10], Step [40/62], Loss: 0.0397\n",
            "Epoch [8/10], Step [45/62], Loss: 0.0171\n",
            "Epoch [8/10], Step [50/62], Loss: 0.0334\n",
            "Epoch [8/10], Step [55/62], Loss: 0.0374\n",
            "Epoch [8/10], Step [60/62], Loss: 0.0905\n",
            "Epoch 8: Train Loss = 3.0360, Accuracy = 98.73%\n",
            "Epoch [9/10], Step [5/62], Loss: 0.0129\n",
            "Epoch [9/10], Step [10/62], Loss: 0.0210\n",
            "Epoch [9/10], Step [15/62], Loss: 0.0561\n",
            "Epoch [9/10], Step [20/62], Loss: 0.0854\n",
            "Epoch [9/10], Step [25/62], Loss: 0.0065\n",
            "Epoch [9/10], Step [30/62], Loss: 0.0242\n",
            "Epoch [9/10], Step [35/62], Loss: 0.0241\n",
            "Epoch [9/10], Step [40/62], Loss: 0.0898\n",
            "Epoch [9/10], Step [45/62], Loss: 0.0358\n",
            "Epoch [9/10], Step [50/62], Loss: 0.0384\n",
            "Epoch [9/10], Step [55/62], Loss: 0.1017\n",
            "Epoch [9/10], Step [60/62], Loss: 0.0275\n",
            "Epoch 9: Train Loss = 2.8223, Accuracy = 99.09%\n",
            "Epoch [10/10], Step [5/62], Loss: 0.0489\n",
            "Epoch [10/10], Step [10/62], Loss: 0.0430\n",
            "Epoch [10/10], Step [15/62], Loss: 0.0405\n",
            "Epoch [10/10], Step [20/62], Loss: 0.0254\n",
            "Epoch [10/10], Step [25/62], Loss: 0.0547\n",
            "Epoch [10/10], Step [30/62], Loss: 0.0404\n",
            "Epoch [10/10], Step [35/62], Loss: 0.0189\n",
            "Epoch [10/10], Step [40/62], Loss: 0.0273\n",
            "Epoch [10/10], Step [45/62], Loss: 0.0280\n",
            "Epoch [10/10], Step [50/62], Loss: 0.0269\n",
            "Epoch [10/10], Step [55/62], Loss: 0.0211\n",
            "Epoch [10/10], Step [60/62], Loss: 0.0751\n",
            "Epoch 10: Train Loss = 2.6921, Accuracy = 99.24%\n",
            "Validation Accuracy: 95.33%\n",
            "Prompt learner saved to output/coop_prompt.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KaiyangZhou/CoOp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-YpILCpiLnP",
        "outputId": "bc209b69-769b-45ea-c89f-a22f1096f40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 199 (delta 199), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 19.65 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CoOp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigoPhUriSKf",
        "outputId": "ae7e6e28-451d-44fe-b600-b4d330823a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python interpret_prompt.py /content/output/coop_prompt.pth 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI3y-lvZicmy",
        "outputId": "e0ef7231-790a-4178-e8f6-5dffcea90aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return the top-5 matched words\n",
            "100%|███████████████████████████████████████| 256M/256M [01:04<00:00, 3.97MiB/s]\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['weaknesses</w>', 'losses</w>', 'alright</w>', 'and', 'aaaaa</w>'] ['0.6468', '0.6477', '0.6496', '0.6518', '0.6522']\n",
            "2: ['troupe</w>', 'decatur</w>', 'aqu', 'arun</w>', 'katz</w>'] ['0.6009', '0.6046', '0.6048', '0.6059', '0.6063']\n",
            "3: ['bcfc</w>', 'phill', 'fri</w>', 'strike', 'fiancÃ©</w>'] ['0.5692', '0.5703', '0.5709', '0.5725', '0.5737']\n",
            "4: ['olives</w>', 'certain</w>', 'boi</w>', 'pelicans</w>', 'elles</w>'] ['0.5862', '0.5864', '0.5911', '0.5926', '0.5930']\n",
            "5: ['ophthal', 'mia</w>', 'kra', 'ials</w>', 'volcanoes</w>'] ['0.6959', '0.6996', '0.7025', '0.7026', '0.7038']\n",
            "6: ['worlds</w>', 'bas</w>', 'aff</w>', 'period</w>', 'litres</w>'] ['0.7474', '0.7508', '0.7516', '0.7530', '0.7535']\n",
            "7: ['terrible</w>', 'for', 'loses</w>', 'rip</w>', 'boa</w>'] ['0.5332', '0.5372', '0.5382', '0.5398', '0.5406']\n",
            "8: ['medi', 'hiphop</w>', 'ðŁĺŀ</w>', 'applies</w>', 'true</w>'] ['0.6509', '0.6516', '0.6539', '0.6547', '0.6563']\n",
            "9: ['necessity</w>', 'saki</w>', 'literacy</w>', 'shaping</w>', 'insist</w>'] ['0.7191', '0.7204', '0.7212', '0.7227', '0.7259']\n",
            "10: ['ones</w>', 'boxer</w>', 'ðŁĴªðŁı¼</w>', 'yes</w>', 'pompeii</w>'] ['0.5826', '0.5863', '0.5868', '0.5870', '0.5878']\n",
            "11: ['mushrooms</w>', 'pow', 'ap', 'pur', 'congrats</w>'] ['0.5856', '0.5870', '0.5887', '0.5888', '0.5892']\n",
            "12: ['yeah</w>', 'heres</w>', 'bha', 'opa</w>', 'ang</w>'] ['0.5535', '0.5555', '0.5561', '0.5577', '0.5578']\n",
            "13: ['alea</w>', 'ities</w>', 'scot</w>', 'ones</w>', 'university</w>'] ['0.6378', '0.6396', '0.6421', '0.6422', '0.6423']\n",
            "14: ['eon</w>', 'ze', 'medusa</w>', 'ðŁĮ¹', 'sizes</w>'] ['0.7327', '0.7339', '0.7354', '0.7362', '0.7396']\n",
            "15: ['olive', 'protec', 'ðŁĩµ', 'cymru</w>', 'uhur'] ['0.7745', '0.7751', '0.7766', '0.7791', '0.7794']\n",
            "16: ['snakes</w>', 'suits</w>', 'İ</w>', 'meaning</w>', 'racers</w>'] ['0.6818', '0.6819', '0.6839', '0.6839', '0.6847']\n"
          ]
        }
      ]
    }
  ]
}