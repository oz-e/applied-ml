{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oz-e/applied-ml/blob/main/CoOp_caltech_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/KaiyangZhou/Dassl.pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbh5uHW1uAer",
        "outputId": "5dbe0a71-a3e0-4238-c96d-e3d7b4bdf46c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
            "  Cloning https://github.com/KaiyangZhou/Dassl.pytorch.git to /tmp/pip-req-build-jx22u0ey\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KaiyangZhou/Dassl.pytorch.git /tmp/pip-req-build-jx22u0ey\n",
            "  Resolved https://github.com/KaiyangZhou/Dassl.pytorch.git to commit c61a1b570ac6333bd50fb5ae06aea59002fb20bb\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8==3.7.9 (from dassl==0.6.3)\n",
            "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting yapf==0.29.0 (from dassl==0.6.3)\n",
            "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting isort==4.3.21 (from dassl==0.6.3)\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting yacs (from dassl==0.6.3)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (5.2.0)\n",
            "Collecting tb-nightly (from dassl==0.6.3)\n",
            "  Downloading tb_nightly-2.20.0a20250323-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (4.67.1)\n",
            "Collecting ftfy (from dassl==0.6.3)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (2024.11.6)\n",
            "Collecting wilds==1.2.2 (from dassl==0.6.3)\n",
            "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from dassl==0.6.3) (0.9.0)\n",
            "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->dassl==0.6.3)\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.0.2)\n",
            "Collecting ogb>=1.2.6 (from wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting outdated>=0.2.0 (from wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (11.1.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2025.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from wilds==1.2.2->dassl==0.6.3) (0.21.0+cu124)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dassl==0.6.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dassl==0.6.3) (3.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->dassl==0.6.3) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown->dassl==0.6.3) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (5.29.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->dassl==0.6.3) (3.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs->dassl==0.6.3) (6.0.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb>=1.2.6->wilds==1.2.2->dassl==0.6.3) (2.3.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->wilds==1.2.2->dassl==0.6.3) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->wilds==1.2.2->dassl==0.6.3) (2025.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->dassl==0.6.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tb-nightly->dassl==0.6.3) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->dassl==0.6.3) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->dassl==0.6.3) (1.7.1)\n",
            "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tb_nightly-2.20.0a20250323-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Building wheels for collected packages: dassl\n",
            "  Building wheel for dassl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dassl: filename=dassl-0.6.3-py3-none-any.whl size=138529 sha256=2d71e4cc66be662fe6edcd5adb393d0febc7140000463d7a566509fbde4b1b0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1sb_miq9/wheels/ff/23/48/2796fd9e0be04cddeae2896067958443ac0eda066bce445caf\n",
            "Successfully built dassl\n",
            "Installing collected packages: yapf, mccabe, yacs, pyflakes, pycodestyle, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, littleutils, isort, ftfy, entrypoints, tb-nightly, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, flake8, nvidia-cusolver-cu12, ogb, wilds, dassl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: entrypoints\n",
            "    Found existing installation: entrypoints 0.4\n",
            "    Uninstalling entrypoints-0.4:\n",
            "      Successfully uninstalled entrypoints-0.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dassl-0.6.3 entrypoints-0.3 flake8-3.7.9 ftfy-6.3.1 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.20.0a20250323 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KaiyangZhou/CoOp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELNIb_sdY-rG",
        "outputId": "6de8845f-4abd-485d-8ea5-c38472187322"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 198 (delta 198), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 26.56 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R +x CoOp/"
      ],
      "metadata": {
        "id": "G1ncHr1YsDTI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/DATA=\\/path\\/to\\/datasets/DATA=\\/content\\/datasets/g' CoOp/scripts/coop/main.sh"
      ],
      "metadata": {
        "id": "LIjrZI_ue60D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets/"
      ],
      "metadata": {
        "id": "uPfI7ioZjNLi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P datasets/ https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\n",
        "!unzip -q datasets/caltech-101.zip -d datasets/\n",
        "!tar -xzf datasets/caltech-101/101_ObjectCategories.tar.gz -C datasets/caltech-101/\n",
        "!rm -rf datasets/__MACOSX/\n",
        "!rm datasets/caltech-101/101_ObjectCategories.tar.gz\n",
        "!rm datasets/caltech-101/Annotations.tar\n",
        "!rm datasets/caltech-101/show_annotation.m\n",
        "!rm datasets/caltech-101.zip\n",
        "!gdown 1hyarUivQE36mY6jSomru6Fjd-JzwcCzN -O datasets/caltech-101/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLiuCh-llnZS",
        "outputId": "fe475674-4c33-4466-803f-22dd112f9d78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 16:02:44--  https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\n",
            "Resolving data.caltech.edu (data.caltech.edu)... 35.155.11.48\n",
            "Connecting to data.caltech.edu (data.caltech.edu)|35.155.11.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://s3.us-west-2.amazonaws.com/caltechdata/47/20/fc77-d78a-4c50-81c9-d47c2004df45/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dcaltech-101.zip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250323T160244Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=24836ada65f5e512993f49d1b33c335d72daec9b9b08b91fca791ad52c868940 [following]\n",
            "--2025-03-23 16:02:45--  https://s3.us-west-2.amazonaws.com/caltechdata/47/20/fc77-d78a-4c50-81c9-d47c2004df45/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dcaltech-101.zip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250323T160244Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=24836ada65f5e512993f49d1b33c335d72daec9b9b08b91fca791ad52c868940\n",
            "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.92.229.216, 52.218.229.72, 52.92.243.216, ...\n",
            "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.92.229.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137414764 (131M) [application/octet-stream]\n",
            "Saving to: ‘datasets/caltech-101.zip’\n",
            "\n",
            "caltech-101.zip     100%[===================>] 131.05M  35.3MB/s    in 3.7s    \n",
            "\n",
            "2025-03-23 16:02:48 (35.3 MB/s) - ‘datasets/caltech-101.zip’ saved [137414764/137414764]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hyarUivQE36mY6jSomru6Fjd-JzwcCzN\n",
            "To: /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "100% 809k/809k [00:00<00:00, 54.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd CoOp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiwDruVXsG_g",
        "outputId": "b3ed7812-6f99-4bee-a882-2ba5e0e20727"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn8dVtYKseOj",
        "outputId": "df5911ba-08a6-4fd3-8f48-7594ddf4c1c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->-r requirements.txt (line 1)) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scripts/coop/main.sh caltech101 vit_b16_ep50 end 16 1 False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj5aYM0TqzmN",
        "outputId": "cdd142b6-a7f3-48ed-a369-b0d747f91372"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-23 16:03:06.895420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742745787.154416    4231 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742745787.219666    4231 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 16:03:07.751868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed1\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             256 KiB (1 instance)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Creating a 1-shot dataset\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/datasets/caltech-101/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  100\n",
            "# train_x  100\n",
            "# val      100\n",
            "# test     2,465\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "100%|███████████████████████████████████████| 351M/351M [00:03<00:00, 97.7MiB/s]\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/3] time 2.909 (2.909) data 0.734 (0.734) loss 2.6992 (2.6992) acc 50.0000 (50.0000) lr 1.0000e-05 eta 0:07:13\n",
            "epoch [1/50] batch [2/3] time 0.229 (1.569) data 0.001 (0.367) loss 2.5469 (2.6230) acc 50.0000 (50.0000) lr 1.0000e-05 eta 0:03:52\n",
            "epoch [1/50] batch [3/3] time 0.231 (1.123) data 0.000 (0.245) loss 2.3242 (2.5234) acc 65.6250 (55.2083) lr 2.0000e-03 eta 0:02:45\n",
            "epoch [2/50] batch [1/3] time 0.962 (0.962) data 0.730 (0.730) loss 2.5020 (2.5020) acc 53.1250 (53.1250) lr 2.0000e-03 eta 0:02:20\n",
            "epoch [2/50] batch [2/3] time 0.232 (0.597) data 0.001 (0.365) loss 1.6348 (2.0684) acc 53.1250 (53.1250) lr 2.0000e-03 eta 0:01:26\n",
            "epoch [2/50] batch [3/3] time 0.233 (0.476) data 0.000 (0.244) loss 1.8027 (1.9798) acc 56.2500 (54.1667) lr 1.9980e-03 eta 0:01:08\n",
            "epoch [3/50] batch [1/3] time 0.796 (0.796) data 0.565 (0.565) loss 1.0684 (1.0684) acc 71.8750 (71.8750) lr 1.9980e-03 eta 0:01:53\n",
            "epoch [3/50] batch [2/3] time 0.232 (0.514) data 0.000 (0.283) loss 1.1797 (1.1240) acc 65.6250 (68.7500) lr 1.9980e-03 eta 0:01:13\n",
            "epoch [3/50] batch [3/3] time 0.231 (0.420) data 0.000 (0.188) loss 0.9810 (1.0763) acc 71.8750 (69.7917) lr 1.9921e-03 eta 0:00:59\n",
            "epoch [4/50] batch [1/3] time 0.770 (0.770) data 0.536 (0.536) loss 0.6675 (0.6675) acc 84.3750 (84.3750) lr 1.9921e-03 eta 0:01:47\n",
            "epoch [4/50] batch [2/3] time 0.232 (0.501) data 0.001 (0.268) loss 0.9697 (0.8186) acc 75.0000 (79.6875) lr 1.9921e-03 eta 0:01:09\n",
            "epoch [4/50] batch [3/3] time 0.234 (0.412) data 0.001 (0.179) loss 0.7598 (0.7990) acc 84.3750 (81.2500) lr 1.9823e-03 eta 0:00:56\n",
            "epoch [5/50] batch [1/3] time 1.182 (1.182) data 0.948 (0.948) loss 1.0293 (1.0293) acc 71.8750 (71.8750) lr 1.9823e-03 eta 0:02:41\n",
            "epoch [5/50] batch [2/3] time 0.232 (0.707) data 0.001 (0.474) loss 0.9292 (0.9792) acc 75.0000 (73.4375) lr 1.9823e-03 eta 0:01:36\n",
            "epoch [5/50] batch [3/3] time 0.232 (0.549) data 0.000 (0.316) loss 0.7729 (0.9105) acc 78.1250 (75.0000) lr 1.9686e-03 eta 0:01:14\n",
            "epoch [6/50] batch [1/3] time 0.780 (0.780) data 0.549 (0.549) loss 0.8037 (0.8037) acc 81.2500 (81.2500) lr 1.9686e-03 eta 0:01:44\n",
            "epoch [6/50] batch [2/3] time 0.232 (0.506) data 0.001 (0.275) loss 0.8340 (0.8188) acc 75.0000 (78.1250) lr 1.9686e-03 eta 0:01:07\n",
            "epoch [6/50] batch [3/3] time 0.234 (0.415) data 0.001 (0.183) loss 0.8901 (0.8426) acc 81.2500 (79.1667) lr 1.9511e-03 eta 0:00:54\n",
            "epoch [7/50] batch [1/3] time 0.798 (0.798) data 0.567 (0.567) loss 0.4348 (0.4348) acc 87.5000 (87.5000) lr 1.9511e-03 eta 0:01:44\n",
            "epoch [7/50] batch [2/3] time 0.233 (0.515) data 0.001 (0.284) loss 0.8970 (0.6659) acc 81.2500 (84.3750) lr 1.9511e-03 eta 0:01:06\n",
            "epoch [7/50] batch [3/3] time 0.233 (0.421) data 0.000 (0.189) loss 0.5986 (0.6435) acc 81.2500 (83.3333) lr 1.9298e-03 eta 0:00:54\n",
            "epoch [8/50] batch [1/3] time 0.687 (0.687) data 0.450 (0.450) loss 1.6621 (1.6621) acc 65.6250 (65.6250) lr 1.9298e-03 eta 0:01:27\n",
            "epoch [8/50] batch [2/3] time 0.231 (0.459) data 0.001 (0.225) loss 0.4692 (1.0657) acc 84.3750 (75.0000) lr 1.9298e-03 eta 0:00:58\n",
            "epoch [8/50] batch [3/3] time 0.233 (0.384) data 0.000 (0.150) loss 0.7563 (0.9626) acc 75.0000 (75.0000) lr 1.9048e-03 eta 0:00:48\n",
            "epoch [9/50] batch [1/3] time 0.798 (0.798) data 0.567 (0.567) loss 0.5347 (0.5347) acc 90.6250 (90.6250) lr 1.9048e-03 eta 0:01:39\n",
            "epoch [9/50] batch [2/3] time 0.234 (0.516) data 0.001 (0.284) loss 0.6660 (0.6003) acc 81.2500 (85.9375) lr 1.9048e-03 eta 0:01:03\n",
            "epoch [9/50] batch [3/3] time 0.234 (0.422) data 0.000 (0.189) loss 1.3086 (0.8364) acc 68.7500 (80.2083) lr 1.8763e-03 eta 0:00:51\n",
            "epoch [10/50] batch [1/3] time 0.804 (0.804) data 0.573 (0.573) loss 0.5527 (0.5527) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:01:38\n",
            "epoch [10/50] batch [2/3] time 0.232 (0.518) data 0.001 (0.287) loss 0.4939 (0.5233) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:01:02\n",
            "epoch [10/50] batch [3/3] time 0.245 (0.427) data 0.000 (0.191) loss 0.7988 (0.6152) acc 75.0000 (83.3333) lr 1.8443e-03 eta 0:00:51\n",
            "epoch [11/50] batch [1/3] time 0.861 (0.861) data 0.627 (0.627) loss 0.9824 (0.9824) acc 75.0000 (75.0000) lr 1.8443e-03 eta 0:01:42\n",
            "epoch [11/50] batch [2/3] time 0.238 (0.549) data 0.001 (0.314) loss 0.6592 (0.8208) acc 81.2500 (78.1250) lr 1.8443e-03 eta 0:01:04\n",
            "epoch [11/50] batch [3/3] time 0.232 (0.444) data 0.000 (0.209) loss 0.3975 (0.6797) acc 90.6250 (82.2917) lr 1.8090e-03 eta 0:00:51\n",
            "epoch [12/50] batch [1/3] time 0.782 (0.782) data 0.548 (0.548) loss 0.7700 (0.7700) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:01:30\n",
            "epoch [12/50] batch [2/3] time 0.236 (0.509) data 0.001 (0.275) loss 0.7803 (0.7751) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:00:58\n",
            "epoch [12/50] batch [3/3] time 0.234 (0.417) data 0.000 (0.183) loss 0.9351 (0.8285) acc 75.0000 (79.1667) lr 1.7705e-03 eta 0:00:47\n",
            "epoch [13/50] batch [1/3] time 1.203 (1.203) data 0.945 (0.945) loss 0.8672 (0.8672) acc 84.3750 (84.3750) lr 1.7705e-03 eta 0:02:15\n",
            "epoch [13/50] batch [2/3] time 0.236 (0.719) data 0.001 (0.473) loss 0.9731 (0.9202) acc 78.1250 (81.2500) lr 1.7705e-03 eta 0:01:20\n",
            "epoch [13/50] batch [3/3] time 0.234 (0.557) data 0.000 (0.315) loss 0.2214 (0.6873) acc 93.7500 (85.4167) lr 1.7290e-03 eta 0:01:01\n",
            "epoch [14/50] batch [1/3] time 0.842 (0.842) data 0.611 (0.611) loss 0.9189 (0.9189) acc 71.8750 (71.8750) lr 1.7290e-03 eta 0:01:32\n",
            "epoch [14/50] batch [2/3] time 0.234 (0.538) data 0.001 (0.306) loss 0.4333 (0.6761) acc 87.5000 (79.6875) lr 1.7290e-03 eta 0:00:58\n",
            "epoch [14/50] batch [3/3] time 0.234 (0.437) data 0.000 (0.204) loss 0.6533 (0.6685) acc 84.3750 (81.2500) lr 1.6845e-03 eta 0:00:47\n",
            "epoch [15/50] batch [1/3] time 0.833 (0.833) data 0.600 (0.600) loss 0.6333 (0.6333) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:01:29\n",
            "epoch [15/50] batch [2/3] time 0.233 (0.533) data 0.001 (0.300) loss 0.7334 (0.6833) acc 78.1250 (81.2500) lr 1.6845e-03 eta 0:00:56\n",
            "epoch [15/50] batch [3/3] time 0.235 (0.434) data 0.000 (0.200) loss 0.4004 (0.5890) acc 90.6250 (84.3750) lr 1.6374e-03 eta 0:00:45\n",
            "epoch [16/50] batch [1/3] time 0.833 (0.833) data 0.600 (0.600) loss 0.9019 (0.9019) acc 75.0000 (75.0000) lr 1.6374e-03 eta 0:01:26\n",
            "epoch [16/50] batch [2/3] time 0.234 (0.533) data 0.000 (0.300) loss 0.7104 (0.8062) acc 84.3750 (79.6875) lr 1.6374e-03 eta 0:00:54\n",
            "epoch [16/50] batch [3/3] time 0.234 (0.434) data 0.000 (0.200) loss 0.5493 (0.7205) acc 84.3750 (81.2500) lr 1.5878e-03 eta 0:00:44\n",
            "epoch [17/50] batch [1/3] time 0.840 (0.840) data 0.607 (0.607) loss 0.8262 (0.8262) acc 81.2500 (81.2500) lr 1.5878e-03 eta 0:01:24\n",
            "epoch [17/50] batch [2/3] time 0.234 (0.537) data 0.001 (0.304) loss 0.6235 (0.7249) acc 78.1250 (79.6875) lr 1.5878e-03 eta 0:00:53\n",
            "epoch [17/50] batch [3/3] time 0.235 (0.436) data 0.000 (0.203) loss 0.3896 (0.6131) acc 87.5000 (82.2917) lr 1.5358e-03 eta 0:00:43\n",
            "epoch [18/50] batch [1/3] time 0.846 (0.846) data 0.614 (0.614) loss 0.3687 (0.3687) acc 93.7500 (93.7500) lr 1.5358e-03 eta 0:01:22\n",
            "epoch [18/50] batch [2/3] time 0.235 (0.541) data 0.001 (0.307) loss 0.3091 (0.3389) acc 93.7500 (93.7500) lr 1.5358e-03 eta 0:00:52\n",
            "epoch [18/50] batch [3/3] time 0.234 (0.438) data 0.000 (0.205) loss 0.6240 (0.4339) acc 78.1250 (88.5417) lr 1.4818e-03 eta 0:00:42\n",
            "epoch [19/50] batch [1/3] time 0.731 (0.731) data 0.497 (0.497) loss 0.7544 (0.7544) acc 78.1250 (78.1250) lr 1.4818e-03 eta 0:01:09\n",
            "epoch [19/50] batch [2/3] time 0.234 (0.483) data 0.001 (0.249) loss 0.6230 (0.6887) acc 75.0000 (76.5625) lr 1.4818e-03 eta 0:00:45\n",
            "epoch [19/50] batch [3/3] time 0.236 (0.400) data 0.000 (0.166) loss 0.8916 (0.7563) acc 78.1250 (77.0833) lr 1.4258e-03 eta 0:00:37\n",
            "epoch [20/50] batch [1/3] time 0.854 (0.854) data 0.619 (0.619) loss 0.3645 (0.3645) acc 87.5000 (87.5000) lr 1.4258e-03 eta 0:01:18\n",
            "epoch [20/50] batch [2/3] time 0.235 (0.544) data 0.001 (0.310) loss 0.6816 (0.5231) acc 84.3750 (85.9375) lr 1.4258e-03 eta 0:00:49\n",
            "epoch [20/50] batch [3/3] time 0.236 (0.442) data 0.000 (0.207) loss 0.6938 (0.5800) acc 84.3750 (85.4167) lr 1.3681e-03 eta 0:00:39\n",
            "epoch [21/50] batch [1/3] time 1.183 (1.183) data 0.947 (0.947) loss 0.5737 (0.5737) acc 81.2500 (81.2500) lr 1.3681e-03 eta 0:01:45\n",
            "epoch [21/50] batch [2/3] time 0.234 (0.708) data 0.001 (0.474) loss 0.2954 (0.4346) acc 93.7500 (87.5000) lr 1.3681e-03 eta 0:01:02\n",
            "epoch [21/50] batch [3/3] time 0.235 (0.551) data 0.000 (0.316) loss 0.4988 (0.4560) acc 84.3750 (86.4583) lr 1.3090e-03 eta 0:00:47\n",
            "epoch [22/50] batch [1/3] time 0.798 (0.798) data 0.564 (0.564) loss 1.1279 (1.1279) acc 65.6250 (65.6250) lr 1.3090e-03 eta 0:01:08\n",
            "epoch [22/50] batch [2/3] time 0.235 (0.517) data 0.001 (0.282) loss 0.5322 (0.8301) acc 87.5000 (76.5625) lr 1.3090e-03 eta 0:00:43\n",
            "epoch [22/50] batch [3/3] time 0.236 (0.423) data 0.000 (0.188) loss 0.3896 (0.6833) acc 87.5000 (80.2083) lr 1.2487e-03 eta 0:00:35\n",
            "epoch [23/50] batch [1/3] time 0.797 (0.797) data 0.563 (0.563) loss 0.2952 (0.2952) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:01:06\n",
            "epoch [23/50] batch [2/3] time 0.236 (0.517) data 0.001 (0.282) loss 0.6562 (0.4757) acc 78.1250 (84.3750) lr 1.2487e-03 eta 0:00:42\n",
            "epoch [23/50] batch [3/3] time 0.234 (0.423) data 0.000 (0.188) loss 0.5713 (0.5076) acc 87.5000 (85.4167) lr 1.1874e-03 eta 0:00:34\n",
            "epoch [24/50] batch [1/3] time 0.736 (0.736) data 0.501 (0.501) loss 0.4609 (0.4609) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:00:58\n",
            "epoch [24/50] batch [2/3] time 0.235 (0.485) data 0.001 (0.251) loss 0.1838 (0.3224) acc 96.8750 (93.7500) lr 1.1874e-03 eta 0:00:38\n",
            "epoch [24/50] batch [3/3] time 0.234 (0.402) data 0.000 (0.167) loss 0.5552 (0.4000) acc 87.5000 (91.6667) lr 1.1253e-03 eta 0:00:31\n",
            "epoch [25/50] batch [1/3] time 0.787 (0.787) data 0.553 (0.553) loss 0.3486 (0.3486) acc 90.6250 (90.6250) lr 1.1253e-03 eta 0:01:00\n",
            "epoch [25/50] batch [2/3] time 0.239 (0.513) data 0.001 (0.277) loss 0.6714 (0.5100) acc 75.0000 (82.8125) lr 1.1253e-03 eta 0:00:39\n",
            "epoch [25/50] batch [3/3] time 0.238 (0.422) data 0.000 (0.185) loss 0.3003 (0.4401) acc 96.8750 (87.5000) lr 1.0628e-03 eta 0:00:31\n",
            "epoch [26/50] batch [1/3] time 0.837 (0.837) data 0.603 (0.603) loss 0.8540 (0.8540) acc 84.3750 (84.3750) lr 1.0628e-03 eta 0:01:01\n",
            "epoch [26/50] batch [2/3] time 0.235 (0.536) data 0.001 (0.302) loss 0.7749 (0.8145) acc 84.3750 (84.3750) lr 1.0628e-03 eta 0:00:39\n",
            "epoch [26/50] batch [3/3] time 0.237 (0.436) data 0.000 (0.201) loss 0.3325 (0.6538) acc 90.6250 (86.4583) lr 1.0000e-03 eta 0:00:31\n",
            "epoch [27/50] batch [1/3] time 0.817 (0.817) data 0.585 (0.585) loss 0.5020 (0.5020) acc 78.1250 (78.1250) lr 1.0000e-03 eta 0:00:58\n",
            "epoch [27/50] batch [2/3] time 0.236 (0.527) data 0.001 (0.293) loss 0.6323 (0.5671) acc 87.5000 (82.8125) lr 1.0000e-03 eta 0:00:36\n",
            "epoch [27/50] batch [3/3] time 0.236 (0.430) data 0.000 (0.195) loss 0.2744 (0.4696) acc 93.7500 (86.4583) lr 9.3721e-04 eta 0:00:29\n",
            "epoch [28/50] batch [1/3] time 0.798 (0.798) data 0.562 (0.562) loss 0.7354 (0.7354) acc 81.2500 (81.2500) lr 9.3721e-04 eta 0:00:54\n",
            "epoch [28/50] batch [2/3] time 0.240 (0.519) data 0.001 (0.281) loss 0.6587 (0.6970) acc 81.2500 (81.2500) lr 9.3721e-04 eta 0:00:34\n",
            "epoch [28/50] batch [3/3] time 0.236 (0.425) data 0.001 (0.188) loss 0.3423 (0.5788) acc 90.6250 (84.3750) lr 8.7467e-04 eta 0:00:28\n",
            "epoch [29/50] batch [1/3] time 1.174 (1.174) data 0.936 (0.936) loss 0.5962 (0.5962) acc 81.2500 (81.2500) lr 8.7467e-04 eta 0:01:16\n",
            "epoch [29/50] batch [2/3] time 0.241 (0.707) data 0.002 (0.469) loss 0.4070 (0.5016) acc 87.5000 (84.3750) lr 8.7467e-04 eta 0:00:45\n",
            "epoch [29/50] batch [3/3] time 0.234 (0.549) data 0.000 (0.313) loss 0.3386 (0.4473) acc 93.7500 (87.5000) lr 8.1262e-04 eta 0:00:34\n",
            "epoch [30/50] batch [1/3] time 0.787 (0.787) data 0.549 (0.549) loss 0.2229 (0.2229) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:00:48\n",
            "epoch [30/50] batch [2/3] time 0.236 (0.511) data 0.001 (0.275) loss 0.2771 (0.2500) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:00:31\n",
            "epoch [30/50] batch [3/3] time 0.236 (0.419) data 0.000 (0.183) loss 0.5024 (0.3341) acc 84.3750 (90.6250) lr 7.5131e-04 eta 0:00:25\n",
            "epoch [31/50] batch [1/3] time 0.801 (0.801) data 0.567 (0.567) loss 0.3462 (0.3462) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:47\n",
            "epoch [31/50] batch [2/3] time 0.236 (0.519) data 0.000 (0.283) loss 0.5303 (0.4382) acc 90.6250 (92.1875) lr 7.5131e-04 eta 0:00:30\n",
            "epoch [31/50] batch [3/3] time 0.235 (0.424) data 0.000 (0.189) loss 0.5254 (0.4673) acc 84.3750 (89.5833) lr 6.9098e-04 eta 0:00:24\n",
            "epoch [32/50] batch [1/3] time 0.828 (0.828) data 0.594 (0.594) loss 0.7280 (0.7280) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:00:46\n",
            "epoch [32/50] batch [2/3] time 0.238 (0.533) data 0.001 (0.297) loss 0.5625 (0.6453) acc 90.6250 (89.0625) lr 6.9098e-04 eta 0:00:29\n",
            "epoch [32/50] batch [3/3] time 0.237 (0.434) data 0.000 (0.198) loss 0.5933 (0.6279) acc 87.5000 (88.5417) lr 6.3188e-04 eta 0:00:23\n",
            "epoch [33/50] batch [1/3] time 0.792 (0.792) data 0.553 (0.553) loss 0.5991 (0.5991) acc 87.5000 (87.5000) lr 6.3188e-04 eta 0:00:41\n",
            "epoch [33/50] batch [2/3] time 0.237 (0.515) data 0.001 (0.277) loss 0.3704 (0.4847) acc 84.3750 (85.9375) lr 6.3188e-04 eta 0:00:26\n",
            "epoch [33/50] batch [3/3] time 0.235 (0.422) data 0.000 (0.185) loss 0.3037 (0.4244) acc 96.8750 (89.5833) lr 5.7422e-04 eta 0:00:21\n",
            "epoch [34/50] batch [1/3] time 0.791 (0.791) data 0.557 (0.557) loss 0.5596 (0.5596) acc 84.3750 (84.3750) lr 5.7422e-04 eta 0:00:39\n",
            "epoch [34/50] batch [2/3] time 0.235 (0.513) data 0.001 (0.279) loss 0.6147 (0.5872) acc 81.2500 (82.8125) lr 5.7422e-04 eta 0:00:25\n",
            "epoch [34/50] batch [3/3] time 0.237 (0.421) data 0.000 (0.186) loss 1.1641 (0.7795) acc 71.8750 (79.1667) lr 5.1825e-04 eta 0:00:20\n",
            "epoch [35/50] batch [1/3] time 0.842 (0.842) data 0.607 (0.607) loss 0.2922 (0.2922) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:00:39\n",
            "epoch [35/50] batch [2/3] time 0.238 (0.540) data 0.001 (0.304) loss 0.2384 (0.2653) acc 90.6250 (92.1875) lr 5.1825e-04 eta 0:00:24\n",
            "epoch [35/50] batch [3/3] time 0.238 (0.439) data 0.000 (0.203) loss 0.5786 (0.3698) acc 81.2500 (88.5417) lr 4.6417e-04 eta 0:00:19\n",
            "epoch [36/50] batch [1/3] time 0.781 (0.781) data 0.546 (0.546) loss 0.6030 (0.6030) acc 81.2500 (81.2500) lr 4.6417e-04 eta 0:00:34\n",
            "epoch [36/50] batch [2/3] time 0.236 (0.509) data 0.001 (0.273) loss 0.3450 (0.4740) acc 84.3750 (82.8125) lr 4.6417e-04 eta 0:00:21\n",
            "epoch [36/50] batch [3/3] time 0.236 (0.418) data 0.001 (0.182) loss 0.2998 (0.4159) acc 96.8750 (87.5000) lr 4.1221e-04 eta 0:00:17\n",
            "epoch [37/50] batch [1/3] time 1.257 (1.257) data 1.022 (1.022) loss 0.4880 (0.4880) acc 84.3750 (84.3750) lr 4.1221e-04 eta 0:00:51\n",
            "epoch [37/50] batch [2/3] time 0.235 (0.746) data 0.001 (0.511) loss 0.5503 (0.5192) acc 84.3750 (84.3750) lr 4.1221e-04 eta 0:00:29\n",
            "epoch [37/50] batch [3/3] time 0.236 (0.576) data 0.000 (0.341) loss 0.5176 (0.5186) acc 87.5000 (85.4167) lr 3.6258e-04 eta 0:00:22\n",
            "epoch [38/50] batch [1/3] time 0.835 (0.835) data 0.600 (0.600) loss 0.3716 (0.3716) acc 87.5000 (87.5000) lr 3.6258e-04 eta 0:00:31\n",
            "epoch [38/50] batch [2/3] time 0.237 (0.536) data 0.001 (0.300) loss 0.4282 (0.3999) acc 90.6250 (89.0625) lr 3.6258e-04 eta 0:00:19\n",
            "epoch [38/50] batch [3/3] time 0.236 (0.436) data 0.001 (0.200) loss 0.7085 (0.5028) acc 87.5000 (88.5417) lr 3.1545e-04 eta 0:00:15\n",
            "epoch [39/50] batch [1/3] time 0.813 (0.813) data 0.578 (0.578) loss 0.1652 (0.1652) acc 100.0000 (100.0000) lr 3.1545e-04 eta 0:00:28\n",
            "epoch [39/50] batch [2/3] time 0.237 (0.525) data 0.001 (0.289) loss 0.6899 (0.4276) acc 87.5000 (93.7500) lr 3.1545e-04 eta 0:00:17\n",
            "epoch [39/50] batch [3/3] time 0.236 (0.428) data 0.000 (0.193) loss 0.5181 (0.4577) acc 87.5000 (91.6667) lr 2.7103e-04 eta 0:00:14\n",
            "epoch [40/50] batch [1/3] time 0.774 (0.774) data 0.538 (0.538) loss 0.9258 (0.9258) acc 81.2500 (81.2500) lr 2.7103e-04 eta 0:00:24\n",
            "epoch [40/50] batch [2/3] time 0.238 (0.506) data 0.001 (0.269) loss 0.7593 (0.8425) acc 78.1250 (79.6875) lr 2.7103e-04 eta 0:00:15\n",
            "epoch [40/50] batch [3/3] time 0.237 (0.416) data 0.000 (0.180) loss 0.6128 (0.7660) acc 87.5000 (82.2917) lr 2.2949e-04 eta 0:00:12\n",
            "epoch [41/50] batch [1/3] time 0.757 (0.757) data 0.522 (0.522) loss 0.5498 (0.5498) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:21\n",
            "epoch [41/50] batch [2/3] time 0.237 (0.497) data 0.001 (0.261) loss 0.4785 (0.5142) acc 87.5000 (89.0625) lr 2.2949e-04 eta 0:00:13\n",
            "epoch [41/50] batch [3/3] time 0.235 (0.410) data 0.000 (0.174) loss 0.5234 (0.5173) acc 87.5000 (88.5417) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [42/50] batch [1/3] time 0.782 (0.782) data 0.546 (0.546) loss 0.4138 (0.4138) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:00:20\n",
            "epoch [42/50] batch [2/3] time 0.237 (0.510) data 0.001 (0.273) loss 0.1075 (0.2607) acc 100.0000 (95.3125) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [42/50] batch [3/3] time 0.238 (0.419) data 0.000 (0.182) loss 0.6914 (0.4042) acc 87.5000 (92.7083) lr 1.5567e-04 eta 0:00:10\n",
            "epoch [43/50] batch [1/3] time 0.794 (0.794) data 0.560 (0.560) loss 0.8843 (0.8843) acc 84.3750 (84.3750) lr 1.5567e-04 eta 0:00:18\n",
            "epoch [43/50] batch [2/3] time 0.237 (0.516) data 0.001 (0.280) loss 0.9321 (0.9082) acc 71.8750 (78.1250) lr 1.5567e-04 eta 0:00:11\n",
            "epoch [43/50] batch [3/3] time 0.238 (0.423) data 0.000 (0.187) loss 0.7153 (0.8439) acc 78.1250 (78.1250) lr 1.2369e-04 eta 0:00:08\n",
            "epoch [44/50] batch [1/3] time 0.786 (0.786) data 0.549 (0.549) loss 0.5840 (0.5840) acc 84.3750 (84.3750) lr 1.2369e-04 eta 0:00:15\n",
            "epoch [44/50] batch [2/3] time 0.240 (0.513) data 0.001 (0.275) loss 0.4021 (0.4930) acc 87.5000 (85.9375) lr 1.2369e-04 eta 0:00:09\n",
            "epoch [44/50] batch [3/3] time 0.243 (0.423) data 0.000 (0.183) loss 0.6133 (0.5331) acc 84.3750 (85.4167) lr 9.5173e-05 eta 0:00:07\n",
            "epoch [45/50] batch [1/3] time 1.174 (1.174) data 0.927 (0.927) loss 0.4321 (0.4321) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:19\n",
            "epoch [45/50] batch [2/3] time 0.236 (0.705) data 0.001 (0.464) loss 0.9780 (0.7051) acc 75.0000 (82.8125) lr 9.5173e-05 eta 0:00:11\n",
            "epoch [45/50] batch [3/3] time 0.236 (0.549) data 0.000 (0.309) loss 0.3347 (0.5816) acc 93.7500 (86.4583) lr 7.0224e-05 eta 0:00:08\n",
            "epoch [46/50] batch [1/3] time 0.992 (0.992) data 0.757 (0.757) loss 0.1869 (0.1869) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:00:13\n",
            "epoch [46/50] batch [2/3] time 0.237 (0.615) data 0.001 (0.379) loss 0.3289 (0.2579) acc 84.3750 (90.6250) lr 7.0224e-05 eta 0:00:07\n",
            "epoch [46/50] batch [3/3] time 0.237 (0.489) data 0.000 (0.252) loss 0.6025 (0.3728) acc 87.5000 (89.5833) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [1/3] time 0.804 (0.804) data 0.567 (0.567) loss 0.4802 (0.4802) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:00:08\n",
            "epoch [47/50] batch [2/3] time 0.237 (0.520) data 0.001 (0.284) loss 0.7817 (0.6310) acc 78.1250 (82.8125) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [3/3] time 0.235 (0.425) data 0.000 (0.189) loss 0.5679 (0.6099) acc 84.3750 (83.3333) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [1/3] time 0.794 (0.794) data 0.558 (0.558) loss 0.5796 (0.5796) acc 81.2500 (81.2500) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [48/50] batch [2/3] time 0.237 (0.516) data 0.001 (0.279) loss 0.4480 (0.5138) acc 87.5000 (84.3750) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [3/3] time 0.236 (0.423) data 0.000 (0.186) loss 0.2737 (0.4338) acc 96.8750 (88.5417) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [1/3] time 0.853 (0.853) data 0.617 (0.617) loss 0.6455 (0.6455) acc 78.1250 (78.1250) lr 1.7713e-05 eta 0:00:04\n",
            "epoch [49/50] batch [2/3] time 0.237 (0.545) data 0.001 (0.309) loss 0.8838 (0.7646) acc 84.3750 (81.2500) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [3/3] time 0.238 (0.442) data 0.000 (0.206) loss 0.0841 (0.5378) acc 100.0000 (87.5000) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [1/3] time 0.778 (0.778) data 0.540 (0.540) loss 0.3672 (0.3672) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [2/3] time 0.238 (0.508) data 0.000 (0.270) loss 0.4495 (0.4083) acc 84.3750 (85.9375) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [3/3] time 0.236 (0.417) data 0.000 (0.180) loss 0.3232 (0.3800) acc 90.6250 (87.5000) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:17<00:00,  1.46it/s]\n",
            "=> result\n",
            "* total: 2,465\n",
            "* correct: 2,239\n",
            "* accuracy: 90.8%\n",
            "* error: 9.2%\n",
            "* macro_f1: 87.4%\n",
            "Elapsed: 0:01:36\n",
            "2025-03-23 16:05:14.562002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742745914.583666    6083 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742745914.590328    6083 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 16:05:14.612743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed2\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             256 KiB (1 instance)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Creating a 1-shot dataset\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/datasets/caltech-101/split_fewshot/shot_1-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  100\n",
            "# train_x  100\n",
            "# val      100\n",
            "# test     2,465\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed2/tensorboard)\n",
            "epoch [1/50] batch [1/3] time 2.000 (2.000) data 0.635 (0.635) loss 2.1348 (2.1348) acc 56.2500 (56.2500) lr 1.0000e-05 eta 0:04:57\n",
            "epoch [1/50] batch [2/3] time 0.235 (1.117) data 0.001 (0.318) loss 2.2754 (2.2051) acc 50.0000 (53.1250) lr 1.0000e-05 eta 0:02:45\n",
            "epoch [1/50] batch [3/3] time 0.235 (0.823) data 0.000 (0.212) loss 2.2559 (2.2220) acc 62.5000 (56.2500) lr 2.0000e-03 eta 0:02:01\n",
            "epoch [2/50] batch [1/3] time 0.770 (0.770) data 0.534 (0.534) loss 2.3711 (2.3711) acc 56.2500 (56.2500) lr 2.0000e-03 eta 0:01:52\n",
            "epoch [2/50] batch [2/3] time 0.236 (0.503) data 0.001 (0.267) loss 1.5137 (1.9424) acc 62.5000 (59.3750) lr 2.0000e-03 eta 0:01:12\n",
            "epoch [2/50] batch [3/3] time 0.238 (0.415) data 0.000 (0.178) loss 1.2471 (1.7106) acc 65.6250 (61.4583) lr 1.9980e-03 eta 0:00:59\n",
            "epoch [3/50] batch [1/3] time 1.168 (1.168) data 0.933 (0.933) loss 0.6421 (0.6421) acc 75.0000 (75.0000) lr 1.9980e-03 eta 0:02:47\n",
            "epoch [3/50] batch [2/3] time 0.239 (0.704) data 0.001 (0.467) loss 0.8281 (0.7351) acc 78.1250 (76.5625) lr 1.9980e-03 eta 0:01:39\n",
            "epoch [3/50] batch [3/3] time 0.237 (0.548) data 0.000 (0.311) loss 0.6426 (0.7043) acc 81.2500 (78.1250) lr 1.9921e-03 eta 0:01:17\n",
            "epoch [4/50] batch [1/3] time 0.810 (0.810) data 0.573 (0.573) loss 0.8179 (0.8179) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:01:53\n",
            "epoch [4/50] batch [2/3] time 0.237 (0.524) data 0.001 (0.287) loss 0.6318 (0.7249) acc 84.3750 (78.1250) lr 1.9921e-03 eta 0:01:12\n",
            "epoch [4/50] batch [3/3] time 0.238 (0.428) data 0.000 (0.191) loss 0.7612 (0.7370) acc 75.0000 (77.0833) lr 1.9823e-03 eta 0:00:59\n",
            "epoch [5/50] batch [1/3] time 0.674 (0.674) data 0.438 (0.438) loss 0.9956 (0.9956) acc 62.5000 (62.5000) lr 1.9823e-03 eta 0:01:32\n",
            "epoch [5/50] batch [2/3] time 0.237 (0.456) data 0.001 (0.219) loss 1.0020 (0.9988) acc 75.0000 (68.7500) lr 1.9823e-03 eta 0:01:01\n",
            "epoch [5/50] batch [3/3] time 0.238 (0.383) data 0.001 (0.146) loss 0.8921 (0.9632) acc 78.1250 (71.8750) lr 1.9686e-03 eta 0:00:51\n",
            "epoch [6/50] batch [1/3] time 0.758 (0.758) data 0.522 (0.522) loss 0.8638 (0.8638) acc 78.1250 (78.1250) lr 1.9686e-03 eta 0:01:41\n",
            "epoch [6/50] batch [2/3] time 0.238 (0.498) data 0.000 (0.261) loss 0.5391 (0.7014) acc 84.3750 (81.2500) lr 1.9686e-03 eta 0:01:06\n",
            "epoch [6/50] batch [3/3] time 0.236 (0.411) data 0.000 (0.174) loss 0.6431 (0.6820) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:00:54\n",
            "epoch [7/50] batch [1/3] time 0.799 (0.799) data 0.564 (0.564) loss 0.9688 (0.9688) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:01:44\n",
            "epoch [7/50] batch [2/3] time 0.237 (0.518) data 0.000 (0.282) loss 0.7290 (0.8489) acc 84.3750 (81.2500) lr 1.9511e-03 eta 0:01:07\n",
            "epoch [7/50] batch [3/3] time 0.237 (0.424) data 0.000 (0.188) loss 0.6953 (0.7977) acc 78.1250 (80.2083) lr 1.9298e-03 eta 0:00:54\n",
            "epoch [8/50] batch [1/3] time 0.742 (0.742) data 0.508 (0.508) loss 0.9565 (0.9565) acc 71.8750 (71.8750) lr 1.9298e-03 eta 0:01:34\n",
            "epoch [8/50] batch [2/3] time 0.237 (0.489) data 0.001 (0.254) loss 0.9663 (0.9614) acc 71.8750 (71.8750) lr 1.9298e-03 eta 0:01:02\n",
            "epoch [8/50] batch [3/3] time 0.239 (0.406) data 0.000 (0.170) loss 1.1260 (1.0163) acc 62.5000 (68.7500) lr 1.9048e-03 eta 0:00:51\n",
            "epoch [9/50] batch [1/3] time 1.599 (1.599) data 1.348 (1.348) loss 0.4065 (0.4065) acc 90.6250 (90.6250) lr 1.9048e-03 eta 0:03:19\n",
            "epoch [9/50] batch [2/3] time 0.237 (0.918) data 0.001 (0.674) loss 0.9785 (0.6925) acc 68.7500 (79.6875) lr 1.9048e-03 eta 0:01:53\n",
            "epoch [9/50] batch [3/3] time 0.238 (0.691) data 0.000 (0.450) loss 1.0205 (0.8018) acc 68.7500 (76.0417) lr 1.8763e-03 eta 0:01:25\n",
            "epoch [10/50] batch [1/3] time 2.652 (2.652) data 1.963 (1.963) loss 0.6484 (0.6484) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:05:23\n",
            "epoch [10/50] batch [2/3] time 0.238 (1.445) data 0.001 (0.982) loss 0.7891 (0.7188) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:02:54\n",
            "epoch [10/50] batch [3/3] time 0.236 (1.042) data 0.000 (0.655) loss 0.5098 (0.6491) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:02:05\n",
            "epoch [11/50] batch [1/3] time 0.930 (0.930) data 0.695 (0.695) loss 1.0322 (1.0322) acc 78.1250 (78.1250) lr 1.8443e-03 eta 0:01:50\n",
            "epoch [11/50] batch [2/3] time 0.238 (0.584) data 0.000 (0.348) loss 0.7368 (0.8845) acc 81.2500 (79.6875) lr 1.8443e-03 eta 0:01:08\n",
            "epoch [11/50] batch [3/3] time 0.240 (0.469) data 0.000 (0.232) loss 0.7588 (0.8426) acc 81.2500 (80.2083) lr 1.8090e-03 eta 0:00:54\n",
            "epoch [12/50] batch [1/3] time 0.826 (0.826) data 0.590 (0.590) loss 0.7661 (0.7661) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:01:35\n",
            "epoch [12/50] batch [2/3] time 0.237 (0.532) data 0.000 (0.295) loss 0.5483 (0.6572) acc 78.1250 (81.2500) lr 1.8090e-03 eta 0:01:01\n",
            "epoch [12/50] batch [3/3] time 0.236 (0.433) data 0.000 (0.197) loss 0.6660 (0.6602) acc 78.1250 (80.2083) lr 1.7705e-03 eta 0:00:49\n",
            "epoch [13/50] batch [1/3] time 0.749 (0.749) data 0.513 (0.513) loss 0.4980 (0.4980) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:01:24\n",
            "epoch [13/50] batch [2/3] time 0.239 (0.494) data 0.000 (0.257) loss 0.6274 (0.5627) acc 81.2500 (84.3750) lr 1.7705e-03 eta 0:00:55\n",
            "epoch [13/50] batch [3/3] time 0.236 (0.408) data 0.000 (0.171) loss 0.4255 (0.5170) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:00:45\n",
            "epoch [14/50] batch [1/3] time 0.775 (0.775) data 0.537 (0.537) loss 0.6694 (0.6694) acc 81.2500 (81.2500) lr 1.7290e-03 eta 0:01:25\n",
            "epoch [14/50] batch [2/3] time 0.238 (0.507) data 0.001 (0.269) loss 0.4336 (0.5515) acc 90.6250 (85.9375) lr 1.7290e-03 eta 0:00:55\n",
            "epoch [14/50] batch [3/3] time 0.236 (0.417) data 0.000 (0.179) loss 0.3999 (0.5010) acc 87.5000 (86.4583) lr 1.6845e-03 eta 0:00:44\n",
            "epoch [15/50] batch [1/3] time 0.704 (0.704) data 0.465 (0.465) loss 0.5532 (0.5532) acc 81.2500 (81.2500) lr 1.6845e-03 eta 0:01:15\n",
            "epoch [15/50] batch [2/3] time 0.238 (0.471) data 0.001 (0.233) loss 0.6484 (0.6008) acc 87.5000 (84.3750) lr 1.6845e-03 eta 0:00:49\n",
            "epoch [15/50] batch [3/3] time 0.238 (0.393) data 0.000 (0.155) loss 0.9805 (0.7274) acc 75.0000 (81.2500) lr 1.6374e-03 eta 0:00:41\n",
            "epoch [16/50] batch [1/3] time 0.807 (0.807) data 0.570 (0.570) loss 0.7695 (0.7695) acc 75.0000 (75.0000) lr 1.6374e-03 eta 0:01:23\n",
            "epoch [16/50] batch [2/3] time 0.238 (0.523) data 0.001 (0.285) loss 0.5210 (0.6453) acc 81.2500 (78.1250) lr 1.6374e-03 eta 0:00:53\n",
            "epoch [16/50] batch [3/3] time 0.237 (0.427) data 0.000 (0.190) loss 0.6782 (0.6562) acc 84.3750 (80.2083) lr 1.5878e-03 eta 0:00:43\n",
            "epoch [17/50] batch [1/3] time 0.751 (0.751) data 0.514 (0.514) loss 0.7188 (0.7188) acc 78.1250 (78.1250) lr 1.5878e-03 eta 0:01:15\n",
            "epoch [17/50] batch [2/3] time 0.238 (0.495) data 0.001 (0.257) loss 0.4851 (0.6019) acc 84.3750 (81.2500) lr 1.5878e-03 eta 0:00:49\n",
            "epoch [17/50] batch [3/3] time 0.239 (0.410) data 0.000 (0.172) loss 0.5205 (0.5748) acc 84.3750 (82.2917) lr 1.5358e-03 eta 0:00:40\n",
            "epoch [18/50] batch [1/3] time 1.190 (1.190) data 0.950 (0.950) loss 0.5762 (0.5762) acc 78.1250 (78.1250) lr 1.5358e-03 eta 0:01:56\n",
            "epoch [18/50] batch [2/3] time 0.240 (0.715) data 0.001 (0.475) loss 0.4897 (0.5330) acc 87.5000 (82.8125) lr 1.5358e-03 eta 0:01:09\n",
            "epoch [18/50] batch [3/3] time 0.239 (0.556) data 0.000 (0.317) loss 0.4453 (0.5037) acc 84.3750 (83.3333) lr 1.4818e-03 eta 0:00:53\n",
            "epoch [19/50] batch [1/3] time 0.767 (0.767) data 0.530 (0.530) loss 0.6304 (0.6304) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:01:12\n",
            "epoch [19/50] batch [2/3] time 0.241 (0.504) data 0.001 (0.265) loss 0.6289 (0.6296) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:00:47\n",
            "epoch [19/50] batch [3/3] time 0.240 (0.416) data 0.000 (0.177) loss 0.3113 (0.5235) acc 87.5000 (83.3333) lr 1.4258e-03 eta 0:00:38\n",
            "epoch [20/50] batch [1/3] time 0.801 (0.801) data 0.564 (0.564) loss 0.2925 (0.2925) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:01:13\n",
            "epoch [20/50] batch [2/3] time 0.238 (0.520) data 0.001 (0.282) loss 0.6997 (0.4961) acc 78.1250 (85.9375) lr 1.4258e-03 eta 0:00:47\n",
            "epoch [20/50] batch [3/3] time 0.238 (0.426) data 0.000 (0.188) loss 0.3406 (0.4443) acc 93.7500 (88.5417) lr 1.3681e-03 eta 0:00:38\n",
            "epoch [21/50] batch [1/3] time 0.763 (0.763) data 0.523 (0.523) loss 0.5508 (0.5508) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:01:07\n",
            "epoch [21/50] batch [2/3] time 0.240 (0.501) data 0.001 (0.262) loss 0.9810 (0.7659) acc 71.8750 (79.6875) lr 1.3681e-03 eta 0:00:44\n",
            "epoch [21/50] batch [3/3] time 0.237 (0.413) data 0.000 (0.175) loss 0.5044 (0.6787) acc 90.6250 (83.3333) lr 1.3090e-03 eta 0:00:35\n",
            "epoch [22/50] batch [1/3] time 0.734 (0.734) data 0.498 (0.498) loss 0.3442 (0.3442) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:03\n",
            "epoch [22/50] batch [2/3] time 0.239 (0.486) data 0.001 (0.249) loss 0.5010 (0.4226) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:00:41\n",
            "epoch [22/50] batch [3/3] time 0.239 (0.404) data 0.000 (0.166) loss 0.3616 (0.4023) acc 84.3750 (86.4583) lr 1.2487e-03 eta 0:00:33\n",
            "epoch [23/50] batch [1/3] time 0.754 (0.754) data 0.517 (0.517) loss 0.5151 (0.5151) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:01:02\n",
            "epoch [23/50] batch [2/3] time 0.239 (0.496) data 0.001 (0.259) loss 0.6816 (0.5984) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:00:40\n",
            "epoch [23/50] batch [3/3] time 0.238 (0.410) data 0.000 (0.173) loss 0.2798 (0.4922) acc 90.6250 (86.4583) lr 1.1874e-03 eta 0:00:33\n",
            "epoch [24/50] batch [1/3] time 0.738 (0.738) data 0.499 (0.499) loss 0.5400 (0.5400) acc 84.3750 (84.3750) lr 1.1874e-03 eta 0:00:59\n",
            "epoch [24/50] batch [2/3] time 0.239 (0.488) data 0.000 (0.250) loss 0.6909 (0.6155) acc 78.1250 (81.2500) lr 1.1874e-03 eta 0:00:38\n",
            "epoch [24/50] batch [3/3] time 0.240 (0.405) data 0.000 (0.167) loss 0.4277 (0.5529) acc 90.6250 (84.3750) lr 1.1253e-03 eta 0:00:31\n",
            "epoch [25/50] batch [1/3] time 0.761 (0.761) data 0.521 (0.521) loss 0.5547 (0.5547) acc 75.0000 (75.0000) lr 1.1253e-03 eta 0:00:58\n",
            "epoch [25/50] batch [2/3] time 0.239 (0.500) data 0.001 (0.261) loss 0.5400 (0.5474) acc 84.3750 (79.6875) lr 1.1253e-03 eta 0:00:38\n",
            "epoch [25/50] batch [3/3] time 0.241 (0.414) data 0.000 (0.174) loss 0.3281 (0.4743) acc 93.7500 (84.3750) lr 1.0628e-03 eta 0:00:31\n",
            "epoch [26/50] batch [1/3] time 1.091 (1.091) data 0.845 (0.845) loss 0.5601 (0.5601) acc 81.2500 (81.2500) lr 1.0628e-03 eta 0:01:20\n",
            "epoch [26/50] batch [2/3] time 0.241 (0.666) data 0.001 (0.423) loss 0.2507 (0.4054) acc 93.7500 (87.5000) lr 1.0628e-03 eta 0:00:48\n",
            "epoch [26/50] batch [3/3] time 0.241 (0.524) data 0.000 (0.282) loss 0.3853 (0.3987) acc 90.6250 (88.5417) lr 1.0000e-03 eta 0:00:37\n",
            "epoch [27/50] batch [1/3] time 0.769 (0.769) data 0.531 (0.531) loss 0.7754 (0.7754) acc 81.2500 (81.2500) lr 1.0000e-03 eta 0:00:54\n",
            "epoch [27/50] batch [2/3] time 0.240 (0.505) data 0.001 (0.266) loss 0.4512 (0.6133) acc 87.5000 (84.3750) lr 1.0000e-03 eta 0:00:35\n",
            "epoch [27/50] batch [3/3] time 0.238 (0.416) data 0.000 (0.177) loss 0.7378 (0.6548) acc 81.2500 (83.3333) lr 9.3721e-04 eta 0:00:28\n",
            "epoch [28/50] batch [1/3] time 0.762 (0.762) data 0.525 (0.525) loss 0.6655 (0.6655) acc 81.2500 (81.2500) lr 9.3721e-04 eta 0:00:51\n",
            "epoch [28/50] batch [2/3] time 0.239 (0.500) data 0.001 (0.263) loss 0.6294 (0.6475) acc 84.3750 (82.8125) lr 9.3721e-04 eta 0:00:33\n",
            "epoch [28/50] batch [3/3] time 0.239 (0.413) data 0.000 (0.175) loss 0.6108 (0.6353) acc 90.6250 (85.4167) lr 8.7467e-04 eta 0:00:27\n",
            "epoch [29/50] batch [1/3] time 0.812 (0.812) data 0.575 (0.575) loss 0.4995 (0.4995) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:00:52\n",
            "epoch [29/50] batch [2/3] time 0.240 (0.526) data 0.001 (0.288) loss 0.8091 (0.6543) acc 78.1250 (85.9375) lr 8.7467e-04 eta 0:00:33\n",
            "epoch [29/50] batch [3/3] time 0.238 (0.430) data 0.000 (0.192) loss 0.3203 (0.5430) acc 93.7500 (88.5417) lr 8.1262e-04 eta 0:00:27\n",
            "epoch [30/50] batch [1/3] time 0.770 (0.770) data 0.533 (0.533) loss 0.4763 (0.4763) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:00:47\n",
            "epoch [30/50] batch [2/3] time 0.243 (0.507) data 0.001 (0.267) loss 0.5430 (0.5096) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:00:30\n",
            "epoch [30/50] batch [3/3] time 0.240 (0.418) data 0.000 (0.178) loss 0.3906 (0.4700) acc 87.5000 (85.4167) lr 7.5131e-04 eta 0:00:25\n",
            "epoch [31/50] batch [1/3] time 0.780 (0.780) data 0.543 (0.543) loss 0.3184 (0.3184) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:45\n",
            "epoch [31/50] batch [2/3] time 0.239 (0.510) data 0.001 (0.272) loss 0.7739 (0.5461) acc 81.2500 (87.5000) lr 7.5131e-04 eta 0:00:29\n",
            "epoch [31/50] batch [3/3] time 0.240 (0.420) data 0.000 (0.181) loss 0.2705 (0.4543) acc 90.6250 (88.5417) lr 6.9098e-04 eta 0:00:23\n",
            "epoch [32/50] batch [1/3] time 0.820 (0.820) data 0.584 (0.584) loss 0.4050 (0.4050) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:00:45\n",
            "epoch [32/50] batch [2/3] time 0.240 (0.530) data 0.000 (0.292) loss 0.3022 (0.3536) acc 96.8750 (93.7500) lr 6.9098e-04 eta 0:00:29\n",
            "epoch [32/50] batch [3/3] time 0.238 (0.433) data 0.000 (0.195) loss 0.5615 (0.4229) acc 87.5000 (91.6667) lr 6.3188e-04 eta 0:00:23\n",
            "epoch [33/50] batch [1/3] time 0.780 (0.780) data 0.541 (0.541) loss 0.4636 (0.4636) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:00:41\n",
            "epoch [33/50] batch [2/3] time 0.240 (0.510) data 0.001 (0.271) loss 0.8481 (0.6559) acc 68.7500 (81.2500) lr 6.3188e-04 eta 0:00:26\n",
            "epoch [33/50] batch [3/3] time 0.241 (0.420) data 0.000 (0.181) loss 0.5527 (0.6215) acc 90.6250 (84.3750) lr 5.7422e-04 eta 0:00:21\n",
            "epoch [34/50] batch [1/3] time 1.238 (1.238) data 0.999 (0.999) loss 0.5024 (0.5024) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:01:01\n",
            "epoch [34/50] batch [2/3] time 0.241 (0.739) data 0.001 (0.500) loss 0.4336 (0.4680) acc 87.5000 (84.3750) lr 5.7422e-04 eta 0:00:36\n",
            "epoch [34/50] batch [3/3] time 0.242 (0.574) data 0.000 (0.333) loss 0.8857 (0.6073) acc 78.1250 (82.2917) lr 5.1825e-04 eta 0:00:27\n",
            "epoch [35/50] batch [1/3] time 0.778 (0.778) data 0.537 (0.537) loss 0.5400 (0.5400) acc 87.5000 (87.5000) lr 5.1825e-04 eta 0:00:36\n",
            "epoch [35/50] batch [2/3] time 0.240 (0.509) data 0.001 (0.269) loss 0.4958 (0.5179) acc 84.3750 (85.9375) lr 5.1825e-04 eta 0:00:23\n",
            "epoch [35/50] batch [3/3] time 0.241 (0.420) data 0.000 (0.179) loss 0.7217 (0.5859) acc 78.1250 (83.3333) lr 4.6417e-04 eta 0:00:18\n",
            "epoch [36/50] batch [1/3] time 0.814 (0.814) data 0.574 (0.574) loss 0.3008 (0.3008) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:00:35\n",
            "epoch [36/50] batch [2/3] time 0.241 (0.527) data 0.001 (0.287) loss 0.3750 (0.3379) acc 87.5000 (89.0625) lr 4.6417e-04 eta 0:00:22\n",
            "epoch [36/50] batch [3/3] time 0.241 (0.432) data 0.000 (0.192) loss 0.4695 (0.3818) acc 87.5000 (88.5417) lr 4.1221e-04 eta 0:00:18\n",
            "epoch [37/50] batch [1/3] time 0.802 (0.802) data 0.563 (0.563) loss 0.2581 (0.2581) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:00:32\n",
            "epoch [37/50] batch [2/3] time 0.241 (0.521) data 0.001 (0.282) loss 0.3831 (0.3206) acc 90.6250 (95.3125) lr 4.1221e-04 eta 0:00:20\n",
            "epoch [37/50] batch [3/3] time 0.240 (0.427) data 0.000 (0.188) loss 0.5078 (0.3830) acc 90.6250 (93.7500) lr 3.6258e-04 eta 0:00:16\n",
            "epoch [38/50] batch [1/3] time 0.800 (0.800) data 0.561 (0.561) loss 0.5581 (0.5581) acc 84.3750 (84.3750) lr 3.6258e-04 eta 0:00:30\n",
            "epoch [38/50] batch [2/3] time 0.240 (0.520) data 0.001 (0.281) loss 0.3547 (0.4564) acc 90.6250 (87.5000) lr 3.6258e-04 eta 0:00:19\n",
            "epoch [38/50] batch [3/3] time 0.242 (0.427) data 0.000 (0.187) loss 0.7690 (0.5606) acc 78.1250 (84.3750) lr 3.1545e-04 eta 0:00:15\n",
            "epoch [39/50] batch [1/3] time 0.744 (0.744) data 0.507 (0.507) loss 0.2197 (0.2197) acc 100.0000 (100.0000) lr 3.1545e-04 eta 0:00:26\n",
            "epoch [39/50] batch [2/3] time 0.240 (0.492) data 0.001 (0.254) loss 0.4368 (0.3282) acc 81.2500 (90.6250) lr 3.1545e-04 eta 0:00:16\n",
            "epoch [39/50] batch [3/3] time 0.239 (0.408) data 0.000 (0.169) loss 0.4285 (0.3617) acc 87.5000 (89.5833) lr 2.7103e-04 eta 0:00:13\n",
            "epoch [40/50] batch [1/3] time 0.803 (0.803) data 0.566 (0.566) loss 0.6792 (0.6792) acc 81.2500 (81.2500) lr 2.7103e-04 eta 0:00:25\n",
            "epoch [40/50] batch [2/3] time 0.240 (0.522) data 0.001 (0.283) loss 0.5132 (0.5962) acc 87.5000 (84.3750) lr 2.7103e-04 eta 0:00:16\n",
            "epoch [40/50] batch [3/3] time 0.240 (0.428) data 0.000 (0.189) loss 0.4705 (0.5543) acc 93.7500 (87.5000) lr 2.2949e-04 eta 0:00:12\n",
            "epoch [41/50] batch [1/3] time 0.822 (0.822) data 0.583 (0.583) loss 0.3384 (0.3384) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:23\n",
            "epoch [41/50] batch [2/3] time 0.241 (0.531) data 0.001 (0.292) loss 0.3806 (0.3595) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:14\n",
            "epoch [41/50] batch [3/3] time 0.243 (0.435) data 0.000 (0.195) loss 0.7554 (0.4915) acc 84.3750 (88.5417) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [42/50] batch [1/3] time 1.157 (1.157) data 0.919 (0.919) loss 0.3770 (0.3770) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:00:30\n",
            "epoch [42/50] batch [2/3] time 0.242 (0.700) data 0.001 (0.460) loss 0.5513 (0.4641) acc 84.3750 (85.9375) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [42/50] batch [3/3] time 0.241 (0.547) data 0.000 (0.307) loss 0.4888 (0.4723) acc 87.5000 (86.4583) lr 1.5567e-04 eta 0:00:13\n",
            "epoch [43/50] batch [1/3] time 0.779 (0.779) data 0.538 (0.538) loss 0.3896 (0.3896) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:00:17\n",
            "epoch [43/50] batch [2/3] time 0.241 (0.510) data 0.001 (0.269) loss 0.2690 (0.3293) acc 93.7500 (90.6250) lr 1.5567e-04 eta 0:00:11\n",
            "epoch [43/50] batch [3/3] time 0.239 (0.420) data 0.000 (0.180) loss 0.2239 (0.2942) acc 96.8750 (92.7083) lr 1.2369e-04 eta 0:00:08\n",
            "epoch [44/50] batch [1/3] time 0.781 (0.781) data 0.542 (0.542) loss 0.5000 (0.5000) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:15\n",
            "epoch [44/50] batch [2/3] time 0.240 (0.511) data 0.001 (0.271) loss 0.6157 (0.5579) acc 84.3750 (87.5000) lr 1.2369e-04 eta 0:00:09\n",
            "epoch [44/50] batch [3/3] time 0.241 (0.421) data 0.000 (0.181) loss 0.5806 (0.5654) acc 84.3750 (86.4583) lr 9.5173e-05 eta 0:00:07\n",
            "epoch [45/50] batch [1/3] time 0.788 (0.788) data 0.550 (0.550) loss 0.6968 (0.6968) acc 81.2500 (81.2500) lr 9.5173e-05 eta 0:00:13\n",
            "epoch [45/50] batch [2/3] time 0.239 (0.514) data 0.000 (0.275) loss 0.4224 (0.5596) acc 90.6250 (85.9375) lr 9.5173e-05 eta 0:00:08\n",
            "epoch [45/50] batch [3/3] time 0.241 (0.423) data 0.000 (0.184) loss 0.6421 (0.5871) acc 84.3750 (85.4167) lr 7.0224e-05 eta 0:00:06\n",
            "epoch [46/50] batch [1/3] time 0.831 (0.831) data 0.592 (0.592) loss 0.4924 (0.4924) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:11\n",
            "epoch [46/50] batch [2/3] time 0.240 (0.536) data 0.001 (0.296) loss 0.1432 (0.3178) acc 93.7500 (90.6250) lr 7.0224e-05 eta 0:00:06\n",
            "epoch [46/50] batch [3/3] time 0.241 (0.438) data 0.000 (0.198) loss 0.3403 (0.3253) acc 87.5000 (89.5833) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [1/3] time 0.790 (0.790) data 0.551 (0.551) loss 0.6592 (0.6592) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:00:08\n",
            "epoch [47/50] batch [2/3] time 0.239 (0.515) data 0.000 (0.276) loss 0.5815 (0.6204) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [3/3] time 0.239 (0.423) data 0.000 (0.184) loss 0.4939 (0.5782) acc 84.3750 (86.4583) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [1/3] time 0.783 (0.783) data 0.544 (0.544) loss 0.2705 (0.2705) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [48/50] batch [2/3] time 0.241 (0.512) data 0.000 (0.272) loss 0.5640 (0.4172) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [3/3] time 0.240 (0.421) data 0.000 (0.182) loss 0.4243 (0.4196) acc 87.5000 (91.6667) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [1/3] time 0.696 (0.696) data 0.457 (0.457) loss 0.4124 (0.4124) acc 81.2500 (81.2500) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [49/50] batch [2/3] time 0.239 (0.468) data 0.001 (0.229) loss 0.6079 (0.5101) acc 84.3750 (82.8125) lr 1.7713e-05 eta 0:00:01\n",
            "epoch [49/50] batch [3/3] time 0.241 (0.392) data 0.000 (0.153) loss 0.3633 (0.4612) acc 90.6250 (85.4167) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [1/3] time 1.158 (1.158) data 0.917 (0.917) loss 0.3362 (0.3362) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:02\n",
            "epoch [50/50] batch [2/3] time 0.243 (0.700) data 0.001 (0.459) loss 0.5093 (0.4227) acc 90.6250 (92.1875) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [3/3] time 0.242 (0.548) data 0.000 (0.306) loss 0.7070 (0.5175) acc 87.5000 (90.6250) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed2/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:16<00:00,  1.55it/s]\n",
            "=> result\n",
            "* total: 2,465\n",
            "* correct: 2,294\n",
            "* accuracy: 93.1%\n",
            "* error: 6.9%\n",
            "* macro_f1: 90.4%\n",
            "Elapsed: 0:01:35\n",
            "2025-03-23 16:07:09.434074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742746029.470220    7834 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742746029.481311    7834 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 16:07:09.515513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/caltech101.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed3\n",
            "resume: \n",
            "root: /content/datasets\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: Caltech101\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/datasets\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             256 KiB (1 instance)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.4.5.8\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.4.127\n",
            "[pip3] nvidia-cudnn-cu12==9.1.0.70\n",
            "[pip3] nvidia-cufft-cu12==11.2.1.3\n",
            "[pip3] nvidia-curand-cu12==10.3.5.147\n",
            "[pip3] nvidia-cusolver-cu12==11.6.1.9\n",
            "[pip3] nvidia-cusparse-cu12==12.3.1.170\n",
            "[pip3] nvidia-cusparselt-cu12==0.6.2\n",
            "[pip3] nvidia-nccl-cu12==2.21.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.4.127\n",
            "[pip3] nvidia-nvtx-cu12==12.4.127\n",
            "[pip3] nvtx==0.2.11\n",
            "[pip3] optree==0.14.1\n",
            "[pip3] pynvjitlink-cu12==0.5.2\n",
            "[pip3] torch==2.6.0+cu124\n",
            "[pip3] torchaudio==2.6.0+cu124\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.21.0+cu124\n",
            "[pip3] triton==3.2.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.1.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: Caltech101\n",
            "Reading split from /content/datasets/caltech-101/split_zhou_Caltech101.json\n",
            "Creating a 1-shot dataset\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/datasets/caltech-101/split_fewshot/shot_1-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    Caltech101\n",
            "# classes  100\n",
            "# train_x  100\n",
            "# val      100\n",
            "# test     2,465\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed3/tensorboard)\n",
            "epoch [1/50] batch [1/3] time 2.296 (2.296) data 0.860 (0.860) loss 1.9727 (1.9727) acc 65.6250 (65.6250) lr 1.0000e-05 eta 0:05:42\n",
            "epoch [1/50] batch [2/3] time 0.238 (1.267) data 0.001 (0.430) loss 1.8906 (1.9316) acc 50.0000 (57.8125) lr 1.0000e-05 eta 0:03:07\n",
            "epoch [1/50] batch [3/3] time 0.239 (0.924) data 0.000 (0.287) loss 2.3164 (2.0599) acc 53.1250 (56.2500) lr 2.0000e-03 eta 0:02:15\n",
            "epoch [2/50] batch [1/3] time 0.833 (0.833) data 0.594 (0.594) loss 1.8340 (1.8340) acc 56.2500 (56.2500) lr 2.0000e-03 eta 0:02:01\n",
            "epoch [2/50] batch [2/3] time 0.239 (0.536) data 0.001 (0.297) loss 0.8286 (1.3313) acc 71.8750 (64.0625) lr 2.0000e-03 eta 0:01:17\n",
            "epoch [2/50] batch [3/3] time 0.239 (0.437) data 0.000 (0.198) loss 0.7153 (1.1260) acc 78.1250 (68.7500) lr 1.9980e-03 eta 0:01:02\n",
            "epoch [3/50] batch [1/3] time 0.769 (0.769) data 0.529 (0.529) loss 0.6304 (0.6304) acc 87.5000 (87.5000) lr 1.9980e-03 eta 0:01:49\n",
            "epoch [3/50] batch [2/3] time 0.239 (0.504) data 0.001 (0.265) loss 1.0029 (0.8167) acc 75.0000 (81.2500) lr 1.9980e-03 eta 0:01:11\n",
            "epoch [3/50] batch [3/3] time 0.241 (0.416) data 0.000 (0.177) loss 1.1201 (0.9178) acc 71.8750 (78.1250) lr 1.9921e-03 eta 0:00:58\n",
            "epoch [4/50] batch [1/3] time 0.723 (0.723) data 0.487 (0.487) loss 1.2070 (1.2070) acc 78.1250 (78.1250) lr 1.9921e-03 eta 0:01:41\n",
            "epoch [4/50] batch [2/3] time 0.238 (0.481) data 0.001 (0.244) loss 0.9404 (1.0737) acc 81.2500 (79.6875) lr 1.9921e-03 eta 0:01:06\n",
            "epoch [4/50] batch [3/3] time 0.239 (0.400) data 0.000 (0.163) loss 1.0439 (1.0638) acc 78.1250 (79.1667) lr 1.9823e-03 eta 0:00:55\n",
            "epoch [5/50] batch [1/3] time 0.811 (0.811) data 0.575 (0.575) loss 0.4277 (0.4277) acc 84.3750 (84.3750) lr 1.9823e-03 eta 0:01:51\n",
            "epoch [5/50] batch [2/3] time 0.241 (0.526) data 0.001 (0.288) loss 0.7739 (0.6008) acc 75.0000 (79.6875) lr 1.9823e-03 eta 0:01:11\n",
            "epoch [5/50] batch [3/3] time 0.240 (0.431) data 0.000 (0.192) loss 0.5381 (0.5799) acc 84.3750 (81.2500) lr 1.9686e-03 eta 0:00:58\n",
            "epoch [6/50] batch [1/3] time 0.819 (0.819) data 0.582 (0.582) loss 0.7959 (0.7959) acc 81.2500 (81.2500) lr 1.9686e-03 eta 0:01:49\n",
            "epoch [6/50] batch [2/3] time 0.238 (0.529) data 0.001 (0.291) loss 0.6006 (0.6982) acc 78.1250 (79.6875) lr 1.9686e-03 eta 0:01:10\n",
            "epoch [6/50] batch [3/3] time 0.240 (0.432) data 0.000 (0.194) loss 0.4658 (0.6208) acc 81.2500 (80.2083) lr 1.9511e-03 eta 0:00:57\n",
            "epoch [7/50] batch [1/3] time 0.963 (0.963) data 0.721 (0.721) loss 0.6211 (0.6211) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:02:06\n",
            "epoch [7/50] batch [2/3] time 0.240 (0.601) data 0.001 (0.361) loss 0.5781 (0.5996) acc 90.6250 (85.9375) lr 1.9511e-03 eta 0:01:18\n",
            "epoch [7/50] batch [3/3] time 0.240 (0.481) data 0.000 (0.241) loss 0.9683 (0.7225) acc 75.0000 (82.2917) lr 1.9298e-03 eta 0:01:02\n",
            "epoch [8/50] batch [1/3] time 2.301 (2.301) data 1.986 (1.986) loss 0.3989 (0.3989) acc 93.7500 (93.7500) lr 1.9298e-03 eta 0:04:54\n",
            "epoch [8/50] batch [2/3] time 0.238 (1.269) data 0.001 (0.993) loss 0.7461 (0.5725) acc 78.1250 (85.9375) lr 1.9298e-03 eta 0:02:41\n",
            "epoch [8/50] batch [3/3] time 0.239 (0.926) data 0.000 (0.662) loss 0.7544 (0.6331) acc 75.0000 (82.2917) lr 1.9048e-03 eta 0:01:56\n",
            "epoch [9/50] batch [1/3] time 1.743 (1.743) data 1.505 (1.505) loss 0.4172 (0.4172) acc 93.7500 (93.7500) lr 1.9048e-03 eta 0:03:37\n",
            "epoch [9/50] batch [2/3] time 0.240 (0.992) data 0.001 (0.753) loss 0.4062 (0.4117) acc 84.3750 (89.0625) lr 1.9048e-03 eta 0:02:02\n",
            "epoch [9/50] batch [3/3] time 0.239 (0.741) data 0.000 (0.502) loss 0.6655 (0.4963) acc 84.3750 (87.5000) lr 1.8763e-03 eta 0:01:31\n",
            "epoch [10/50] batch [1/3] time 2.116 (2.116) data 1.532 (1.532) loss 0.1991 (0.1991) acc 93.7500 (93.7500) lr 1.8763e-03 eta 0:04:18\n",
            "epoch [10/50] batch [2/3] time 0.240 (1.178) data 0.000 (0.766) loss 0.3745 (0.2868) acc 81.2500 (87.5000) lr 1.8763e-03 eta 0:02:22\n",
            "epoch [10/50] batch [3/3] time 0.241 (0.865) data 0.000 (0.511) loss 0.5923 (0.3886) acc 84.3750 (86.4583) lr 1.8443e-03 eta 0:01:43\n",
            "epoch [11/50] batch [1/3] time 0.758 (0.758) data 0.520 (0.520) loss 0.5249 (0.5249) acc 84.3750 (84.3750) lr 1.8443e-03 eta 0:01:30\n",
            "epoch [11/50] batch [2/3] time 0.241 (0.499) data 0.001 (0.260) loss 0.6094 (0.5671) acc 81.2500 (82.8125) lr 1.8443e-03 eta 0:00:58\n",
            "epoch [11/50] batch [3/3] time 0.242 (0.414) data 0.000 (0.174) loss 0.3386 (0.4910) acc 90.6250 (85.4167) lr 1.8090e-03 eta 0:00:48\n",
            "epoch [12/50] batch [1/3] time 0.786 (0.786) data 0.547 (0.547) loss 0.4666 (0.4666) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:01:31\n",
            "epoch [12/50] batch [2/3] time 0.240 (0.513) data 0.001 (0.274) loss 0.5835 (0.5250) acc 87.5000 (85.9375) lr 1.8090e-03 eta 0:00:59\n",
            "epoch [12/50] batch [3/3] time 0.240 (0.422) data 0.000 (0.183) loss 0.7026 (0.5842) acc 68.7500 (80.2083) lr 1.7705e-03 eta 0:00:48\n",
            "epoch [13/50] batch [1/3] time 1.225 (1.225) data 0.978 (0.978) loss 0.6279 (0.6279) acc 78.1250 (78.1250) lr 1.7705e-03 eta 0:02:18\n",
            "epoch [13/50] batch [2/3] time 0.242 (0.733) data 0.001 (0.489) loss 0.4556 (0.5417) acc 87.5000 (82.8125) lr 1.7705e-03 eta 0:01:22\n",
            "epoch [13/50] batch [3/3] time 0.241 (0.569) data 0.000 (0.326) loss 0.3657 (0.4831) acc 87.5000 (84.3750) lr 1.7290e-03 eta 0:01:03\n",
            "epoch [14/50] batch [1/3] time 0.752 (0.752) data 0.506 (0.506) loss 0.4871 (0.4871) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:01:22\n",
            "epoch [14/50] batch [2/3] time 0.241 (0.496) data 0.001 (0.253) loss 0.5444 (0.5157) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:00:54\n",
            "epoch [14/50] batch [3/3] time 0.242 (0.412) data 0.000 (0.169) loss 0.4702 (0.5006) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:00:44\n",
            "epoch [15/50] batch [1/3] time 0.798 (0.798) data 0.560 (0.560) loss 0.8350 (0.8350) acc 78.1250 (78.1250) lr 1.6845e-03 eta 0:01:25\n",
            "epoch [15/50] batch [2/3] time 0.240 (0.519) data 0.001 (0.280) loss 0.5645 (0.6997) acc 87.5000 (82.8125) lr 1.6845e-03 eta 0:00:55\n",
            "epoch [15/50] batch [3/3] time 0.241 (0.426) data 0.000 (0.187) loss 0.7939 (0.7311) acc 81.2500 (82.2917) lr 1.6374e-03 eta 0:00:44\n",
            "epoch [16/50] batch [1/3] time 0.755 (0.755) data 0.516 (0.516) loss 0.5415 (0.5415) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:01:18\n",
            "epoch [16/50] batch [2/3] time 0.239 (0.497) data 0.001 (0.258) loss 0.2944 (0.4180) acc 90.6250 (89.0625) lr 1.6374e-03 eta 0:00:51\n",
            "epoch [16/50] batch [3/3] time 0.241 (0.412) data 0.000 (0.172) loss 0.1906 (0.3422) acc 93.7500 (90.6250) lr 1.5878e-03 eta 0:00:42\n",
            "epoch [17/50] batch [1/3] time 0.807 (0.807) data 0.564 (0.564) loss 0.4514 (0.4514) acc 87.5000 (87.5000) lr 1.5878e-03 eta 0:01:21\n",
            "epoch [17/50] batch [2/3] time 0.240 (0.523) data 0.001 (0.282) loss 0.8169 (0.6342) acc 78.1250 (82.8125) lr 1.5878e-03 eta 0:00:52\n",
            "epoch [17/50] batch [3/3] time 0.241 (0.429) data 0.000 (0.188) loss 0.2473 (0.5052) acc 90.6250 (85.4167) lr 1.5358e-03 eta 0:00:42\n",
            "epoch [18/50] batch [1/3] time 0.815 (0.815) data 0.575 (0.575) loss 0.4988 (0.4988) acc 84.3750 (84.3750) lr 1.5358e-03 eta 0:01:19\n",
            "epoch [18/50] batch [2/3] time 0.241 (0.528) data 0.001 (0.288) loss 0.7871 (0.6429) acc 84.3750 (84.3750) lr 1.5358e-03 eta 0:00:51\n",
            "epoch [18/50] batch [3/3] time 0.239 (0.432) data 0.000 (0.192) loss 0.7412 (0.6757) acc 75.0000 (81.2500) lr 1.4818e-03 eta 0:00:41\n",
            "epoch [19/50] batch [1/3] time 0.783 (0.783) data 0.537 (0.537) loss 0.4851 (0.4851) acc 84.3750 (84.3750) lr 1.4818e-03 eta 0:01:14\n",
            "epoch [19/50] batch [2/3] time 0.239 (0.511) data 0.001 (0.269) loss 0.4172 (0.4512) acc 87.5000 (85.9375) lr 1.4818e-03 eta 0:00:48\n",
            "epoch [19/50] batch [3/3] time 0.241 (0.421) data 0.000 (0.179) loss 0.3059 (0.4028) acc 93.7500 (88.5417) lr 1.4258e-03 eta 0:00:39\n",
            "epoch [20/50] batch [1/3] time 0.793 (0.793) data 0.555 (0.555) loss 0.7744 (0.7744) acc 75.0000 (75.0000) lr 1.4258e-03 eta 0:01:12\n",
            "epoch [20/50] batch [2/3] time 0.240 (0.517) data 0.001 (0.278) loss 0.4473 (0.6108) acc 84.3750 (79.6875) lr 1.4258e-03 eta 0:00:47\n",
            "epoch [20/50] batch [3/3] time 0.243 (0.426) data 0.000 (0.185) loss 0.6191 (0.6136) acc 87.5000 (82.2917) lr 1.3681e-03 eta 0:00:38\n",
            "epoch [21/50] batch [1/3] time 1.215 (1.215) data 0.975 (0.975) loss 0.3335 (0.3335) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:01:48\n",
            "epoch [21/50] batch [2/3] time 0.242 (0.728) data 0.001 (0.488) loss 0.4829 (0.4082) acc 84.3750 (85.9375) lr 1.3681e-03 eta 0:01:04\n",
            "epoch [21/50] batch [3/3] time 0.245 (0.567) data 0.003 (0.326) loss 0.4285 (0.4150) acc 87.5000 (86.4583) lr 1.3090e-03 eta 0:00:49\n",
            "epoch [22/50] batch [1/3] time 0.804 (0.804) data 0.564 (0.564) loss 0.4639 (0.4639) acc 84.3750 (84.3750) lr 1.3090e-03 eta 0:01:09\n",
            "epoch [22/50] batch [2/3] time 0.242 (0.523) data 0.001 (0.282) loss 0.6831 (0.5735) acc 87.5000 (85.9375) lr 1.3090e-03 eta 0:00:44\n",
            "epoch [22/50] batch [3/3] time 0.241 (0.429) data 0.000 (0.188) loss 0.2617 (0.4696) acc 93.7500 (88.5417) lr 1.2487e-03 eta 0:00:36\n",
            "epoch [23/50] batch [1/3] time 0.822 (0.822) data 0.583 (0.583) loss 0.3801 (0.3801) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:01:08\n",
            "epoch [23/50] batch [2/3] time 0.239 (0.530) data 0.001 (0.292) loss 0.4353 (0.4077) acc 84.3750 (89.0625) lr 1.2487e-03 eta 0:00:43\n",
            "epoch [23/50] batch [3/3] time 0.240 (0.434) data 0.000 (0.195) loss 0.4634 (0.4263) acc 84.3750 (87.5000) lr 1.1874e-03 eta 0:00:35\n",
            "epoch [24/50] batch [1/3] time 0.830 (0.830) data 0.589 (0.589) loss 0.4111 (0.4111) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:01:06\n",
            "epoch [24/50] batch [2/3] time 0.240 (0.535) data 0.001 (0.295) loss 0.2026 (0.3069) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:00:42\n",
            "epoch [24/50] batch [3/3] time 0.241 (0.437) data 0.000 (0.197) loss 0.3618 (0.3252) acc 90.6250 (92.7083) lr 1.1253e-03 eta 0:00:34\n",
            "epoch [25/50] batch [1/3] time 0.827 (0.827) data 0.586 (0.586) loss 0.6948 (0.6948) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:01:03\n",
            "epoch [25/50] batch [2/3] time 0.241 (0.534) data 0.001 (0.293) loss 0.3972 (0.5460) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:00:40\n",
            "epoch [25/50] batch [3/3] time 0.242 (0.437) data 0.000 (0.196) loss 0.7012 (0.5977) acc 81.2500 (85.4167) lr 1.0628e-03 eta 0:00:32\n",
            "epoch [26/50] batch [1/3] time 0.837 (0.837) data 0.595 (0.595) loss 0.5308 (0.5308) acc 84.3750 (84.3750) lr 1.0628e-03 eta 0:01:01\n",
            "epoch [26/50] batch [2/3] time 0.242 (0.540) data 0.001 (0.298) loss 0.3682 (0.4495) acc 87.5000 (85.9375) lr 1.0628e-03 eta 0:00:39\n",
            "epoch [26/50] batch [3/3] time 0.242 (0.440) data 0.000 (0.199) loss 0.3730 (0.4240) acc 90.6250 (87.5000) lr 1.0000e-03 eta 0:00:31\n",
            "epoch [27/50] batch [1/3] time 0.843 (0.843) data 0.603 (0.603) loss 0.4419 (0.4419) acc 87.5000 (87.5000) lr 1.0000e-03 eta 0:00:59\n",
            "epoch [27/50] batch [2/3] time 0.240 (0.542) data 0.001 (0.302) loss 0.3479 (0.3949) acc 93.7500 (90.6250) lr 1.0000e-03 eta 0:00:37\n",
            "epoch [27/50] batch [3/3] time 0.241 (0.441) data 0.000 (0.201) loss 0.4204 (0.4034) acc 84.3750 (88.5417) lr 9.3721e-04 eta 0:00:30\n",
            "epoch [28/50] batch [1/3] time 0.809 (0.809) data 0.566 (0.566) loss 0.4900 (0.4900) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:00:55\n",
            "epoch [28/50] batch [2/3] time 0.243 (0.526) data 0.001 (0.283) loss 0.4800 (0.4850) acc 84.3750 (87.5000) lr 9.3721e-04 eta 0:00:35\n",
            "epoch [28/50] batch [3/3] time 0.243 (0.432) data 0.000 (0.189) loss 0.3171 (0.4290) acc 90.6250 (88.5417) lr 8.7467e-04 eta 0:00:28\n",
            "epoch [29/50] batch [1/3] time 1.231 (1.231) data 0.991 (0.991) loss 0.8008 (0.8008) acc 78.1250 (78.1250) lr 8.7467e-04 eta 0:01:20\n",
            "epoch [29/50] batch [2/3] time 0.241 (0.736) data 0.000 (0.496) loss 0.7290 (0.7649) acc 78.1250 (78.1250) lr 8.7467e-04 eta 0:00:47\n",
            "epoch [29/50] batch [3/3] time 0.242 (0.571) data 0.000 (0.331) loss 0.3572 (0.6290) acc 93.7500 (83.3333) lr 8.1262e-04 eta 0:00:35\n",
            "epoch [30/50] batch [1/3] time 0.816 (0.816) data 0.577 (0.577) loss 0.5049 (0.5049) acc 90.6250 (90.6250) lr 8.1262e-04 eta 0:00:50\n",
            "epoch [30/50] batch [2/3] time 0.240 (0.528) data 0.001 (0.289) loss 0.4758 (0.4904) acc 87.5000 (89.0625) lr 8.1262e-04 eta 0:00:32\n",
            "epoch [30/50] batch [3/3] time 0.241 (0.432) data 0.000 (0.193) loss 0.6211 (0.5339) acc 81.2500 (86.4583) lr 7.5131e-04 eta 0:00:25\n",
            "epoch [31/50] batch [1/3] time 0.750 (0.750) data 0.509 (0.509) loss 0.7183 (0.7183) acc 84.3750 (84.3750) lr 7.5131e-04 eta 0:00:44\n",
            "epoch [31/50] batch [2/3] time 0.241 (0.495) data 0.001 (0.255) loss 0.2991 (0.5087) acc 96.8750 (90.6250) lr 7.5131e-04 eta 0:00:28\n",
            "epoch [31/50] batch [3/3] time 0.240 (0.410) data 0.000 (0.170) loss 0.5234 (0.5136) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:00:23\n",
            "epoch [32/50] batch [1/3] time 0.844 (0.844) data 0.605 (0.605) loss 0.2527 (0.2527) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:00:47\n",
            "epoch [32/50] batch [2/3] time 0.241 (0.542) data 0.001 (0.303) loss 0.1741 (0.2134) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:00:29\n",
            "epoch [32/50] batch [3/3] time 0.242 (0.442) data 0.000 (0.202) loss 0.7666 (0.3978) acc 75.0000 (89.5833) lr 6.3188e-04 eta 0:00:23\n",
            "epoch [33/50] batch [1/3] time 0.843 (0.843) data 0.602 (0.602) loss 0.6055 (0.6055) acc 84.3750 (84.3750) lr 6.3188e-04 eta 0:00:44\n",
            "epoch [33/50] batch [2/3] time 0.239 (0.541) data 0.001 (0.301) loss 0.4336 (0.5195) acc 84.3750 (84.3750) lr 6.3188e-04 eta 0:00:28\n",
            "epoch [33/50] batch [3/3] time 0.241 (0.441) data 0.000 (0.201) loss 0.5127 (0.5173) acc 90.6250 (86.4583) lr 5.7422e-04 eta 0:00:22\n",
            "epoch [34/50] batch [1/3] time 0.984 (0.984) data 0.745 (0.745) loss 0.1569 (0.1569) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:00:49\n",
            "epoch [34/50] batch [2/3] time 0.242 (0.613) data 0.001 (0.373) loss 0.4934 (0.3251) acc 84.3750 (90.6250) lr 5.7422e-04 eta 0:00:30\n",
            "epoch [34/50] batch [3/3] time 0.240 (0.489) data 0.000 (0.249) loss 0.4885 (0.3796) acc 84.3750 (88.5417) lr 5.1825e-04 eta 0:00:23\n",
            "epoch [35/50] batch [1/3] time 0.807 (0.807) data 0.556 (0.556) loss 0.4282 (0.4282) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:00:37\n",
            "epoch [35/50] batch [2/3] time 0.242 (0.524) data 0.001 (0.278) loss 0.4526 (0.4404) acc 93.7500 (92.1875) lr 5.1825e-04 eta 0:00:24\n",
            "epoch [35/50] batch [3/3] time 0.243 (0.430) data 0.000 (0.186) loss 0.4807 (0.4539) acc 81.2500 (88.5417) lr 4.6417e-04 eta 0:00:19\n",
            "epoch [36/50] batch [1/3] time 1.029 (1.029) data 0.790 (0.790) loss 0.2864 (0.2864) acc 96.8750 (96.8750) lr 4.6417e-04 eta 0:00:45\n",
            "epoch [36/50] batch [2/3] time 0.240 (0.635) data 0.001 (0.395) loss 0.3496 (0.3180) acc 87.5000 (92.1875) lr 4.6417e-04 eta 0:00:27\n",
            "epoch [36/50] batch [3/3] time 0.240 (0.503) data 0.000 (0.264) loss 0.6655 (0.4338) acc 84.3750 (89.5833) lr 4.1221e-04 eta 0:00:21\n",
            "epoch [37/50] batch [1/3] time 1.081 (1.081) data 0.838 (0.838) loss 0.6118 (0.6118) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:44\n",
            "epoch [37/50] batch [2/3] time 0.241 (0.661) data 0.001 (0.419) loss 0.5054 (0.5586) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:26\n",
            "epoch [37/50] batch [3/3] time 0.241 (0.521) data 0.000 (0.280) loss 0.5298 (0.5490) acc 90.6250 (88.5417) lr 3.6258e-04 eta 0:00:20\n",
            "epoch [38/50] batch [1/3] time 0.789 (0.789) data 0.548 (0.548) loss 0.3853 (0.3853) acc 84.3750 (84.3750) lr 3.6258e-04 eta 0:00:29\n",
            "epoch [38/50] batch [2/3] time 0.242 (0.515) data 0.001 (0.274) loss 0.4492 (0.4172) acc 93.7500 (89.0625) lr 3.6258e-04 eta 0:00:19\n",
            "epoch [38/50] batch [3/3] time 0.241 (0.424) data 0.000 (0.183) loss 0.8447 (0.5597) acc 75.0000 (84.3750) lr 3.1545e-04 eta 0:00:15\n",
            "epoch [39/50] batch [1/3] time 0.827 (0.827) data 0.589 (0.589) loss 0.8203 (0.8203) acc 81.2500 (81.2500) lr 3.1545e-04 eta 0:00:28\n",
            "epoch [39/50] batch [2/3] time 0.242 (0.534) data 0.001 (0.295) loss 0.6528 (0.7366) acc 81.2500 (81.2500) lr 3.1545e-04 eta 0:00:18\n",
            "epoch [39/50] batch [3/3] time 0.242 (0.437) data 0.001 (0.197) loss 0.2559 (0.5763) acc 90.6250 (84.3750) lr 2.7103e-04 eta 0:00:14\n",
            "epoch [40/50] batch [1/3] time 0.797 (0.797) data 0.558 (0.558) loss 0.2935 (0.2935) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:25\n",
            "epoch [40/50] batch [2/3] time 0.243 (0.520) data 0.000 (0.279) loss 0.5591 (0.4263) acc 84.3750 (87.5000) lr 2.7103e-04 eta 0:00:16\n",
            "epoch [40/50] batch [3/3] time 0.240 (0.427) data 0.000 (0.186) loss 0.2399 (0.3641) acc 90.6250 (88.5417) lr 2.2949e-04 eta 0:00:12\n",
            "epoch [41/50] batch [1/3] time 0.802 (0.802) data 0.563 (0.563) loss 0.2563 (0.2563) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:23\n",
            "epoch [41/50] batch [2/3] time 0.242 (0.522) data 0.001 (0.282) loss 0.5757 (0.4160) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:14\n",
            "epoch [41/50] batch [3/3] time 0.242 (0.428) data 0.000 (0.188) loss 0.2435 (0.3585) acc 93.7500 (91.6667) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [42/50] batch [1/3] time 0.826 (0.826) data 0.586 (0.586) loss 0.4380 (0.4380) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [42/50] batch [2/3] time 0.241 (0.533) data 0.001 (0.293) loss 0.2517 (0.3448) acc 93.7500 (90.6250) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [42/50] batch [3/3] time 0.242 (0.436) data 0.000 (0.196) loss 0.6011 (0.4303) acc 84.3750 (88.5417) lr 1.5567e-04 eta 0:00:10\n",
            "epoch [43/50] batch [1/3] time 0.816 (0.816) data 0.575 (0.575) loss 0.2671 (0.2671) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:00:18\n",
            "epoch [43/50] batch [2/3] time 0.243 (0.529) data 0.001 (0.288) loss 0.1848 (0.2260) acc 96.8750 (95.3125) lr 1.5567e-04 eta 0:00:11\n",
            "epoch [43/50] batch [3/3] time 0.241 (0.433) data 0.000 (0.192) loss 0.5469 (0.3329) acc 87.5000 (92.7083) lr 1.2369e-04 eta 0:00:09\n",
            "epoch [44/50] batch [1/3] time 1.148 (1.148) data 0.908 (0.908) loss 0.2983 (0.2983) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:22\n",
            "epoch [44/50] batch [2/3] time 0.241 (0.695) data 0.001 (0.454) loss 0.4844 (0.3914) acc 81.2500 (87.5000) lr 1.2369e-04 eta 0:00:13\n",
            "epoch [44/50] batch [3/3] time 0.243 (0.544) data 0.000 (0.303) loss 0.4619 (0.4149) acc 87.5000 (87.5000) lr 9.5173e-05 eta 0:00:09\n",
            "epoch [45/50] batch [1/3] time 0.919 (0.919) data 0.679 (0.679) loss 0.3408 (0.3408) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:15\n",
            "epoch [45/50] batch [2/3] time 0.242 (0.580) data 0.001 (0.340) loss 0.4910 (0.4159) acc 84.3750 (87.5000) lr 9.5173e-05 eta 0:00:09\n",
            "epoch [45/50] batch [3/3] time 0.242 (0.468) data 0.000 (0.226) loss 0.4521 (0.4280) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:07\n",
            "epoch [46/50] batch [1/3] time 0.780 (0.780) data 0.540 (0.540) loss 0.4487 (0.4487) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:10\n",
            "epoch [46/50] batch [2/3] time 0.242 (0.511) data 0.001 (0.270) loss 0.4155 (0.4321) acc 84.3750 (85.9375) lr 7.0224e-05 eta 0:00:06\n",
            "epoch [46/50] batch [3/3] time 0.241 (0.421) data 0.000 (0.180) loss 0.2549 (0.3730) acc 90.6250 (87.5000) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [1/3] time 0.767 (0.767) data 0.527 (0.527) loss 0.2052 (0.2052) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:08\n",
            "epoch [47/50] batch [2/3] time 0.240 (0.503) data 0.001 (0.264) loss 0.4187 (0.3120) acc 87.5000 (92.1875) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [3/3] time 0.241 (0.416) data 0.000 (0.176) loss 0.4829 (0.3689) acc 87.5000 (90.6250) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [1/3] time 0.775 (0.775) data 0.536 (0.536) loss 0.2637 (0.2637) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [48/50] batch [2/3] time 0.243 (0.509) data 0.001 (0.268) loss 0.1655 (0.2146) acc 93.7500 (95.3125) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [3/3] time 0.241 (0.419) data 0.000 (0.179) loss 0.5479 (0.3257) acc 78.1250 (89.5833) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [1/3] time 0.828 (0.828) data 0.587 (0.587) loss 0.3616 (0.3616) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:04\n",
            "epoch [49/50] batch [2/3] time 0.242 (0.535) data 0.001 (0.294) loss 0.3633 (0.3624) acc 84.3750 (87.5000) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [3/3] time 0.242 (0.437) data 0.000 (0.196) loss 0.9629 (0.5626) acc 75.0000 (83.3333) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [1/3] time 0.812 (0.812) data 0.573 (0.573) loss 0.3635 (0.3635) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [2/3] time 0.242 (0.527) data 0.001 (0.287) loss 0.5161 (0.4398) acc 87.5000 (90.6250) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [3/3] time 0.242 (0.432) data 0.000 (0.191) loss 0.5869 (0.4889) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/caltech101/CoOp/vit_b16_ep50_1shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:17<00:00,  1.45it/s]\n",
            "=> result\n",
            "* total: 2,465\n",
            "* correct: 2,304\n",
            "* accuracy: 93.5%\n",
            "* error: 6.5%\n",
            "* macro_f1: 90.6%\n",
            "Elapsed: 0:01:40\n"
          ]
        }
      ]
    }
  ]
}